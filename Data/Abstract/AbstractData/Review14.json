{
  "1.Machine Translation": {
    "Adaptive Machine Translation with Large Language Models": "Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, LLMs can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).",
    "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DIPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DIPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
    "Document-Level Machine Translation with Large Language Models": "Large language models (LLMs) such as Chat- GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document- level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs\u2019 ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context- Aware Prompts , where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models , where we compare the translation performance of Chat- GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities , where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.",
    "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models": "Machine Translation (MT) has greatly advanced over the years due to the developments in deep neural networks. However, the emergence of Large Language Models (LLMs) like GPT -4 and ChatGPT is introducing a new phase in the MT domain. In this context, we believe that the future of MT is intricately tied to the capabilities of LLMs. These models not only offer vast linguistic understandings but also bring innovative methodologies, such as prompt-based techniques, that have the potential to further elevate MT. In this paper, we provide an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations. We highlight several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation. Additionally, we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies. By showcasing practical instances, we aim to demonstrate the advantages that LLMs offer, particularly in tasks like translating extended documents. We conclude by emphasizing the critical role of LLMs in guiding the future evolution of MT and offer a roadmap for future exploration in the sector.",
    "RAMP : Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation": "Attribute-controlled translation (ACT) is a sub-task of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP ), which leverages large multilingual language models to perform ACT in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings.",
    "Instruction Position Matters in Sequence Generation with Large Language Models": "Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model\u2019s learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks.",
    "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions": "Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment- Weighted Instruction Embedding) and an instruction-following dataset OVERMISS.SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISSimproves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two mainstream open-source LLMs, BLOOM and LLaMA. The experimental results demonstrate significant improvements in translation performance with SWIE based on BLOOMZ-3b, particularly in zero-shot and long text translations due to reduced instruction forgetting risk. Additionally, OVERMISS outperforms the baseline in translation performance ( e.g.an increase in BLEU scores from 0.69 to 3.12 and an average improvement of 0.48 percentage comet scores for LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE (e.g.the BLUE scores increase up to 0.56 from English to German across three different backbones), and both exhibit improvements in the faithfulness metric based on word alignment.",
    "Neural Machine Translation Models Can Learn to be Few-shot Learners": "The emergent ability of Large Language Mod- els to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural ma- chine translation. With this capacity for ICL, the model can take advantage of relevant few- shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch in- ference on a mix of domains and outperforms state-of-the-art baselines in terms of both trans- lation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
    "Adapting Large Language Models for Document-Level Machine Translation": "Large language models (LLMs) have significantly advanced various natural language processing (NLP) tasks. Recent research indicates that moderately-sized LLMs often outperform larger ones after task-specific fine-tuning. This study focuses on adapting LLMs for document-level machine translation ( DOCMT) for specific language pairs. We first investigate the impact of prompt strategies on translation performance and then conduct extensive experiments using two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our results show that specialized models can sometimes surpass GPT-4 in translation performance but still face issues like off-target translation due to error propagation in decoding. We provide an in-depth analysis of these LLMs tailored for DOCMT, examining translation errors, discourse phenomena, strategies for training and inference, the data efficiency of parallel documents, recent test set evaluations, and zero-shot crosslingual transfer. Our findings highlight the strengths and limitations of LLM-based DOCMTmodels and provide a foundation for future research.",
    "Translation Performance from the User\u2019s Perspective of Large Language Models and Neural Machine Translation Systems": "The rapid global expansion of ChatGPT, which plays a crucial role in interactive knowledge sharing and translation, underscores the importance of comparative performance assessments in artificial intelligence (AI) technology. This study concentrated on this crucial issue by exploring and contrasting the translation performances of large language models (LLMs) and neural machine translation (NMT) systems. For this aim, the APIs of Google Translate, Microsoft Translator, and OpenAI\u2019s ChatGPT were utilized, leveraging parallel corpora from the Workshop on Machine Translation (WMT) 2018 and 2020 benchmarks. By applying recognized evaluation metrics such as BLEU, chrF, and TER, a comprehensive performance analysis across a variety of language pairs, translation directions, and reference token sizes was conducted. The findings reveal that while Google Translate and Microsoft Translator generally surpass ChatGPT in terms of their BLEU, chrF, and TER scores, ChatGPT exhibits superior performance in specific language pairs. Translations from non - English to English consistently yielded better results across all three systems compared with translations from English to non - English. Significantly, an improvement in translation system performance was observed as the token size increased, hinting at the potential benefits of training models on larger token sizes."
  },
  "2.Text Generation": {
    "Token-level Optimization for Enhanced Text Generation: A Prompt Engineering Framework with Large Language Models": "The increasing reliance on machine-generated text across various applications has highlighted the importance of ensuring high-quality outputs that are both coherent and contextually accurate. Conventional approaches to prompt engineering often involve manual tuning, which introduces limitations in scalability and consistency, particularly for tasks that demand real-time responsiveness. A novel token-level-guided automatic prompt optimization (TAPO) framework has been developed to address these challenges, offering an adaptive mechanism that refines prompts through real-time feedback at the token level. Through its integration with the Mistral model, the framework significantly improves fluency, coherence, and factual accuracy across a range of text generation tasks. The results demonstrate that the TAPO framework outperforms human-designed prompts by dynamically adjusting token probabilities, providing a more efficient and scalable solution for high-quality text generation. This contribution opens new possibilities for fully automated prompt optimization in LLMs, reducing human intervention while maintaining the adaptability required for complex language tasks.",
    "Evaluating Generative Models for Graph-to-Text Generation": "Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available."
  },
  "3.Text Classification": {
    "Universal Language Model Fine-tuning for Text Classi\ufb01cation": "Inductive transfer learning has greatly im- pacted computer vision, but existing ap- proaches in NLP still require task-speci\ufb01c modi\ufb01cations and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective trans- fer learning method that can be applied to any task in NLP, and introduce techniques that are key for \ufb01ne-tuning a language model. Our method signi\ufb01cantly outper- forms the state-of-the-art on six text clas- si\ufb01cation tasks, reducing the error by 18- 24% on the majority of datasets. Further- more, with only 100labeled examples, it matches the performance of training from scratch on 100\u0002more data. We open- source our pretrained models and code1.",
    "Text Classi\ufb01cation Using Label Names Only: A Language Model Self-Training Approach": "Current text classi\ufb01cation methods typically require a good number of human-labeled doc- uments as training data, which can be costly and dif\ufb01cult to obtain in real applications. Hu- mans can perform classi\ufb01cation without see- ing any labeled examples but only based on a small set of words describing the categories to be classi\ufb01ed. In this paper, we explore the potential of only using the label name of each class to train classi\ufb01cation models on un- labeled data, without using any labeled doc- uments. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as rep- resentation learning models for document clas- si\ufb01cation. Our method (1) associates semanti- cally related words with the label names, (2) \ufb01nds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% ac- curacy on four benchmark datasets including topic and sentiment classi\ufb01cation without us- ing any labeled documents but learning from unlabeled data supervised by at most 3words (1in most cases) per class as the label name1.",
    "Large Scale Legal Text Classi\ufb01cation Using Transformer Models": "Large multi-label text classi\ufb01cation is a challenging Natural Language Processing (NLP) problem that is concerned with text classi\ufb01cation for datasets with thousands of labels. We tackle this problem in the legal domain, where datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc vocabulary were created within the legal information systems of the European Union. The EuroVoc taxonomy includes around 7000 concepts. In this work, we study the performance of various recent transformer-based models in combination with strategies such as generative pretraining, gradual unfreezing and discriminative learning rates in order to reach competitive classi\ufb01cation performance, and present new state-of-the-art results of 0:661 (F1) for JRC-Acquis and 0:754 for EURLEX57K. Furthermore, we quantify the impact of individual steps, such as language model \ufb01ne-tuning or gradual unfreezing in an ablation study, and provide reference dataset splits created with an iterative strati\ufb01cation algorithm.",
    "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection": "Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.8% to 99.2% on an in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test set. The results indicate the effectiveness, good generalization ability, and great potential of adaptive ensemble algorithms in LLM-generated text detection.",
    "MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Unit Detection": "The Facial Action Coding System (FACS) encodes the action units (AUs) in facial images, which has attracted extensive research attention due to its wide use in facial expression analysis. Many methods that perform well on automatic facial action unit (AU) detection primarily focus on modeling various AU relations between corresponding local muscle areas or mining global attention\u2013aware facial features; however, they neglect the dynamic interactions among local-global features. We argue that encoding AU features just from one perspective may not capture the rich contextual information between regional and global face features, as well as the detailed variability across AUs, because of the diversity in expression and individual characteristics. In this article, we propose a novel Multi-level Graph Relational Reasoning Network (termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a multi-level (i.e., region-level, pixel-wise, and channel-wise level) feature learning. On the one hand, the region-level feature learning from the local face patch features via graph neural network can encode the correlation across different AUs. On the other hand, pixel-wise and channel-wise feature learning via graph attention networks (GAT) enhances the discrimination ability of AU features by adaptively recalibrating feature responses of pixels and channels from global face features. The hierarchical fusion strategy combines features from the three levels with gated fusion cells to improve AU discriminative ability. Extensive experiments on DISFA and BP4D AU datasets show that the proposed approach achieves superior performance than the state-of-the-art methods.",
    "Improving Text Classification with Large Language Model-Based Data Augmentation": "Large Language Models (LLMs) such as ChatGPT possess advanced capabilities in understanding and generating text. These capabilities enable ChatGPT to create text based on specific instructions, which can serve as augmented data for text classification tasks. Previous studies have approached data augmentation (DA) by either rewriting the existing dataset with ChatGPT or generating entirely new data from scratch. However, it is unclear which method is better without comparing their effectiveness. This study investigates the application of both methods to two datasets: a general - topic dataset (Reuters news data) and a domain - specific dataset (Mitigation dataset). Our findings indicate that: 1. ChatGPT generated new data consistently enhanced model\u2019s classification results for both datasets. 2. Generating new data generally outperforms rewriting existing data, though crafting the prompts carefully is crucial to extract the most valuable information from ChatGPT, particularly for domain -",
    "A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification": "Text Classi\ufb01cation problem has been thoroughly studied in information retrieval problems and data mining tasks. It is bene\ufb01cial in multiple tasks including medical diagnose health and care department, targeted marketing, entertainment industry, and group \ufb01ltering processes. A recent innovation in both data mining and natural language processing gained the attention of researchers from all over the world to develop automated systems for text classi\ufb01cation. NLP allows categorizing documents containing di\ufb00erent texts. A huge amount of data is generated on social media sites through social media users. Three datasets have been used for experimental purposes including the COVID - 19 fake news dataset, COVID - 19 English tweet dataset, and extremist - non - extremist dataset which contain news blogs, posts, and tweets related to coronavirus and hate speech. Transfer learning approaches do not experiment on COVID - 19 fake news and extremist - non - extremist datasets. Therefore, the proposed work applied transfer learning classi\ufb01cation models on both these datasets to check the performance of transfer learning models. Models are trained and evaluated on the accuracy, precision, recall, and F1 - score. Heat maps are also generated for every model. In the end, future directions are proposed."
  },
  "4.Text Summarization": {
    "SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization": "Medical dialogue summarization is challeng- ing due to the unstructured nature of medical conversations, the use of medical terminology in gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for section- wise summarization (Task A) is a two-stage process of selecting semantically similar dia- logues and using the top- ksimilar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solu- tion with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompt- ing for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 sum- maries are more abstractive and shorter. We make our code publicly available1.",
    "Legal Summarisation through LLMs: The PRODIGIT Project": "We present some initial results of a large-scale Italian project called PRODIGIT which aims to support tax judges and lawyers through digital technology, focusing on AI. We have focused on generation of summaries of judicial decisions and on the extraction of related information, such as the identification of legal issues and decision-making criteria, and the specification of keywords. To this end, we have deployed and evaluated different tools and approaches to extractive and abstractive summarisation. We have applied LLMs, and particularly on GPT4, which has enabled us to obtain results that proved satisfactory, according to an evaluation by expert tax judges and lawyers. On this basis, a prototype application is being built which will be made publicly available.",
    "Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models": "Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets. The assessment of these models\u2019 effectiveness contributes valuable insights to researchers and practitioners within the NLP domain. This work serves as a resource for those interested in harnessing the potential of LLMs for text summarization and lays the foundation for the development of advanced Generative AI applications aimed at addressing a wide spectrum of business challenges.",
    "AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization": "Despite the advances in the abstractive sum- marization task using Large Language Mod- els (LLM), there is a lack of research that as- sess their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of LLMs on the sum- marization task across various domains in both fine-tuning and in-context learning settings. We also present AdaptEval, the first domain adap- tation evaluation suite. AdaptEval includes a domain benchmark and a set of metrics to fa- cilitate the analysis of domain adaptation. Our results demonstrate that LLMs exhibit compa- rable performance in the in-context learning setting, regardless of their parameter scale."
  },
  "5.Sentiment Analysis": {
    "Sentiment trading with large language models": "We analyse the performance of the large language models (LLMs) OPT, BERT, and FinBERT, alongside the traditional Loughran - McDonald dictionary, in the sentiment analysis of 965,375 U.S. financial news articles from 2010 to 2023. Our findings reveal that the GPT - 3 - based OPT model significantly outperforms the others, predicting stock market returns with an accuracy of 74.4%. A long - short strategy based on OPT, accounting for 10 basis points (bps) in transaction costs, yields an exceptional Sharpe ratio of 3.05. From August 2021 to July 2023, this strategy produces an impressive 355% gain, outperforming other strategies and traditional market portfolios. This underscores the transformative potential of LLMs in financial market prediction and portfolio management and the necessity of employing sophisticated language models to develop effective investment strategies based on news sentiment.",
    "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models": "Financial sentiment analysis is a challenging task due to the spe- cialized language and lack of labeled data in that domain. General- purpose models are not effective enough because of specialized language used in financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain- specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in financial domain. Our results show improvement in every measured metric on current state-of-the- art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.",
    "Leveraging Pre-trained Language Model for Speech Sentiment Analysis": "In this paper, we explore the use of pre-trained language models to learn sentiment information of written texts for speech sentiment analysis. First, we investigate how useful a pre-trained language model would be in a 2-step pipeline approach employing Automatic Speech Recognition (ASR) and transcripts-based sentiment analysis separately. Second, we propose a pseudo label-based semi-supervised training strategy using a language model on an end-to-end speech sentiment approach to take advantage of a large, but unlabeled speech dataset for training. Although spoken and written texts have different linguistic characteristics, they can complement each other in understanding sentiment. Therefore, the proposed system can not only model acoustic characteristics to bear sentiment-specific information in speech signals, but learn latent information to carry sentiments in the text representation. In these experiments, we demonstrate the proposed approaches improve F1 scores consistently compared to systems without a language model. Moreover, we also show that the proposed framework can reduce 65% of human supervision by leveraging a large amount of data without human sentiment annotation and boost performance in a low-resource condition where the human sentiment annotation is not available enough.",
    "Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study": "Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions ,sentiments , and emotions contained in the text. Specifically, we evaluate it in three settings, including standard evaluation, polarity shift evaluation and open - domain evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine - tuned BERT and corresponding state - of - the - art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.",
    "Sentiment Analysis in the Era of Large Language Models: A Reality Check": "Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs\u2019 SA abilities and propose a novel benchmark, SENTI EVAL, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at https://github.com/DAMO-NLP-SG/LLM-Sentiment.",
    "LETS: A Label-Efficient Training Scheme for Aspect-Based Sentiment Analysis by Using a Pre-Trained Language Model": "Recently proposed pre-trained language models can be easily \u001cne-tuned to a wide range of downstream tasks. However, a large-scale labelled task-speci\u001cc dataset is required for \u001cne-tuning creating a bottleneck in the development process of machine learning applications. To foster a fast development by reducing manual labelling efforts, we propose a Label-Ef\u001ccient Training Scheme (LETS). The proposed LETS consists of three elements: (i) task-speci\u001cc pre-training to exploit unlabelled task-speci\u001cc corpus data, (ii) label augmentation to maximise the utility of labelled data, and (iii) active learning to label data strategically. In this paper, we apply LETS to a novel aspect-based sentiment analysis (ABSA) use-case for analysing the reviews of the health-related program supporting people to improve their sleep quality. We validate the proposed LETS on a custom health-related program-reviews dataset and another ABSA benchmark dataset. Experimental results show that the LETS can reduce manual labelling efforts 2-3 times compared to labelling with random sampling on both datasets. The LETS also outperforms other state-of-the-art active learning methods. Furthermore, the experimental results show that LETS can contribute to better generalisability with both datasets compared to other methods thanks to the task-speci\u001cc pre-training and the proposed label augmentation. We expect this work could contribute to the natural language processing (NLP) domain by addressing the issue of the high cost of manually labelling data. Also, our work could contribute to the healthcare domain by introducing a new potential application",
    "AraXLNet: pre\u2011trained language model for sentiment analysis of Arabic": "The Arabic language is a complex language with little resources; therefore, its limitations create a challenge to produce accurate text classification tasks such as sentiment analysis. The main goal of sentiment analysis is to determine the overall orientation of a given text in terms of whether it is positive, negative, or neutral. Recently, language models have shown great results in promoting the accuracy of text classification in English. The models are pre - trained on a large dataset and then fine - tuned on the downstream tasks. Particularly, XLNet has achieved state - of - the - art results for diverse natural language processing (NLP) tasks in English. In this paper, we hypothesize that such parallel success can be achieved in Arabic. The paper aims to support this hypothesis by producing the first XLNet - based language model in Arabic called AraXLNet, demonstrating its use in Arabic sentiment analysis in order to improve the prediction accuracy of such tasks. The results showed that the proposed model, AraXLNet, with Farasa segmenter achieved an accuracy results of 94.78%, 93.01%, and 85.77% in sentiment analysis task for Arabic using multiple benchmark datasets. This result outperformed AraBERT that obtained 84.65%, 92.13%, and 85.05% on the same datasets, respectively. The improved accuracy of the proposed model was evident using multiple benchmark datasets, thus offering promising advancement in the Arabic text classification task",
    "Sentiment Analysis Using Pre-Trained Language Model With No Fine-Tuning and Less Resource": "Sentiment analysis has become popular when Natural Language Processing algorithms were proven to be able to process complex sentences with good accuracy. Recently, pre-trained language models such as BERT and mBERT, have been shown to be effective for improving language tasks. Most of the work in implementing the models focuses on \u001cne-tuning BERT to achieve desirable results. However, this approach is resource-intensive and requires a long training time, up to a few hours on a GPU, depending on the dataset. Hence, this paper proposes a less complex system with less training time using the BERT model without the \u001cne-tuning process and adopting a feature reduction algorithm to reduce sentence embeddings. The experimental results show that with 50% fewer sentence embeddings, the proposed system improves the accuracy by 1-2% with 71% less training time and 89% less memory usage. The proposed approach has also been proven to work for multilingual tasks by using a single mBERT model.",
    "Improving Sentiment Analysis in Election-Based Conversations on Twitter with ElecBERT Language Model": "Sentiment analysis plays a vital role in understanding public opinions and sentiments toward various topics. In recent years, the rise of social media platforms (SMPs) has provided a rich source of data for analyzing public opinions, particularly in the context of election-related conversations. Nevertheless, sentiment analysis of election-related tweets presents unique challenges due to the complex language used, including figurative expressions, sarcasm, and the spread of misinformation. To address these challenges, this paper proposes Election-focused Bidirectional Encoder Representations from Transformers (ElecBERT), a new model for sentiment analysis in the context of election-related tweets. Election-related tweets pose unique challenges for sentiment analysis due to their complex language, sarcasm, and misinformation. ElecBERT is based on the Bidirectional Encoder Representations from Transformers (BERT) language model and is fine-tuned on two datasets: Election-Related Sentiment-Annotated Tweets (ElecSent)-Multi-Languages, containing 5.31 million labeled tweets in multiple languages, and ElecSent-English, containing 4.75 million labeled tweets in English. The model outperforms other machine learning models such as Support Vector Machines (SVM), Na\u00efve Bayes (NB), and eXtreme Gradient Boosting (XGBoost), with an accuracy of 0.9905 and F1-score of 0.9816 on ElecSent-Multi-Languages, and an accuracy of 0.9930 and F1-score of 0.9899 on ElecSent-English. The performance of different models was compared using the 2020 United States (US) Presidential Election as a case study. The ElecBERT-English and ElecBERT-Multi-Languages models outper"
  },
  "6.Spam Filtering": {
    "An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham \u2013 A Large Language Model Approach": "Phishing and spam detection is long standing challenge that has been the subject of much academic research. Large Language Models (LLM) have vast potential to transform society and provide new and innovative approaches to solve well-established challenges. Phishing and spam have caused financial hardships and lost time and resources to email users all over the world and frequently serve as an entry point for ransomware threat actors. While detection approaches exist, especially heuristic-based approaches, LLMs offer the potential to venture into a new unexplored area for understanding and solving this challenge. LLMs have rapidly altered the landscape from business, consumers, and throughout academia and demonstrate transformational potential for the potential of society. Based on this, applying these new and innovative approaches to email detection is a rational next step in academic research. In this work, we present IPSDM, our model based on fine-tuning the BERT family of models to specifically detect phishing and spam email. We demonstrate our fine-tuned version, IPSDM, is able to better classify emails in both unbalanced and balanced datasets. This work serves as an important first step towards employing LLMs to improve the security of our information systems.",
    "Investigating the Effectiveness of Bayesian Spam Filters in Detecting LLM-modified Spam Mails": "Spam and phishing remain critical threats in cybersecurity, responsible for nearly 90% of security incidents. As these attacks grow in sophistication, the need for robust defensive mechanisms intensifies. Bayesian spam filters, like the widely adopted open - source SpamAssassin, are essential tools in this fight. However, the emergence of large language models (LLMs) such as ChatGPT presents new challenges. These models are not only powerful and accessible, but also inexpensive to use, raising concerns about their misuse in crafting sophisticated spam emails that evade traditional spam filters. This work aims to evaluate the robustness and effectiveness of SpamAssassin against LLM - modified email content. We developed a pipeline to test this vulnerability. Our pipeline modifies spam emails using GPT - 3.5 Turbo and assesses SpamAssassin\u2019s ability to classify these modified emails correctly. The results show that SpamAssassin misclassified up to 73.7% of LLM - modified spam emails as legitimate. In contrast, a simpler dictionary - replacement attack showed a maximum success rate of only 0.4%. These findings highlight the significant threat posed by LLM - modified spam, especially given the cost - efficiency of such attacks (0.17 cents per email). This paper provides crucial insights into the vulnerabilities of current spam filters and the need for continuous improvement in cybersecurity measures.",
    "Large Language Models for Phishing and Spam Detection: A BERT Approach": "In the modern world, emerging hazards not only attack computers but also steal personal information and financial resources.  The most widely used means of interaction lately are emails and text messages, and as the percentage of emails increases, so does the amount of spam and phishing. Spam is any type of unwanted, unauthorized electronic communication that is transmitted in large quantities. Spam emails and instant messages waste a lot of resources by redundantly inundating wireless networks. However, most spam emails are sent by commercials looking to promote their offerings, and few are extremely malicious in nature, such as phishing emails, which attempt to trick those targeted into disclosing confidential data such as website credentials or credit card details. Phishing offers a combination of technological and social engineering techniques to steal data on victims' identities and accounts, it is imperative to curtail the threat and criminal activities associated with it. Spam and Phishing mail have become increasingly prevalent significantly in recent years, necessitating sophisticated countermeasures. Despite developed approaches for detecting this type of email, a comprehensive solution remains needed to combat these threats. Mail detectors will be used to identify spam, phishing, and ham messages.  This research focuses on demonstrating the potential of a Large Language model technique (specifically the pre -trained BERT model) to detect phishing and spam emails according to their context. A comparative evaluation and analysis are conducted on these approaches. The performance of the model is measured using various evaluation metrics such as Accuracy, Precision, Recall, and F1 -Score."
  },
  "7.Question Answering": {
    "Pre-trained Language Model for Biomedical Question Answering": "The recent success of question answering systems is largely attributed to pre-trained language models. However, as language models are mostly pre-trained on general domain corpora such as Wikipedia, they often have difficulty in understanding biomedical questions. In this paper, we investigate the performance of BioBERT, a pre-trained biomedical language model, in answering biomedical questions including factoid, list, and yes/no type questions. BioBERT uses almost the same structure across various question types and achieved the best performance in the 7th BioASQ Challenge (Task 7b, Phase B). BioBERT pre-trained on SQuAD or SQuAD 2.0 easily outperformed previous state-of-the-art models. BioBERT obtains the best performance when it uses the appropriate pre-/post-processing strategies for questions, passages, and answers.",
    "Multilingual Question Answering applied to Conversational Agents": "Recent advances with language models (e.g. BERT, XLNet, ...), have allowed surpassing human performance on complex NLP tasks such as Reading Comprehension. However, labeled datasets for training are available mostly in English which makes it dif\ufb01cult to acknowledge progress in other languages. Fortunately, models are now pre-trained on unlabeled data from hundreds of languages and exhibit interesting transfer abilities from one language to another. In this paper, we show that multilingual BERT is naturally capable of zero-shot transfer for an extractive Question Answering task (eQA) from English to other languages. More specif-ically, it outperforms the best previously known baseline for transfer to Japanese and French. Moreover, using a recently published large eQA French dataset, we are able to further show that (1) zero-shot transfer provides results really close to a direct training on the target language and (2) combination of transfer and training on target is the best option overall. We \ufb01nally present a practical application: a multilingual conversational agent called Kate which answers to HR-related questions in several languages directly from the content of intranet pages.",
    "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering": "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA - GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, Open - BookQA) and biomedical (MedQA - USMLE) domains. QA - GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions. Our code and data are available at https://github.com/michiyasunaga/qagnn.",
    "Towards Expert-Level Medical Question Answering with Large Language Models": "Recent arti\ufb01cial intelligence (AI) systems have reached milestones in \u201cgrand challenges\u201d ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed signi\ufb01cant progress in medical question answering; Med-PaLM was the \ufb01rst model to exceed a \u201cpassing\u201d score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested signi\ufb01cant room for improvement, especially when models\u2019 answers were compared to clinicians\u2019 answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain \ufb01netuning, and prompting strategies including a novel ensemble re\ufb01nement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p< 0.001). We also observed signi\ufb01cant improvements compared to Med-PaLM on every evaluation axis (p< 0.001) on newly introduced datasets of 240 long-form \u201cadversarial\u201d questions to probe LLM limitations. While further studies are necessary to validate the e\ufb03cacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question",
    "MATHCHAT: CONVERSE TO TACKLE CHALLENGING MATH PROBLEMS WITH LLM AGENTS": "Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. LLMs, with their generalized ability, are used as a foundation model to build AI agents for different tasks. In this paper, we study the effectiveness of utilizing LLM agents to solve math problems through conversations. We propose MathChat, a conversational problem - solving framework designed for math problems. MathChat consists of an LLM agent and a user proxy agent which is responsible for tool execution and additional guidance. This synergy facilitates a collaborative problem - solving process, where the agents engage in a dialogue to solve the problems. We perform evaluation on difficult high school competition problems from the MATH dataset. Utilizing Python, we show that MathChat can further improve previous tool - using prompting methods by 6%.",
    "Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models": "Efficient knowledge management plays a pivotal role in aug- menting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectoriza- tion, a variety of knowledge retrieval methods have emerged, significantly enhancing the efficacy of knowledge management systems. Recently, the rapid advancements in generative natural language processing technolo- gies paved the way for generating precise and coherent answers after retrieving relevant documents tailored to user queries. However, for en- terprise knowledge bases, assembling extensive training data from scratch for knowledge retrieval and generation is a formidable challenge due to the privacy and security policies of private data, frequently entailing substantial costs. To address the challenge above, in this paper, we pro- pose EKRG, a novel Retrieval- Generation framework based on large language models (LLMs), expertly designed to enable question-answering forEnterprise Knowledge bases with limited annotation costs . Specif- ically, for the retrieval process, we first introduce an instruction-tuning method using an LLM to generate sufficient document-question pairs for training a knowledge retriever. This method, through carefully de- signed instructions, efficiently generates diverse questions for enterprise knowledge bases, encompassing both fact-oriented and solution-oriented knowledge. Additionally, we develop a relevance-aware teacher-student",
    "Generalizing Question Answering System with Pre-trained Language Model Fine-tuning": "With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC) tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these models and techniques can generalize to out-of-domain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our model is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our methods, with an average Exact Match score of 56.59 and an average F1 score of 68.98, which significantly improves the BERT-Large baseline by 8.39 and 7.22, respectively.",
    "Real Life Application of a Question Answering System Using BERT Language Model": "Real life scenarios are often left untouched by the newest advances in research. They usually require the resolution of some speci\ufb01c task applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in Deep Learning to a task of text classi\ufb01cation. The goal is to create a question answering system in Italian that provides information about a speci\ufb01c subject, e - invoicing and digital billing. Italy recently introduced a new legislation about e - invoicing and people have some legit doubts, therefore a large share of professionals could bene\ufb01t from this tool. We gathered few pairs of question and answers; afterwards, we expanded the data, using it as a training corpus for BERT language model. Through a separate test corpus we evaluated the accuracy of the answer provided. Values show that the automatic system alone performs surprisingly well. The demo interface is hosted on Telegram, which makes the system immediately available to test.",
    "Prompting Large Language Models for\nTopic Modeling": "Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopics ability to uncover relevant topics in multiple datasets."

  },
  "8.specialized domains such as healthcare, education, and": {
    "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models": "We document the capability of large language models (LLMs) like ChatGPT to predict stock market reactions from news headlines without direct financial training. Using post-knowledge-cutoff headlines, GPT-4 captures initial market responses, achieving approximately 90% portfolio-day hit rates for the non-tradable initial reaction. GPT-4 scores also significantly predict the subsequent drift, especially for small stocks and negative news. Forecasting ability generally increases with model size, suggesting that financial reasoning is an emerging capacity of complex LLMs. Strategy returns decline as LLM adoption rises, consistent with improved price efficiency. To rationalize these findings, we develop a theoretical model that incorporates LLM technology, information-processing capacity constraints, underreaction, and limits to arbitrage.",
    "Simulating Classroom Education with LLM-Empowered Agents": "Large language models (LLMs) have been applied across various intelligent educational tasks to assist teaching. While preliminary studies have focused on task-specific, independent LLM-empowered agents, the potential of LLMs within a multi-agent collaborative framework for classroom simulation with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation teaching framework. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Using the Flanders Interactive Analysis System and Community of Inquiry theoretical frameworks from educational analysis, we demonstrate that LLMs can simulate a dynamic learning environment for users with active teacher-student and student-student interactions. We also observe group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching.",
    "Advancements and Applications of Generative Artificial Intelligence and Large Language Models on Business Management: A Comprehensive Review": "This comprehensive review delves into the landscape and recent advancements of Generative Artificial Intelligence (AI) and Large Language Models (LLMs), shedding light on their transformative potential and applications across various sectors. Generative AI, exemplified by models like ChatGPT, DALL -E, and Midjourney, has rapidly evolved  and is  driven by breakthroughs in deep learning architectures and the availability of vast datasets. Concurrently, LLMs have revolutionized natural language processing tasks , utilizing vast text corpora to generate human -like text. The study explores recent developments, including the introduction o",
    "A systematic review of large language models and their implications in medical education": "Introduction: In the past year, the use of large lan guage models (LLMs) has generated significant interest and excitement because of their potential to revolutionise various fields, including medical education for aspiring physicians. Although medical students undergo a demanding educational process to become competent health care professionals, the emergence of LLMs presen ts a promising solution to challenges like information overload, time constraints and pre ssure on clinical educators. However, inte - grating LLMs into medical education raises critical concerns and challenges for educators, professionals and students. This systematic review aims to explore LLM applications in medical education, specifically their impac t on medical students' learning experiences. Methods: A systematic search was performed in PubMed, Web of Science and Embase for articles discussing the applications of LLMs in medical education using selected keywords related to LLMs and medical education, from the time of ChatGPT's debut until February 2024. Only articles available in full text or English were reviewed. The credibility of each study was critically appraised by two independent reviewers. Results: The systematic review identified 166 studies, of which 40 were found by review to be relevant to the study. Among the 40 relevant studies, key themes included LLM capabilities, benefits such as personalised learning and challenges regarding content accuracy. Importantly, 42.5% of these studies specifically evalu - ated LLMs in a novel way, including ChatGPT, in contexts such as medical exams and clinical/biomedical information, highlighting their potential in replicating human - level performance in medical knowledge. The remaining studies broadly discussed the pro - spective role of LLMs in med",
    "The ethics of ChatGPT in medicine and healthcare: a systematic review on Large Language Models (LLMs)": "With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare. Despite potential bene\ufb01ts, researchers have underscored various ethical implications. While individual instances have garnered attention, a systematic and comprehensive overview of practical applications currently researched and ethical issues connected to them is lacking. Against this background, this work maps the ethical landscape surrounding the current deployment of LLMs in medicine and healthcare through a systematic review. Electronic databases and preprint servers were queried using a comprehensive search strategy which generated 796 records. Studies were screened and extracted following a modi\ufb01ed rapid review approach. Methodological quality was assessed using a hybrid approach. For 53 records, a meta - aggregative synthesis was performed. Four general \ufb01elds of applications emerged showcasing a dynamic exploration phase. Advantages of using LLMs are attributed to their capacity in data analysis, information provisioning, support in decision - making or mitigating information loss and enhancing information accessibility. However, our study also identi\ufb01es recurrent ethical concerns connected to fairness, bias, non - male\ufb01cence, transparency, and privacy. A distinctive concern is the tendency to produce harmful or convincing but inaccurate content. Calls for ethical guidance and human oversight are recurrent. We suggest that the ethical guidance debate should be reframed to focus on de\ufb01ning what constitutes acceptable human oversight across the spectrum of applications. This involves considering the diversity of settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare. Additionally, critical inquiry is needed to evaluate the necessity and justi\ufb01cation of LLMs\u2019 current experimental use."
  }
}