{
    "CS & SE": {
        "SQL-PaLM: Improved large language model adaptation for Text-to-SQL (extended)": "Text-to-SQL, the process of translating natural language into Structured Query Language (SQL), represents a transformative application of large language models (LLMs), potentially revolutionizing how humans interact with data. This paper introduces the SQL-PaLM framework, a comprehensive solution for understanding and enhancing Text-to-SQL using LLMs, using in the learning regimes of few-shot prompting and instruction fine-tuning. With few-shot prompting, we explore the effectiveness of consistency decoding with execution-based error filtering. With instruction fine-tuning, we delve deep in understanding the critical paradigms that influence the performance of tuned LLMs. In particular, we investigate how performance can be improved through expanded training data coverage and diversity, synthetic data augmentation, and integrating query-specific database content. We propose a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Additionally, we tackle the practical challenge of navigating intricate databases with a significant number of tables and columns, proposing efficient techniques for accurately selecting relevant database elements to enhance Text-to-SQL performance. Our holistic approach yields substantial advancements in Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.",
        "RestGPT: Connecting Large Language Models with Real-World RESTful APIs": "Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a. more realistic scenario by connecting LLMs with RESTful APIs, which adhere to. the widely adopted REST software architectural style for web service development.. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to. achieve impressive results in complex tasks and has strong robustness, which paves. a new way towards AGI.",
        "Personality Traits in Large Language Models": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor. determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of. such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under. specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be. shaped along desired dimensions to mimic specific human personality profiles. We discuss application and. ethical implications of the measurement and shaping method, in particular regarding responsible AI.",
        "LLM As DBA": "Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bor, a LLM-based database administrator that can con-. tinuously acquire database maintenance experience from textual. sources, and provide reasonable, well-founded, in-time diagnosis. and optimization advice for target databases. This paper presents a. revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs.. Our preliminary experimental results that D-BoT can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
        "PENTEsTGPT: Evaluating and Harnessing Large Language Models for Automated Penetration Testing": "Penetration testing, a crucial industrial practice for ensur-. ing system security, has traditionally resisted automation due to the extensive expertise required by human professionals.s. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this work,. we establish a comprehensive benchmark using real-world penetration testing targets and further use it to explore the capabilities of LLMs in this domain. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining a whole context of the overall testing scenario.  \n\nBased on these insights, we introduce PeNTEsTGPT, an LLM-empowered automated penetration testing framework that leverages the abundant domain knowledge inherent in LLMs. PENTEsTGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PeNTEsTGPT not only outperforms LLMs with a task-completion increase of. $228.6\\%$ compared to the GPT-3.5 model among the benchmark targets, but also proves effective in tackling real-world penetration testing targets and CTF challenges. Having been. open-sourced on GitHub, PeNTEsTGPT has garnered over. 6,200 stars in 9 months and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",
        "CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes": "Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students' usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.",
        "RAH! $\\dot{\\frac{b}{10}}$ RecSys-Assistant-Human: A Human-Centered Recommendation Framework with LLM Agents": "The rapid evolution of the web has led to an exponential growth in content. Recommender systems play a crucial role in Human-Computer Interaction (HCI) by tailoring content based on individual preferences. Despite their importance, challenges persist in balancing recommendation accuracy with user satisfaction, addressing biases while preserving user privacy, and solving cold-start problems in cross-domain situations. This research argues that addressing these issues is not solely the recommender systems' responsibility, and a human-centered approach is vital. We introduce the RAH Recommender system, Assistant, and Human) framework, an innovative solution with LLM-based agents such as Perceive, Learn, Act, Critic, and Reflect, emphasizing the alignment with user personalities. The framework utilizes the Learn-Act-Critic loop and a reflection mechanism for improving user alignment. Using the real-world data, our experiments demonstrate the RAH framework's efficacy in various recommendation domains, from reducing human burden to mitigating biases and enhancing user control. Notably, our contributions provide a human-centered recommendation framework that partners effectively with various recommendation models.",
        "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA.": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA.  \n\nHaoyuan $\\mathbf{W}\\mathbf{u}^{\\dagger}$ ,Zhuolun $\\mathrm{He}^{\\dagger}$ , Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng, Bei Yu  \n\nAbstract-The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by an LLM, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDsII) by effectively managing task decomposition, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other similar LLMs.  \n\nIndex Terms-Electronic design automation, large language models, machine learning algorithms.",
        "RecMind: Large Language Model Powered Agent For Recommendation": "While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and finetune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLMpowered autonomous recommender agent, RecMind, which is capable of leveraging external. knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM \"self-inspires\" to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning. for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various. tasks and achieves comparable performance to a fully trained recommendation model P5.",
        "Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations": "Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing. remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called InteRecAgent, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender sys-. tem, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent."
    },
    "Documentation and data management Experiment Assistant": {
        "Emergent autonomous scientific research capabilities of large language models.": "Transformer-based large language models are rapidly advancing in the field of machine. learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models. to perform various tasks and reason about their choices. In this paper, we present an. Intelligent Agent system that combines multiple large language models for autonomous. design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being. the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the. safety implications of such systems and propose measures to prevent their misuse..",
        "Augmenting large language models with chemistry tools": "Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Publicly available code can be found at https://github.com/ur-whitelab/chemcrow-public.",
        "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/3716": "AI and the transformation of social science research  \n\nArticle in Science $\\cdot$ June 2023   \nDOI: 10.1126/science.adi1778"
    },
    "Industrial automation": {
        "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs": "Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-. specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different im-. plementation or working mechanisms, they are not easily accessible or compatible. with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing. foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.",
        "Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems": "Large Language Models (LLMs) have shown great potential in solving complex problems in various fields, including oil and gas engineering and other industrial engineering disciplines like factory automation, PLC. programming etc. However, automatic identification of strong and weak solutions to fundamental physics equations governing several industrial. processes remain a challenging task. This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively. The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.",
        "Towards autonomous system: flexible modular production system enhanced with large language model agents": " \n\nAbstract -- In this paper, we present a novel framework that. combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. We retrofit the automation system for a modular production facility and create executable control interfaces of fine-granular functionalities and coarsegranular skills. Low-level functionalities are executed by automation components, and high-level skills are performed by automation modules. Subsequently, a digital twin system is. developed, registering these interfaces and  containing additional descriptive information about the production system. Based on the retrofitted automation system and the created digital twins, LLM-agents are designed to interpret descriptive information in the digital twins and control the physical system through service interfaces. These LLM-agents serve as intelligent agents on different levels within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents. orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production process, and execute the operations. This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work. Demos at: https://github.com/YuchenXia/GPT4IndustrialAutomation  \n\nKeywords- autonomous system, intelligent agent, GPT, digital twin, Asset Administration Shell, smart factory"
    },
    "Jurisprudence": {
        "Blind Judgement: Agent-Based Supreme Court Modelling With GPT": "We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the realworld Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal. conservatism & liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.",
        "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model": "Al legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of Al-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by $7.73\\%$ in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case. consultations, demonstrating our robust capability for legal consultation.  \n\nKeywords Artificial Intelligence in Law, Mixture of Experts, Large Language Model, Knowledge Graph, Legal Technology  \n\nLegal services play a crucial role in protecting individual rights and maintaining social fairness1-3. However, the limited availability of legal professionals and the high cost of their services often restrict access to these services, particularly in China, with its vast population and extensive social interactions. Statistics show that Chinese legal aid centers have accepted 80o,000 cases over the past seven years, aiding over 6 million people, yet annually, only one-quarter of the 700,000 cases received can be processed4. This gap in legal service provision deeply impacts justice and equity, especially for those lacking the resources to effectively navigate the legal system. This raises a crucial question: can we establish an automated legal assistant to address these challenges?  \n\nIn recent years, the efficacy of LLMs has been validated across multiple scientific fields, encompassing natural language processing5.6, biochemistry7-14, and the medical field15-2. LMs also offer potential solutions t the challenges in legal services. Popular models like ChatGPT23 and the LLaMA24 series, along with other general-purpose25-30 or law-specific models,31,32 can respond to user inputs based on their internal legal knowledge repositories and provide advisory recommendations. However, the inherent hallucination issues in LLMs pose potential risks in their application to legal domains33 since they operate at the level of word distributions rather than validated facts34. The knowledge generated by these models is often incomplete or outdated, leading them to produce illusions that, although seemingly relevant, may be misleading or incorrect33,35 .  \n\nTo address the mentioned issues, this paper designs Chatlaw, a multi-agent virtual legal assistant based on a multi-expert large language model. Chatlaw effectively mitigates hallucination issues through key aspects of data quality, model optimization, and consulting processes. Initially, we create a high-quality legal dataset through multiple screenings and integrate similar advisory knowledge into a knowledge graph to ensure data accuracy. Next, we expand from a single-expert model to an. MoE model, increasing the parameter space to allow different experts to handle specific advisory tasks, thus enhancing the. accuracy of the legal consult. Building on this, we emulate the service workflows of real law firms and develop a set of SOP for multi-agent collaboration. This SOP includes four independent intelligent agent roles responsible for initial information. gathering, in-depth material research, legal advice, and final consultation report writing. This procedural operation ensures each step of information processing is efficient and accurate, significantly enhancing the quality of legal services and client venting inconsistencies and accumulation of hallucinations in handling c  \n\nOur model demonstrates superior performance in various evaluations, surpassing existing large language models, including GPT $.4^{23}$ . Specifically, it outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by $7.73\\%$ in accuracy and 11 points, respectively, and receives the highest scores in real-case evaluation feedback from legal experts on 4 dimensions, the completeness, correctness, guidance and authority ."
    },
    "Natural Science education": {
        "Emergent autonomous scientific research capabilities of large language models.": "Transformer-based large language models are rapidly advancing in the field of machine. learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models. to perform various tasks and reason about their choices. In this paper, we present an. Intelligent Agent system that combines multiple large language models for autonomous. design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being. the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the. safety implications of such systems and propose measures to prevent their misuse..",
        "Augmenting large language models with chemistry tools": "Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Publicly available code can be found at https://github.com/ur-whitelab/chemcrow-public.",
        "Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics Melanie Swan,a Takashi Kido,b Eric Roland, Renato P. dos Santosd": "The innovation in generative AI could be further accelerated with more-readily usable and evaluable mathematics as part of the computational infrastructure. Beyond human-AI chat. interaction, LLM (large language model)-based mathematical analysis tools are emerging in software programming, algorithm discovery, and automated theorem proving, but have not yet been widely applied to genomics. Towards disease-solving, this work introduces Math Agents and the mathematical embedding (vector-space representation of an equation as a data string) as. new \"Moore's Law of Mathematics\" entries. The project consists of a GPT-based workflow to extract equations from published literature PDFs with Mathpix OCR and process them into LaTeX and Python embeddings. There are many ways to represent equations digitally, but few. automated means for evaluating large bodies of equations (mathematical ecologies/mathscapes).  \n\nThe important result of LLMs is that they are a linguistic user interface, a language-based access. tool, via natural language for human-AI chat, but more extensively, via formal languages for atscale AI-aided build-out of the computational infrastructure. AI tools are suggested as although the possibility space of natural language is relatively finite, formal possibility spaces are infinite (e.g. the programmatic space of algorithms, the mathematics space of theorems, and the computational complexity space of quantum-classical-relativistic classes).  \n\nWhereas humans interact with natural language, Math Agents interact with math, the implication.   \nof which could be a shift from \"big data' to \"big math' as a higher-order lever for interacting.   \nwith reality. Natural language as a language is flexible and open to contextual interpretation;.   \nmathematics as a language has well-formedness properties subject to proof. Hence, mathematical use cases beyond math-as-math could include high-validation math-certified icons (by analogy to.   \ngreen seals) towards AI alignment aims of serving humanity in the broadest possible ways..  \n\nThe current project develops a theoretical model for the deployment of Math Agents and mathematical embeddings to the information systems biology problem of aging, applying multiscalar physics mathematics (elucidating near-far entropic correlations in systems) to disease model mathematics and whole-human genomic data. Generative AI with episodic memory (per file dating/time-stamping) could assess causal relations in longitudinal personal health dossiers, deployed via SIR (sustaining, intervening, recovering) compartmental Precision Health models. In the short term, genomic variant and expression data is indicated for practical application to the unresolved challenge of Alzheimer's disease as the top-five human pathology with no survivors..  \n\nKeywords: math agent, mathematical embedding, equation cluster, mathematical ecology, LLMs,. generative AI, cognitive architecture, computational infrastructure, human-AI entities, genomics,. information system biology, Alzheimer's disease, personal health dossier, SIR, precision health",
        "CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes": "Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students' usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students."
    },
    "Political Science and Economy": {
        "Out of One, Many: Using Language Models to Simulate Human Samples": "We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \"algorithmic bias\" within one such tool- the GPT-3 language model- is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \"silicon samples\" by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",
        "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?\\*.": "Newly-developed large language models (LLM)-because of how they are trained and designed-are implicit computational models of humans a homo silicus. LLMs can be used like economists use homo economicus: they can be given endowments, information,. preferences, and so on, and then their behavior can be explored in scenarios via simulation. Experiments using this approach, derived from Charness and Rabin (2002), Kahneman,. Knetsch and Thaler (1986), and Samuelson and Zeckhauser (1988) show qualitatively similar results to the original, but it is also easy to try variations for fresh insights. LLMs could allow researchers to pilot studies via simulation first, searching for novel social science insights to test in the real world.."
    },
    "Psychology": {
        "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior.. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and. illustrate its use to compare how well different language models are able to reproduce classic economic,. psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram. Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a \"hyper-accuracy distortion\" present in some language. models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.",
        "Can Large Language Models Transform Computational Social Science?": "Can Large Language Models Transform Computational Social Science?  \n\nCaleb Ziems\\*t Stanford University  \n\nWilliam Held. Georgia Institute of Technology  \n\nOmar Shaikh Stanford University  \n\nJiaao Chen Georgia Institute of Technology  \n\nZhehao Zhang Dartmouth College  \n\nDiyi Yang\\*\\* Stanford University  \n\nLarge Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CsS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting. best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zeroshot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans..",
        "Playing repeated games with Large Language Models": "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct. families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way. for a behavioral game theory for machines..",
        "Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support": "Conversational agents powered by large language models (LLM) have increasingly been utilized in the realm of mental well-being support. However, the implications and outcomes associated with their usage in such a critical field remain somewhat ambiguous and unexplored. We conducted a qualitative analysis of 120 posts, encompassing. 2917 user comments, drawn from the most popular subreddit focused on mental health support applications. powered by large language models (u/Replika). This exploration aimed to shed light on the advantages and potential pitfalls associated with the integration of these sophisticated models in conversational agents intended for mental. health support. We found the app (Replika) beneficial in offering on-demand, non-judgmental support, boosting user confidence, and aiding self-discovery. Yet, it faced challenges in filtering harmful content, sustaining consistent communication, remembering new information, and mitigating users' overdependence. The stigma attached further risked isolating users socially. We strongly assert that future researchers and designers must thoroughly evaluate the appropriateness of employing LLMs for mental well-being support, ensuring their responsible and effective. application."
    },
    "Research Assistant": {
        "Can Large Language Models Transform Computational Social Science?": "Can Large Language Models Transform Computational Social Science?  \n\nCaleb Ziems\\*t Stanford University  \n\nWilliam Held. Georgia Institute of Technology  \n\nOmar Shaikh Stanford University  \n\nJiaao Chen Georgia Institute of Technology  \n\nZhehao Zhang Dartmouth College  \n\nDiyi Yang\\*\\* Stanford University  \n\nLarge Language Models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the Computational Social Science (CsS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting. best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zeroshot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans..",
        "Can Generative Al improve social science?": "Can Generative Al improve social science?  \n\nChristopher A. Baila,b,c,1  \n\nEdited by David Lazer, Northeastern University, Boston, MA; received September 7, 2023; accepted April 5, 2024, by Editorial Board Member Mark Granovetter  \n\nGenerative Al that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative Al has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative Al. I examine how bias in the data used to train these tools can negatively impact social science research-as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of Al will require deeper understanding of the social forces that guide human behavior.  \n\nGenerative Al | computational social science | agent-based model | survey research  algorithmic bias.  \n\nGenerative Al-technology capable of producing realistic text, images, music, and other creative forms-continues to captivate large audiences. Many speculate such technology. will impact a range of industries and scientific disciplines-- from creative and legal writing to computational biology. Yet sociologists, political scientists, economists, and other social. scientists are only beginning to explore how Generative Al will transform their research. In this article, I argue these tools may advance the scale, scope, and speed of social science research--and may enable new forms of scientific inquiry. as well. At the same time, I assess the many limitations of Generative Al for social science research-and discuss how scholars can mitigate risks while exploring this promising new technology.  \n\nIn the first section of this article, I provide a brief history of Generative Al for social scientists. In the second section, I ask whether Generative Al can effectively simulate human behavior for the purposes of social science research. I assess whether these tools can be useful for survey research, or creating experimental primes within online experiments. Next, I review recent studies that employ Generative Al models to simulate dynamic human behaviors. These include experiments where human respondents interact with Generative Al, or simulations where researchers prompt models to interact with each other to study emergent group behaviors. I argue such research may help social scientists begin to reverse engineer the \"social sense\" of human beings--or how we create shared understandings of acceptable behavior in different social milieux. Finally, I argue Generative Al has the potential to transform automated text analysis. Since Generative Al tools can analyze very large groups of documents in many different languages with great speed, I propose they may significantly expand the range of research questions that social scientists can study.  \n\nIn the third section of this article, I turn to the various limitations and potential dangers associated with Generative Al. Much of the public discourse surrounding this new technology focuses on the possibility of a \"singularity\" where Al models supersede human intelligence and threaten our well-being. Many scholars believe such concerns eschew welldocumented social harms that are already occurring in the short term (1). These include the tendency of Generative Al to exhibit strong bias against stigmatized groups, spread misinformation, and potentially exacerbate social inequality or climate change-among other negative outcomes. I discuss how these issues may negatively impact the quality, efficiency, interpretability, and replicability of social science research as well--and generate new questions about ethics and the protection of human subjects. I also evaluate the potential of these models to generate and disseminate \"junk science\" which could impede scientific inquiry for years to come. Mitigating each of these risks is challenging, I argue, because the processes used to train Generative Al are largely. opaque--and accurate tools for detecting Al-generated content are not yet effective at scale..  \n\nIn the final section of this article, I argue that social scientists can address many of the challenges of research with Generative Al by creating our own open-source infrastructure (2). By developing our own Generative Al models, social scientists can more effectively diagnose how. the model training process impacts scientific analysis of human behavior and ensure these new tools evolve according to the interest of science, and not only the corporations that produce many of the most popular models at present. Most importantly, I argue open-source infrastructure could create. a community of scholars that work to identify best practices for research with Generative Al, prevent these tools from reproducing the academic caste system, and allow social scientists to develop solutions to future challenges and prevent these tools from being repurposed for malicious purposes.  \n\nSeveral caveats are in order. First, my analysis is limited to social science and thus does not engage with the many. different ways Generative Al might shape other fields. Second, I focus on the impact of Generative Al on scientific research, and not its broader impact on social life--a topic that is certainly worthy of another analysis. Third, the field of Generative Al research is changing so rapidly that any. attempt to take stock of its potential will become out of date quickly-as well as information about its possible risks or dangers. Indeed, many of the studies I discuss below are preprints that have not yet undergone rigorous peer review, and may therefore fail to replicate. I therefore urge. readers to take caution in evaluating the potential of the research techniques described below, which may yet be judged scientifically unsound, unethical, or both through a more systematic future review. Third, I do not provide a technical discussion of how Generative Al models work, since these are broadly available elsewhere (3). Instead of a \"user's guide\" for Generative Al in social science research, I hope. to inspire ongoing dialog among researchers about how this new technology should be used to study human behavior in different settings."
    },
    "Robotics & embodied ai": {
        "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's hands and eyes, while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",
        "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments.? Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose \"Describe, Explain, Plan and Select' (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing selfexplanation of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal selector, which is a trainable module that. ranks parallel candidate sub-goals based on the estimated steps of completion,. consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish $^{70+}$ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the ObtainDiamond grand challenge with our approach. The code is released at https://qithub.com/CraftJarvis/MC-Planner.",
        "Plan, Eliminate, and Track. Language Models are Good Teachers for Embodied Agents.": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nontext environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it.  \n\nWe propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant $15\\%$ improvement over SOTA for generalization to human goal specifications.",
        "TidyBot: Personalized Robot Assistance with Large Language Models.": "For a robot to personalize physical assistance effectively, it must learn user preferences that can. be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that. can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves $91.2\\%$ accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on. a real-world mobile manipulator called TidyBot, which successfully puts away $85.0\\%$ of objects in real- world test scenarios.  \n\nKeywords: service robotics, mobile manipulation, large language models",
        "Language Models Meet World Models:. Embodied Experiences Enhance Language Models": "While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household ac-. tivities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse. embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences. through both goal-oriented planning and random exploration. These experiences. are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical elastic weight consolidation (EwC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show. our approach substantially improves base LMs on 18 downstream tasks by $64.28\\%$ on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our. approach match or even outperform much larger LMs (e.g., ChatGPT). 1",
        "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach": "Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an embodied agent in solving complex sequential decision making tasks by providing high-level instructions. However, interactions with LLMs can be time-consuming. In many practical scenarios, it requires a significant amount of storage space that can only be deployed on remote cloud servers. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable intelligent costeffective interactions between a down stream task oriented agent and an LLM. We find that this problem can be naturally formulated by a Markov decision process (MDP), and propose When2Ask, a reinforcement learning based approach that learns when it is necessary to query LLMs for high-level instructions to accomplish a target task. On one side, When2Ask discourages unnecessary redundant interactions, while on the other side, it enables the agent to identify and follow useful instructions from the LLM. This enables the agent to halt an ongoing plan and transition to a more suitable one based on new environmental observations. Experiments on MiniGrid and Habitat environments that entail planning sub-. goals demonstrate that When2Ask learns to solve target tasks with only a few necessary interactions with the LLM, significantly reducing interaction costs in testing environments compared with baseline methods. Our code is available at: https://github.com/ZJLABAMMI/LLM4RL.",
        "Large Language Models Are Semi-Parametric Reinforcement Learning Agents": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model). agent framework is proposed as REMEMBERER. By equipping the LLM with a long-. term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training. sets exceed the prior SOTA by $4\\%$ and $2\\%$ for the success rate on two task sets and. demonstrate the superiority and robustness of REMEMBERER.1",
        "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models  \n\nZhao Mandi Shreeya Jain Shuran Song Columbia University Columbia University Columbia University  \n\nWe propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level. communication and low-level path planning. Robots are equipped with LLMs to. discuss and collectively reason task strategies. They then generate sub-task plans. and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment,. such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied. by a text-only dataset for agent representation and reasoning. We experimentally. demonstrate the effectiveness of our approach - it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility - in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website project-roco.github.io for videos and code..  \n\n  \nFigure 1: We propose RoCo, a unified approach for multi-robot collaboration that leverages LLMs for both high-level task coordination and low-level motion planning. We demonstrate its utility on RoCoBench, a benchmark we introduce that includes a diverse set of challenges in collaboration task scenarios.",
        "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning": "Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page this https URL.",
        "TOWARDS A UNIFIED AGENT WITH FOUNDATION MODELS": "Language Models and Vision Language Models have recently demonstrated un-. precedented capabilities in terms of understanding human intentions, reasoning,. scene understanding, and planning-like behaviour, in text form, among many oth-. ers. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language. as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience. data, scheduling skills, and learning from observations, which traditionally require. separate, vertically designed algorithms. We test our method on a sparse-reward. simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate. how to reuse learned skills to solve novel tasks or imitate videos of human experts.",
        "ProAgent: Building Proactive Cooperative Agents with Large Language Models": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer. the intentions of teammates from observations. It then up-. dates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high. degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of. ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding. $i0\\%$ compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent. github.io."
    },
    "Social Simulation": {
        "Chain of Hindsight aligns Language Models with Feedback.": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.'",
        "Generative Agents: Interactive Simulacra of Human Behavior": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate. conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those. memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents. to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents. autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right. time. We demonstrate through ablation that the components of our agent architecture-observation, planning, and reflection-each contribute critically to the believability of agent behavior. By fusing. large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling. believable simulations of human behavior..",
        "The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents": "\n\nAbstract  \n\nDevelopmental psychologists have long-established socio-cognitive abilities as fundamental to human intelligence and development. These abilities enable individuals to enter, learn from, and contribute to a surrounding culture. This drives the process of cumulative cultural evolution, which is responsible for humanity's most remarkable achievements. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture as well. We draw inspiration from the work of Michael Tomasello and Jerome Bruner, who studied socio-cognitive development and emphasized the influence of a cultural environment on intelligence. We outline a broader set of concepts than those currently studied in AI to provide a foundation for research in artificial social intelligence. Those concepts include social cognition (joint attention, perspective taking), communication, social learning, formats, and scaffolding. To facilitate research in this domain, we present The SocialAI school - a tool that offers a customizable parameterized suite of procedurally generated environments. This tool simplifies experimentation with the introduced concepts. Additionally, these environments can be used both with multimodal RL agents, or with pure-text Large Language Models (LLMs) as interactive agents. Through a series of case studies, we demonstrate the versatility of the SocialAI school for studying both RL and LLM-based agents. Our motivation is to engage the AI community around social intelligence informed by developmental psychology, and to provide a user-friendly resource and tool for initial investigations in this direction. Refer to the project website for code and additional information: https://sites.google.com/view/socialai-school.",
        "Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks": "Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks  \n\nSIYU LI, Sichuan University, China JIN YANG\\*, Sichuan University, China KUI ZHAO, Sichuan University, China  \n\nAs the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and userfriendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings. indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting. certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMsdriven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing. the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide. primary insights for the research and governance of LLMs-driven social bots within the research community.  \n\nCCS Concepts: $\\bullet$ Human-centered computing $\\rightarrow$ Empirical studies in collaborative and social computing; $\\bullet$ Information systems $\\rightarrow$ Web mining.  \n\nAdditional Key Words and Phrases: Large Language Models, Social Bots, Human-bot Interaction, Online Social Networks, Toxic Behaviors",
        "$\\mathbf{S}^{3}$ : Social-network Simulation System with. Large Language Model-Empowered Agents": "Simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the. human-like capabilities of large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the. $\\mathrm{S^{3}}$ system (short for Social network Simulation System). Adhering to the widely employed agent-based simulation paradigm, we employ fine-tuning and prompt engineering techniques to. ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the. ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We an-. ticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science..",
        "Quantifying the Impact of Large Language Models on Collective": "The process of opinion expression and exchange is a critical component of democratic societies. As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned. However, the knowledge about how LLMs affect the process of opinion expression and exchange of social opinion networks is very limited. Here, we create an opinion network dynamics model to encode the opinions of LLMs, cognitive acceptability and usage strategies of individuals, and simulate the impact of LLMs on opinion dynamics in a variety of scenarios. The outcomes of the simulations inform about effective demand-oriented opinion network interventions. The results from this study. suggested that the output opinion of LLMs has a unique and positive effect on the collective opinion difference. The marginal effect of cognitive acceptability on collective opinion formation is nonlinear and shows a decreasing trend. When people partially rely on LLMs, the exchange process of opinion becomes more intense and the diversity of opinion becomes more favorable. In fact, there is $38.6\\%$ more opinion diversity when people all partially rely on LLMs, compared to prohibiting. the use of LLMs entirely. The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1. Our experiments also find that introducing extra agents with opposite/neutral/random opinions, we can effectively mitigate the impact of biased/toxic output from LLMs. Our findings provide valuable insights into opinion dynamics in the age of LLMs, highlighting the need for customized interventions tailored to specific scenarios to address the drawbacks of improper output and use of LLMs..",
        "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation": "With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-. based evaluation, where LLM agents complete tasks in a simulated environment, is a onefor-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https: / /agentsims. com.."
    }
}