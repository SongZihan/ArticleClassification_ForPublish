{"Human-Crafted": {"UNIFIED QA: Crossing Format Boundaries with a Single QA System": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format - specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre - trained QA model, UNIFIED QA, that performs well across 20 QA datasets spanning 4 diverse formats. UNIFIED QA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIED QA performs surprisingly well, showing strong generalization from its out - of - format training data. Finally, fine - tuning this pre - trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense QA datasets, establishing UNIFIED QA as a strong starting point for building QA systems.", "MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models\u2019 pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pre-trained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16\u00d7 its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6\u00d7 its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts a", "UNIFIED SKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models": "Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the U NIFIED SKG framework, which uni\ufb01es 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use U NIFIED SKG to benchmark T5 with different sizes and show that T5, with simple modi\ufb01cations when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task pre\ufb01x-tuning improves the performance on most tasks, largely improving the overall performance. U NIFIED SKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use U NIFIED SKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. U NIFIED SKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg .", "Training language models to follow instructions with human feedback": "Making language models bigger does not inherently make them better at following a user\u2019s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by \ufb01ne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to \ufb01ne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further \ufb01ne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT . In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that \ufb01ne-tuning with human feedback is a promising direction for aligning language models with human intent.", "SUPER -NATURAL INSTRUCTIONS : Generalization via Declarative Instructions on 1600+ NLP Tasks": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we \ufb01rst introduce S UPER -NATURAL INSTRUCTIONS, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classi\ufb01cation, extraction, in\ufb01lling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build T k-INSTRUCT, a transformer model trained to follow a variety of in-context instructions (plain language task de\ufb01nitions or k-shot examples). Our experiments show that T k-INSTRUCT outperforms existing instruction-following models such as Instruct-GPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.", "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions : a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open - source manually - curated datasets, surpassing the performance of models such as T0++ and Tk - Instruct across various benchmarks. These results demonstrate the potential of model - generated data as a cost - effective alternative to crowdsourcing for dataset expansion and diversification.", "The Flan Collection: Designing Data and Methods for E\ufb00ective Instruction Tuning": "We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 models (Chung et al., 2022). Through careful ablation studies on the Flan Collection of instruction tuning tasks and methods ,weteaseapartthee\ufb00ectofdesigndecisionsthatenableFlan-T5 to outperform prior work by 3-17%+ across evaluation settings. We \ufb01nd task balancing and enrichment techniques are overlooked but critical to e\ufb00ective instruction tuning, and in particular, training with mixed promptsettings(zero-shot,few-shot,andchain-of-thought)actuallyyieldsstronger(2%+)performance inallsettings. In further experiments, we show Flan-T5 requires less \ufb01netuning to converge higher and faster than T5 on single downstream tasks\u2014motivating instruction-tuned models as more computationally-e\ufb03cientstartingcheckpointsfornewtasks. Finally,toaccelerateresearchoninstructiontuning,wemake the Flan 2022 collection of datasets, templates, and methods publicly available.", "OpenAssistant Conversations - Democratizing Large Language Model Alignment": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine - tuning ( SFT) and reinforcement learning from human feedback ( RLHF ) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state - of - the - art alignment techniques like RLHF rely on high - quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large - scale alignment, we release OpenAssistant Conversations, a human - generated, human - annotated assistant - style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd - sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code2and data3under a fully permissive licence. A list of contributors who have chosen to be acknowledged by name can be found at https://open - assistant.io/contributors.", "CHINESE OPEN INSTRUCTION GENERALIST : A PRELIMINARY RELEASE": "Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT (Ouyang et al., 2022) and ChatGPT3. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting Chinese OpenInstruction Generalist ( COIG ) corpora are available in Huggingface4and Github5, and will be continuously updated.", "LIMA: Less Is More for Alignment": "Largelanguagemodelsaretrainedintwostages: (1)unsupervisedpretrainingfrom rawtext,tolearngeneral-purposerepresentations,and(2)largescaleinstruction tuning and reinforcement learning, to better align to end tasks and user preferences. Wemeasure therelativeimportanceofthesetwo stagesbytrainingLIMA,a65B parameter LLaMa language model \ufb01ne-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learningorhumanpreferencemodeling. LIMAdemonstratesremarkablystrong performance,learningtofollowspeci\ufb01cresponseformatsfromonlyahandfulof examples in the training data, including complex queries that range from planning tripitinerariestospeculatingaboutalternatehistory. Moreover,themodeltends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "IMPROVING DATA EFFICIENCY VIA CURATING LLM-D RIVEN RATING SYSTEMS": "Instruction tuning is critical for adapting large language models (LLMs) to down- stream tasks, and recent studies have demonstrated that small amounts of human- curated data can outperform larger datasets, challenging traditional data scal- ing laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and bi- ases, even in powerful models like GPT-4. In this work, we introduce DS2, a Diversity-aware Score curation method for DataSelection. By systematically modeling error patterns through a score transition matrix, DS2corrects LLM- based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full- scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sam- ple size (1k samples). These findings challenge conventional data scaling as- sumptions, highlighting that redundant, low-quality samples can degrade per- formance and reaffirming that \u201cmore can be less.\u201d The code is available at: https://github.com/UCSC-REAL/DS2 ."}, "Synthetic Data\uff08Distillation\uff09": {"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions : a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open - source manually - curated datasets, surpassing the performance of models such as T0++ and Tk - Instruct across various benchmarks. These results demonstrate the potential of model - generated data as a cost - effective alternative to crowdsourcing for dataset expansion and diversification.", "INSTRUCTION TUNING WITH GPT-4": "Prior work has shown that \ufb01netuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the \ufb01rst attempt to use GPT-4 to generate instruction-following data for LLM \ufb01netuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.", "WizardLM : EMPOWERING LARGE PRE-TRAINED LANGUAGE MODELS TO FOLLOW COMPLEX INSTRUCTIONS": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM . Both automatic and human evaluations consistently indicate that WizardLM outperforms baselines such as Alpaca (trained from Self-Instruct) and Vicuna (trained from human-created instructions). The experimental results demonstrate that the quality of instruction-following dataset crafted by Evol-Instruct can significantly improve the performance of LLMs.", "LogiCoT: Logical Chain-of-Thought Instruction Tuning": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.", "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released.", "Orca: Progressive Learning from Complex Explanation Traces of GPT-4": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model\u2019s capability as they tend to learn to imitate the style, but not the reasoning process of LFMs . To address these challenges, we develop Orca, a 13 - billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT - 4 including explanation traces; step - by - step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large - scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state - of - the - art instruction - tuned models such as Vicuna - 13B by more than 100% in complex zero - shot reasoning benchmarks like Big - Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero - shot settings without CoT; while trailing behind GPT - 4. Our research indicates that learning from step - by - step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.", "WizardCoder : E MPOWERING CODE LARGE LAN- GUAGE MODELS WITH EVOL - INSTRUCT": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated remarkable performance in various code - related tasks. However, different from their counterparts in the general language modeling field, the technique of instruction fine - tuning remains relatively under - researched in this domain. In this paper, we present Code Evol - Instruct, a novel approach that adapts the Evol - Instruct method to the realm of code, enhancing Code LLMs to create novel models WizardCoder. Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS - 1000, and MultiPL - E, our models showcase outstanding performance. They consistently outperform all other open - source Code LLMs by a significant margin. Remarkably, WizardCoder 15B even surpasses the well - known closed - source LLMs, including Anthropic\u2019s Claude and Google\u2019s Bard, on the HumanEval and HumanEval+ benchmarks. Additionally, WizardCoder 34B not only achieves a HumanEval score comparable to GPT3.5 (ChatGPT) but also surpasses it on the HumanEval+ benchmark. Furthermore, our preliminary exploration highlights the pivotal role of instruction complexity in achieving exceptional coding performance.", "Textbooks Are All You Need": "We introduce phi-1 , a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1 .3B parameters, trained for 4 days on 8 A100s, using a selection of \u201ctextbook quality\u201d data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50 .6% on HumanEval and 55 .5% on MBPP. It also displays surprising emergent properties compared to phi-1-base , our model before our finetuning stage on a dataset of coding exercises, and phi-1-small , a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "Textbooks Are All You Need II: phi-1.5 technical report": "We continue the investigation into the power of smaller Transformer-based language models as initiated by TinyStories \u2013 a 10 million parameter model that can produce coherent English \u2013 and the follow-up work on phi-1 , a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate \u201ctextbook quality\u201d data as a way to enhance the learning process compared to traditional web data. We follow the \u201cTextbooks Are All You Need\u201d approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named phi-1.5 , with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, phi-1.5 exhibits many of the traits of much larger LLMs, both good \u2013such as the ability to \u201cthink step by step\u201d or perform some rudimentary in-context learning\u2013 and bad, including hallucinations and the potential for toxic and biased generations \u2013encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source phi-1.5 to promote further research on these urgent topics.", "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder , a series of Code LLMs trained with Widespread AndVersatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset , a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks."}, "Synthetic Data\uff08Self-Improvement)\uff09": {"SELF-INSTRUCT : Aligning Language Models with Self-Generated Instructions": "Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond toinstructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity,andcreativity,thereforehinderingthe generality of the tuned model. We introduce SELF-INSTRUCT , a framework for improving the instruction-following capabilities of pre-trained language models by bootstrapping off theirowngenerations. Ourpipelinegenerates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanillaGPT3, we demonstrate a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS ,onparwith the performance of InstructGPT001,1which was trained with private user data and human annotations. Forfurtherevaluation,wecuratea setofexpert-writteninstructionsfornoveltasks,andshowthroughhumanevaluationthattuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT001.SELF-INSTRUCT provides an almost annotation-free method for aligning pre-trained language models with instructions, and wereleaseourlargesyntheticdatasettofacilitate future studies on instruction tuning.2", "SELF-ALIGNMENT WITH INSTRUCTION BACKTRANS - LATION": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.", "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing ( SPIN ), which starts from a supervised fine-tuned model. At the heart ofSPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM\u2019s performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available at https://github.com/uclaml/SPIN ."}}