{
    "Aggregations": {
        "Embedding Logic Rules Into Recurrent Neural Networks": " \n\nABSTRACT Incorporating prior knowledge into recurrent neural network (RNN) is of great importance for. many natural language processing tasks. However, most of the prior knowledge is in the form of structured knowledge and is difficult to be exploited in the existing RNN framework. By extracting the logic rules from the structured knowledge and embedding the extracted logic rule into the RNN, this paper proposes. an effective framework to incorporate the prior information in the RNN models. First, we demonstrate that. commonly used prior knowledge could be decomposed into a set of logic rules, including the knowledge graph, social graph, and syntactic dependence. Second, we present a technique to embed a set of logic rules. into the RNN by the way of feedback masks. Finally, we apply the proposed approach to the sentiment. classification and named entity recognition task. The extensive experimental results verify the effectiveness of the embedding approach. The encouraging results suggest that the proposed approach has the potential for applications in other NLP tasks.  \n\n: INDEX TERMS RNN, logic rules, sentiment classification, named entity recognition.",
        "BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies": "Background: Recent studies have proposed deep learning techniques, namely recurrent neural networks, to. improve biomedical text mining tasks. However, these techniques rarely take advantage of existing domain-specific resources, such as ontologies. In Life and Health Sciences there is a vast and valuable set of such resources publicly. available, which are continuously being updated. Biomedical ontologies are nowadays a mainstream approach to formalize existing knowledge about entities, such as genes, chemicals, phenotypes, and disorders. These resources. contain supplementary information that may not be yet encoded in training data, particularly in domains with limitec labeled data.  \n\nResults: We propose a new model to detect and classify relations in text, BO-LSTM, that takes advantage of. domain-specific ontologies, by representing each entity as the sequence of its ancestors in the ontology. We. implemented BO-LSTM as a recurrent neural network with long short-term memory units and using open biomedical. ontologies, specifically Chemical Entities of Biological Interest (ChEBl), Human Phenotype, and Gene Ontology. We assessed the performance of BO-LSTM with drug-drug interactions mentioned in a publicly available corpus from an international challenge, composed of 792 drug descriptions and 233 scientific abstracts. By using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of both the detection and classification of drug-drug interactions, particularly in a document set with a limited number of annotations. We adapted an existing DDI extraction model with our ontology-based method, obtaining a higher F1 score than the original model. Furthermore, we developed and made available a corpus of 228 abstracts annotated with relations between genes and phenotypes, and demonstrated how BO-LSTM can be applied to other types of relations..  \n\nConclusions: Our findings demonstrate that besides the high performance of current deep learning techniques, domain-specific ontologies can still be useful to mitigate the lack of labeled data.  \n\nKeywords: Text mining, Drug-drug interactions, Deep learning, Long short term memory, Relation extraction"
    },
    "Modifications to DL Architecture": {
        "TREE-STRUCTURED ATTENTION WITH HIERARCHI-CAL ACCUMULATION": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with \"Hierarchical Accumulation' to encode parse tree structures into selfattention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.",
        "Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation": "Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some selfattention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntac-. tic enhancement in machine translation. The. importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on. WMT14 and WMT16 English $\\rightarrow$ German and English $\\rightarrow$ Czech language machine translation validate the RHE effectiveness.  \n\n  \nFigure 1: Rationale of the multi-head SAN. The left and middle parts are two existing SAN methods, the right one illustrates our proposed method. The colored circles represent different functions of individual heads.  \n\net al., 2018), a directional self-attention network (DiSAN) uses one to multiple positional masks to model the asymmetric attention between two elements and capture context-aware relations for all tokens. (Yang et al., 2018) modeled the local information by revising the attention distribution with a learnable Gaussian bias to focus on neighboring relations.  (Shaw et al., 2018) extended SAN to efficiently consider distinct representations of the relative linear position relations between sequence elements. However, the above approaches consider the multi-head SAN as a whole but ignore unbalanced contribution distributions between heads.",
        "MED1TRON-70B: Scaling Medical Pretraining for Large Language Models": "Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closedsource (e.g., PaLM, GPT-4) or limited in scale $\\langle\\le13\\mathrm{{B}}$ parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MeDiTron builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a $6\\%$ absolute performance gain over the best public baseline in. its parameter class and. $3\\%$ over the strongest baseline we finetuned from Llama-2.. Compared to closed-source LLMs, MED1TRON-70B outperforms GPT-3.5 and Med-PaLM and is within $5\\%$ of GPT-4 and $10\\%$ of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MeDITroN model weights to drive open-source development of more capable medical LLMs.  \n\n  \nFigure 1: MED1TRON-70B's performance on MedQA MEDITRON-70B achieves an accuracy of $70.2\\,\\%$ on USMLE-style questions in the MedQA (4 options) dataset.",
        "Aspect-Based Sentiment Analysis via Virtual Node Augmented Graph Convolutional Networks.": "Aspect-Based Sentiment Analysis via Virtual Node Augmented Graph Convolutional Networks.  \n\nRunzhong Xu()  \n\nUniversity of Nottingham, Nottingham NG7 2RD, UK xrunzhong@gmail. com  \n\nAbstract. Aspect-based sentiment analysis (ABSA) refers to a finegrained sentiment analysis task aimed at detecting sentiment polarity towards a given aspect. Recently, graph convolutional networks (GCN) integrated with dependency trees have achieved related appealing results in ABSA. Nevertheless, most existing models fail to preserve the information of the whole graph although global information can often sig-. nificantly improve their performance. To address this problem, a novel. virtual node augmented graph convolutional network (ViGCN) is proposed to further enhance the performance of GCNs in the ABSA task by adding a virtual node to the graph. The virtual node can connect to all. the nodes in the graph to aggregate global information from the entire. graph and then propagate it to each node. In particular, we construct edges between the virtual node and other nodes based on affective com-. monsense knowledge from SenticNet and the semantic-relative distances between contextual words and the aspect, effectively enhancing the col-. lected global information towards the given aspect. Extensive experiments on three benchmark datasets illustrate that the ViGCN model can beat state-of-the-art models, proving its effectiveness..  \n\nKeywords: Sentiment analysis : Opinion mining : Aspect-based sentiment analysis : Graph neural network",
        "Grammatically Derived Factual Relation Augmented Neural Machine Translation": "Transformer-based neural machine translation (NMT) has achieved state-of-the-art performance in the NMT paradigm. This method assumes that the model can automatically learn linguistic knowledge (e.g., grammar and syntax) from the parallel corpus via an attention network. However, the attention network cannot capture the deep internal structure of a sentence. Therefore, it is natural to introduce some prior knowledge to guide the model. In this paper, factual relation information is introduced into NMT as prior knowledge, and a novel approach named Factual Relation Augmented (FRA) is proposed to guide the decoder in Transformer-based NMT. In the encoding procedure, a factual relation mask matrix is constructed to generate the factual relation representation for the source sentence, while in the decoding procedure an effective method is proposed to incorporate the factual relation representation and the original representation of the source sentence into the decoder. Positive results obtained in several different translation tasks indicate the effectiveness of the proposed approach.",
        "A neuro-symbolic method for understanding free-text medical evidence": "Objective: We introduce Medical evidence Dependency (MD)-informed attention, a novel neuro-symbolic model for understanding free-text clinical trial publications with generalizability and interpretability..  \n\nMaterials and Methods: We trained one head in the multi-head self-attention model to attend to the Medical evidence Ddependency (MD) and to pass linguistic and domain knowledge on to later layers (MD informed). This MD-informed attention model was integrated into BioBERT and tested on 2 public machine reading comprehen-. sion benchmarks for clinical trial publications: Evidence Inference 2.0 and PubMedQA. We also curated a small set of recently published articles reporting randomized controlled trials on COvID-19 (coronavirus disease 2019) following the Evidence Inference 2.0 guidelines to evaluate the model's robustness to unseen data..  \n\nResults: The integration of MD-informed attention head improves BioBERT substantially in both benchmark tasks-as large as an increase of $+30\\%$ in the F1 score--and achieves the new state-of-the-art performance on the Evidence Inference 2.0. It achieves. $84\\%$ and $82\\%$ in overall accuracy and F1 score, respectively, on the un-. seen COVID-19 data.  \n\nConclusions: MD-informed attention empowers neural reading comprehension models with interpretability and generalizability via reusable domain knowledge. Its compositionality can benefit any transformer-based architecture for machine reading comprehension of free-text medical evidence..  \n\nKey words: natural language understanding, machine reading comprehension, transformer, medical evidence computing"
    },
    "Pipelines": {
        "PTR: Prompt Tuning with Rules for Text Classification": "Keywords:   \nPre-trained language models   \nPrompt tuning  \n\nRecently, prompt tuning has been widely applied to stimulate the rich knowledge in pre-trained language. models (PLMs) to serve NLP tasks. Although prompt tuning has achieved promising results on some fewclass classification tasks, such as sentiment classification and natural language inference, manually designing. prompts is cumbersome. Meanwhile, generating prompts automatically is also difficult and time-consuming. Therefore, obtaining effective prompts for complex many-class classification tasks still remains a challenge.. In this paper, we propose to encode the prior knowledge of a classification task into rules, then design sub-. prompts according to the rules, and finally combine the sub-prompts to handle the task. We name this Prompt Tuning method with Rules \"PTR'. Compared with existing prompt-based methods, PTR achieves a good tradeoff between effectiveness and efficiency in building prompts. We conduct experiments on three many-class. classification tasks, including relation classification, entity typing, and intent classification. The results show. that PTR outperforms both vanilla and prompt tuning baselines, indicating the effectiveness of utilizing rules. for prompt tuning. The source code of PTR is available at https://github.com/thunlp/PTR..",
        "Dice Loss for Data-imbalanced NLP Tasks": "Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.  \n\nIn this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sgrensen-Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.  \n\nWith the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at  \n\nTable 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.",
        "Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach": "Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.'",
        "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shufling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.  BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.'.",
        "Knowledge-enhanced Prompt-tuning for Stance Detection": "Knowledge-enhanced Prompt-tuning for Stance Detection  \n\nHU HUANG, School of Cyberspace Science and Technology, University of Science and Technology of   \nChina, China   \nBOWEN ZHANG, College of Big Data and Internet, Shenzhen Technology University, China   \nYANGYANG LI, Academy of Cyber, China   \nBAOQUAN ZHANG, YUXI SUN, and CHUYAO LUO, School of Computer Science and   \nTechnology, Harbin Institute of Technology, China   \nCHENG PENG, University of Electronic Science and Technology of China, Zhongshan Institute, Chin?  \n\nInvestigating public attitudes on social media is important in opinion mining systems. Stance detection aims to analyze the attitude of an opinionated text (e.g., favor, neutral, or against) toward a given target. Existing methods mainly address this problem from the perspective of fine-tuning. Recently, prompt-tuning has achieved success in natural language processing tasks. However, conducting prompt-tuning methods for stance detection in real-world remains a challenge for several reasons: (1) The text form of stance detection is usually short and informal, which makes it difficult to design label words for the verbalizer. (2) The tweet text may not explicitly give the attitude. Instead, users may use various hashtags or background knowledge to express stance-aware perspectives. In this article, we first propose a prompt-tuning-based framework that performs stance detection in a cloze question manner. Specifically, a knowledge-enhanced prompt-tuning framework (KEprompt) method is designed, which consists of an automatic verbalizer (AutoV) and background knowledge injection (BKI). Specifically, in AutoV, we introduce a semantic graph to build a better mapping from the predicted word of the pretrained language model and detection labels. In BKI, we first propose a topic model for learning hashtag representation and introduce ConceptGraph as the supplement of the target. At last, we present a challenging dataset for stance detection, where all stance categories are expressed in an implicit manner. Extensive experiments on a large real-world dataset demonstrate the superiority of KEprompt over state-of-the-art methods.  \n\nCCS Concepts: $\\bullet$ Information systems $\\to$ Social networks ds and Phrases: Stance detection, deep learning, prompt-tunin",
        "PAL: Program-aided Language Models": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at this http URL .",
        "Toolformer: Language Models Can Teach Themselves to Use Tools": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically,. struggle with basic functionality, such as arithmetic or factual lookup, where much. simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation. system, and a calendar. Toolformer achieves substantially improved zero-shot. performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."
    },
    "Rule-Constrained Model": {
        "Cancer Registry Coding via Hybrid Neural. Symbolic Systems in the Cross-Hospital Setting": "ABstRAcT Cancer registries are critical databases for cancer research whose maintenance requires various types of domain knowledge with labor-intensive data curation. In order to facilitate the curation process with high quality in a timely manner, we developed a hybrid neural symbolic system for cancer registry coding. Unlike previous works which mainly worked on the dataset collected from one hospital or formulated the task as text classification problems, we collaborated with two medical centers in Taiwan to compile a cross-hospital corpus and applied neural networks to extract cancer registry variables described in unstructured pathology reports along with expert systems for generating registry coding. We conducted experiments to study the feasibility of the proposed hybrid for the task of cancer registry coding and compare its performance with state-of-the-art non-hybrid approaches. Furthermore, cross-hospital experiments were performed to study the advantages and limitations of transfer learning for processing reports from different sources. The experiment results demonstrated that the proposed hybrid neural symbolic system is a robust approach which works well across hospitals and outperformed classification-based baselines by F-scores of $0.13{\\sim}0.27$ . Compared to the baseline models, the F-scores of the proposed approaches are apparently higher when fewer training instances were used. All methods benefited from the transferred parameters learned from the source dataset, but the results suggest that it is a better strategy to transfer the learned knowledge through the concept recognition task followed by the symbolic expert system to address the task of cancer registry coding.  \n\n: INDEx TERMs Electronic medical records, medical expert systems, medical information systems, natural language processing.",
        "Learning from Noisy Crowd Labels with Logics": "Learning from Noisy Crowd Labels with Logics  \n\nZhijun Chen1,2, Hailong $\\mathrm{Sun^{1,2*}}$ , Haoqian $\\mathrm{He^{1,2}}$ , Pengpeng Chen1,2,3 SKLSDE Lab, Beihang University, Beijing, China1 Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China? China's Aviation System Engineering Research Institute, Beijing, China3 {zhijunchen, sunhl, hehaoqian, chenpp} @buaa.edu.cn  \n\nAbstract-This paper explores the integration of symbolic logic knowledge into deep neural networks for learning from noisy crowd labels. We introduce Logic-guided Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic. knowledge distillation framework that learns from both noisy labeled data and logic rules of interest. Unlike traditional EM methods, our framework contains a \"pseudo-E-step'' that distills from the logic rules a new type of learning target, which is then used in the \"pseudo-M-step'' for training the classifier. Extensive evaluations on two real-world datasets for text sentiment classification and named entity recognition demonstrate that the proposed framework improves the state-of-the-art and provides. a new solution to learning from noisv crowd labels.  \n\nIndex Terms-crowdsourcing, noisy labels, weak supervision, neural-symbolic learning",
        "Leveraging Symbolic Knowledge Bases for Commonsense Natural Language Inference Using Pattern Theory.": "Leveraging Symbolic Knowledge Bases for Commonsense Natural Language Inference Using Pattern Theory.  \n\nSathyanarayanan N. Aakur D, Member, IEEE, and Sudeep Sarkar D, Fellow, IEEE  \n\nAbstract--The commonsense natural language inference (CNLI) tasks aim to select the most likely follow-up statement to a contextual description of ordinary, everyday events and facts. Current approaches to transfer learning of CNLI models across tasks require many labeled data from the new task. This paper presents a way to reduce this need for additional annotated training data from the new task by leveraging symbolic knowledge bases, such as ConceptNet. We formulate a teacher-student framework for mixed symbolic-neural reasoning, with the large-scale symbolic. knowledge base serving as the teacher and a trained CNLI model as the student. This hybrid distillation process involves two steps. The first step is a symbolic reasoning process. Given a collection of. unlabeled data, we use an abductive reasoning framework based on Grenander's pattern theory to create weakly labeled data. Pat-. tern theory is an energy-based graphical probabilistic framework for reasoning among random variables with varying dependency structures. In the second step, the weakly labeled data, along with a fraction of the labeled data, is used to transfer-learn the CNLI. model into the new task. The goal is to reduce the fraction of labeled data required. We demonstrate the efficacy of our approach by using three publicly available datasets (OpenBookQA, SWAG, and HellaSWAG) and evaluating three CNLI models (BERT, LSTM, and ESIM) that represent different tasks. We show that, on average, we achieve ${\\bf{63\\%}}$ of the top performance of a fully supervised BERT model with no labeled data. With only 1,o0o labeled samples, we can improve this performance to. $72\\%$ . Interestingly, without. training, the teacher mechanism itself has significant inference power. The pattern theory framework achieves $32.7\\%$ accuracy on OpenBookQA, outperforming transformer-based models such as GPT $(26.6\\%)$ , GPT-2 $(30.2\\%)$ , and BERT $(27.1\\%)$ by a significant margin. We demonstrate that the framework can be generalized to successfully train neural CNLI models using knowledge distillation under unsupervised and semi-supervised learning settings. Our results show that it outperforms all unsupervised and weakly supervised baselines and some early supervised approaches, while. offering competitive performance with fully supervised baselines. Additionally, we show that the abductive learning framework can be adapted for other downstream tasks, such as unsupervised semantic textual similarity, unsupervised sentiment classification, and zero-shot text classification, without significant modification to the framework. Finally, user studies show that the generated interpretations enhance its explainability by providing key insights. into its reasoning mechanism.  \n\nIndex Terms-Multiple choice CNLI, commonsense reasoning, pattern theory.",
        "A Primal-Dual Formulation for Deep Learning with Constraints": "For several problems of interest, there are natural constraints which exist over the output label space. For example, for the joint task of NER and POS labeling, these constraints might specify that the NER label 'organization' is consistent only with. the POS labels 'noun' and 'preposition'. These constraints can be a great way of injecting prior knowledge into a deep learning model, thereby improving overall. performance. In this paper, we present a constrained optimization formulation for. training a deep network with a given set of hard constraints on output labels. Our novel approach first converts the label constraints into soft logic constraints over probability distributions outputted by the network. It then converts the constrained optimization problem into an alternating min-max optimization with Lagrangian. variables defined for each constraint. Since the constraints are independent of the target labels, our framework easily generalizes to semi-supervised setting. We experiment on the tasks of Semantic Role Labeling (SRL), Named Entity Recognition (NER) tagging, and fine-grained entity typing and show that our constraints not only significantly reduce the number of constraint violations, but can also result in state-of-the-art performance."
    }
}