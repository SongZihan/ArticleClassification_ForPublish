{"Chatbot": {"LaMDA: Language Models for Dialog Applications": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model\u2019s responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.", "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long - term memory, and having been trained on a large number of user de\ufb01ned tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open - domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever - improving responsible agents that learn through interaction.", "Improving alignment of dialogue agents via targeted human judgements": "We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.", "ChatGPT: Jack of all trades, master of none": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However,theexistingstudiesaremostlynon-automatedandtestedonaverylimitedscale.Inthiswork,weexaminedChatGPT\u2019scapabilitieson25diverseanalyticalNLPtasks,mostofthemsubjectiveeven to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses.OurcomparisonofitsresultswithavailableState-of-the-Art(SOTA)solutionsshowedthat theaveragelossinqualityoftheChatGPTmodelwasabout25%forzero-shotandfew-shotevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization,andweobtainedsignificantlybetteruser-bas", "GPT-4 Technical Report": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4\u2019s performance based on models trained with no more than 1/1,000th the compute of GPT-4.", "Sparks of Arti\fcial General Intelligence: Early experiments with GPT-4": "Arti\fcial intelligence (AI) researchers have been developing and re\fning large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and di\u000ecult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an arti\fcial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with re\rections on societal in\ruences of the recent technological leap and future research directions.", "OpenAssistant Conversations - Democratizing Large Language Model Alignment": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised \ufb01ne-tuning ( SFT) and reinforcement learning from human feedback ( RLHF ) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code2and data3under a fully permissive licence. A list of contributors who have chosen to be acknowledged by name can be found at https://open-assistant.io/contributors ."}, "Computational Biology": {"Generative language modeling for antibody design": "Discovery and optimization of monoclonal antibodies for therapeutic applications relies on large sequence libraries, but is hindered by developability issues such as low solubility, low thermal stability, high aggregation, and high immunogenicity. Generative language models, trained on millions of protein sequences, are a powerful tool for on-demand generation of realistic, diverse sequences. We present Immunoglobulin Language Model (IgLM), a deep generative language model for creating synthetic libraries by re-designing variable-length spans of antibody sequences. IgLM formulates antibody design as an autoregressive sequence generation task based on text-in\ufb01lling in natural language. We trained IgLM on 558M antibody heavy- and light-chain variable sequences, conditioning on each sequence\u2019s chain type and species-of-origin. We demonstrate that IgLM can generate full-length heavy and light chain sequences from a variety of species, as well as in\ufb01lled CDR loop libraries with improved developability pro\ufb01les. IgLM is a powerful tool for antibody design and should be useful in a variety of applications.", "Language models of protein sequences at the scale of evolution enable accurate structure prediction": "Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.", "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein": "Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concur- rently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibil- ity and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experi- ments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing lan- guage model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can per- form programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contribut- ing to the evolving landscape of foundation models in protein science. Trained weight for the xTrimoPGLM model, and downstream datasets are available at https://huggingface.co/proteinglm .", "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning": "Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT , Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM- embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81%-87%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81%) and membrane versus water-soluble (2-state accuracy Q2=91%). For secondary structure, the most informative embeddings (ProtT5) for the \ufb01rst time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the grammar of the language of life . All our models are available through https://github.com/agemagician/ProtTrans.", "Large language models generate functional protein sequences across diverse families": "Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from >19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.", "Nucleotide Transformer: building and evaluating robust foundation models for human genomics": "The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.", "Codon language embeddings provide strong signals for use in protein engineering": "Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models\u2019 capacities surpassing the size of the very datasets they were trained on. Here we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, such as species recognition, prediction of protein and transcript abundance or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results indicate that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.", "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts": "Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance ProteinSequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM\u2019s original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation. Source code and model weights are available at https://github.com/DeepGraphLearning/ProtST .", "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics": "We seek to transform how new and emergent variants of pandemic-causing viruses, speci \ufb01cally SARS-CoV-2, are identi \ufb01ed and classi \ufb01ed. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on over 110 millionprokaryotic gene sequences and \ufb01ne-tuning a SARS-CoV-2-speci \ufb01c model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the \ufb01rst whole-genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs onGPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zetta \ufb02ops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scienti \ufb01c insights from"}, "Computer programming": {"Evaluating Large Language Models Trained on Code": "We introduce Codex, a GPT language model \ufb01ne-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we \ufb01nd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to dif\ufb01cult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including dif\ufb01culty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "Competition-Level Code Generation with AlphaCode": "Programming is a powerful and ubiquitous problem - solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large - scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem - solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient - to - sample transformer - based architectures, and (3) large - scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.", "CODEGEN: AN OPEN LARGE LANGUAGE MODEL FOR CODE WITH MULTI -TURN PROGRAM SYNTHESIS": "Program synthesis strives to generate a computer program as a solution to a given problem speci\ufb01cation, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAX FORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion signi\ufb01cantly improves program synthesis over that provided as a single turn. We make the training library JAX FORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "INCODER : A GENERATIVE MODEL FOR CODE INFILLING AND SYNTHESIS": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and re\ufb01ned. We introduce INCODER, a uni\ufb01ed generative model that can perform program synthesis (via left-to-right generation) as well as editing (via masking and in\ufb01lling). InCoder is trained to generate code \ufb01les from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each \ufb01le, allowing code in\ufb01lling with bidirectional context. Our model is the \ufb01rst large generative code model that is able to in\ufb01ll arbitrary regions of code, which we evaluate in a zero-shot setting on challenging tasks such as type inference, comment generation, and variable re-naming. We \ufb01nd that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. Our models and code are publicly released.", "SANTA CODER :DON \u2019T REACH FOR THE STARS !": "The BigCode project is an open-scienti\ufb01c collaboration working on the responsi-ble development of large language models for code.1This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identi\ufb01able Information (PII) redaction pipeline, the experi-ments conducted to de-risk the model architecture, and the experiments investi-gating better preprocessing methods for the training data. We train 1.1B param-eter models on the Java, JavaScript, and Python subsets of The Stack (Kocetkov et al., 2022) and evaluate them on the MultiPL-E text-to-code benchmark (Cas-sano et al., 2022). We \ufb01nd that more aggressive \ufb01ltering of near-duplicates can further boost performance and, surprisingly, that selecting \ufb01les from repositories with 5+ GitHub stars deteriorates performance signi\ufb01cantly. Our best model out-performs previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and in\ufb01lling on the Java", "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder", "TEACHING LARGE LANGUAGE MODELS TO SELF-DEBUG": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose SELF-DEBUGGING , which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate thatSELF-DEBUGGING can teach the large language model to perform rubber duck debugging ; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, SELF-DEBUGGING with code explanation consistently improves the baseline by 2\u22123%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, SELF-DEBUGGING improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, SELF-DEBUGGING notably improves sample efficiency, and can match or outperform baseline models that generate more than 10 \u00d7candidate programs.", "Textbooks Are All You Need": "We introduce phi-1 , a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1 .3B parameters, trained for 4 days on 8 A100s, using a selection of \u201ctextbook quality\u201d data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50 .6% on HumanEval and 55 .5% on MBPP. It also displays surprising emergent properties compared to phi-1-base , our model before our finetuning stage on a dataset of coding exercises, and phi-1-small , a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "A Systematic Evaluation of Large Language Models of Code": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex [ 10]) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and Code-Parrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex . Our trained models are open-source and publicly available athttps://github.com/VHellendoorn/Code-LMs , which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169 .", "Repository-Level Prompt Generation for Large Language Models of Code": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex (Chen et al., 2021) used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn\u2019t require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at https://github.com/shrivastavadisha/repo_level_prompt_generation.", "ViperGPT : Visual Inference via Python Execution for Reasoning": "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks."}, "Creative work": {"Re3: Generating Longer Stories With Recursive Reprompting and Revision": "We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long - range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general - purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar - length stories generated directly from the same base model, human evaluators judged substantially more of Re3\u2019s stories as having a coherent overarching plot (by 14% absolute increase), and relevant to the given initial premise (by 20%).", "Help me write a poem : Instruction Tuning as a Vehicle for Collaborative Poetry Writing": "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of LLMs in the realm of computer-assisted creativity, we aim to study if LLMs can improve the quality of user-generated content through collaboration. We present CoPoet, a collaborative poetry writing system. In contrast to auto-completing a user\u2019s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about \u2018love\u2019 or Write a sentence ending in \u2018fly\u2019. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive with publicly available LLMs trained on instructions (InstructGPT), but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change. Further, the collaboratively written poems are preferred by third-party evaluators over those written without the system.", "DOC: Improving Long Story Coherence With Detailed Outline Control": "We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.", "Little Red Riding Hood Goes Around the Globe: Crosslingual Story Planning and Generation with Large Language Models": "Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.", "Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity": "A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots:  alpa.ai, Copy.ai, ChatGPT (versions 3 and 4), Studio.ai, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no qualitative difference between AI and human-generated creativity, although there are differences in how ideas are generated. Interestingly, 9.4% of humans were more creative than the most creative GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being \u201ctruly\u201d creative.", "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two - stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off - the - shelf diffusion model for layout - grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction - based multi - round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users\u2019 creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm - grounded - diffusion.github.io .", "Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals": "Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron\u2019s usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report reflections both from our interviewees and from independent reviewers who critiqued performances of several of the scripts to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations\u2014including plagiarism and bias\u2014and participatory models for the design and deployment of such tools.", "Spinning Coherent Interactive Fiction through Foundation Model Prompts": "We present Spindle , a mixed initiative tool for authoring choice-based interactive fiction that targets Twine , a popular framework for text-based storygames. Twine artifacts have properties of both stories and games, placing our system at the intersection of Automated Game Design (AGD) and Automated Story Generation (ASG). We construct a generative pipeline that involves condensing narrative context into a compact representation in order to feed to a pretrained language model, which we further fine-tune. We demonstrate that, by maintaining narrative context in the prompt presented to the language model, we can greatly improve over the loss of long - term coherence that still plagues such models. Our story compression technique for representing narrative context uses a handful of freely available natural language processing libraries and models, demonstrating that such interpretive pipelines can be built with limited computational resources and low cost. The resulting tool is capable of producing full - text branching narratives, or of generating individual passages that maintain a high degree of narrative coherence with the prior passages. The framework we design is both language model - agnostic and narrative theory agnostic, allowing future researchers to easily expand on it with new language models and story representations. We release our code under the BSD - 4 - Clause1.", "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models": "Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains."}, "Knowledge work": {"Understanding Emails and Drafting Responses An Approach Using GPT-3": "", "Galactica: A Large Language Model for Science": "Information overload is a major obstacle to scienti\ufb01c progress. The explosive growth in scienti\ufb01cliteratureanddatahasmadeiteverhardertodiscoverusefulinsightsinalarge mass of information. Today scienti\ufb01c knowledge is accessed through search engines, but they are unable to organize scienti\ufb01c knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scienti\ufb01c knowledge. We trainonalargescienti\ufb01ccorpusofpapers,referencematerial,knowledgebasesandmany othersources. Weoutperformexistingmodelsonarangeofscienti\ufb01ctasks. Ontechnical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT - 3 by 68.2%versus49.0%. Galacticaalsoperformswellonreasoning,outperformingChinchilla onmathematicalMMLUby41.3%to35.7%,andPaLM540BonMATHwithascoreof20.4% versus8.8%. Italsosetsanewstate - of - the - artondownstreamtaskssuchasPubMedQAand MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT - 175B on BIG - bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the bene\ufb01t of the scienti\ufb01c community1.", "Structured information extraction from complex scienti\ufb01c text with \ufb01ne-tuned large language models": "Intelligently extracting and linking complex scienti\ufb01c information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scienti\ufb01c text. The approach leverages a pre-trained large language model (LLM), GPT-3, that is \ufb01ne-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of JSON objects. We demonstrate that LLMs trained in this way are capable of accurately extracting useful records of complex scienti\ufb01c knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-\ufb02exible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.", "GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities": "The global economy is increasingly dependent on knowledge workers to meet the needs of public and private organizations. While there is no single de\ufb01nition of knowledge work, organizations and industry groups still attempt to measure individuals\u2019 capability to engage in it. The most comprehensive assessment of capability readiness for professional knowledge workers is the Uniform CPA Examination developed by the American Institute of Certi\ufb01ed Public Accountants (AICPA). In this paper, we experimentally evaluate OpenAI\u2019s text-davinci -003 and prior versions of GPT on both a sample Regulation (REG) exam and an assessment of over 200 multiple-choice questions based on the AICPA Blueprints for legal, \ufb01nancial, accounting, technology, and ethical tasks. First, we \ufb01nd that text-davinci -003 achieves a correct rate of 14.4% on a sample REG exam section, signi\ufb01cantly underperforming human capabilities on quantitative reasoning in zero-shot prompts. Second, text-davinci -003 appears to be approaching human-level performance on the Remembering & Understanding and Application skill levels in the Exam absent calculation. For best prompt and parameters, the model answers 57.6% of questions correctly, signi\ufb01cantly better than the 25% guessing rate, and its top two answers are correct 82.1% of the time, indicating strong non-entailment. Finally, we \ufb01nd that recent generations of GPT-3 demonstrate material improvements on this assessment, rising from 30% for text-davinci -001 to 57% for text-davinci -003. These \ufb01ndings strongly suggest that large language models have the potential to transform the quality and e \u000eciency of future knowledge work.", "BloombergGPT: A Large Language Model for Finance": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT , a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg\u2019s extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT .", "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in the reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT - 4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT - 4) for three tasks: 1.Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2.Verifying checklists: We task the LLM to verify 16 closed - ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper } pairs, the LLM had an 86.6% accuracy. 3.Choosing the \u201cbetter\u201d paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.", "Synthesizing Natural Language to Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks": "Natural language (/n.sc/l.sc) is a promising interaction paradigm for data visualization (/v.sc/i.sc/s.sc). However, there are not any /n.sc/l.scto/v.sc/i.sc/s.sc(/n.sc/l.sc2/v.sc/i.sc/s.sc) benchmarks available. Our goal is to provide the rst /n.sc/l.sc2/v.sc/i.sc/s.sc benchmark to enable and push the eld of /n.sc/l.sc2/v.sc/i.sc/s.sc, especially with deep learning technologies. In this paper, we propose a /n.sc/l.sc2/v.sc/i.sc/s.sc synthesizer (/n.sc/l.sc2/s.sc/q.sc/l.sc-to-/n.sc/l.sc2/v.sc/i.sc/s.sc) that synthesizes /n.sc/l.sc2/v.sc/i.sc/s.sc benchmarks by piggybacking /n.sc/l.sc2/s.sc/q.sc/l.sc benchmarks. The intuition is based on the semantic connec- tion between /s.sc/q.sc/l.scqueries and /v.sc/i.sc/s.scqueries: /s.sc/q.sc/l.scqueries specify what data is needed and /v.sc/i.sc/s.scqueries additionally need to specify how to visualize. However, dierent from /s.sc/q.sc/l.scthat has well-dened syntax, /v.sc/i.sc/s.sclanguages (e.g., Vega-Lite, VizQL, ggplot2) are syntactically very dierent. To provide /n.sc/l.sc2/v.sc/i.sc/s.sc benchmarks that can support many /v.sc/i.sc/s.sc languages, we use a unied intermediate representation, abstract syntax trees (ASTs), for both /s.sc/q.sc/l.scand/v.sc/i.sc/s.scqueries. We can synthesize multiple /v.sc/i.sc/s.sctrees through adding/deleting nodes to/from an /s.sc/q.sc/l.sc tree. Each /v.sc/i.sc/s.sctree can then be converted to (any) /v.sc/i.sc/s.sclanguage. The/n.sc/l.scfor/v.sc/i.sc/s.scwill be modied based on the /n.sc/l.scfor/s.sc/q.sc/l.scto reect corresponding tree edits. We produce the rst /n.sc/l.sc2/v.sc/i.sc/s.sc benchmark (/n.sc/v.scB/e.sc/n.sc/c.sc/h.sc), by applying /n.sc/l.sc2/s.sc/q.sc/l.sc-to-/n.sc/l.sc2/v.sc/i.sc/s.sc on a popular /n.sc/l.sc2/s.sc/q.sc/l.sc benchmark Spider, which covers 105domains, supports seven common types of visualizations, and contains 25,750 (/n.sc/l.sc, /v.sc/i.sc/s.sc) pairs. Our method reduces the man- hour to 5:7%of developing a /n.sc/l.sc2/v.sc/i.sc/s.sc benchmark from scratch (or building a /n.sc/l.sc2/v.", "Benchmarking Large Language Models for News Summarization": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluationon ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find in-struction tuning, not model size, is the key to the LLM\u2019s zero-shot summarization capability. Second, existing studies have beenlimited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance.To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite ma-jor stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human writtensummaries."}, "law": {"Legal Prompting: Teaching a Language Model to Think Like a Lawyer": "Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks signi\ufb01cantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and \ufb01ne-tuning approaches. Our \ufb01ndings show that while CoT prompting and \ufb01ne-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from speci\ufb01c legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.", "Blind Judgement: Agent-Based Supreme Court Modelling With GPT": "We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We \ufb01nd our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further \ufb01nd a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism & liberalism. Our methods and results hold signi\ufb01cance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.", "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?": "Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the summaries. We see that abstractive summarization models generally achieve slightly higher scores than extractive models in terms of standard summary evaluation metrics such as ROUGE and BLEU. However, we often find inconsistent or hallucinated information in the generated abstractive summaries. Overall, our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more suitable at present.", "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)": "Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings open the door to the building of systems that can autonomously retrieve relevant sentences from case law and condense them into a useful explanation for legal scholars, educators or practicing lawyers alike.", "Can GPT-3 Perform Statutory Reasoning?": "Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003 , on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.", "GPT-4 passes the bar exam": "In this paper, we experimentally evaluate the zero - shot performance of GPT-4 against prior generations of GPT on the entire uniform bar examination (UBE), including not only the multiple - choice multistate bar examination (MBE), but also the open - ended multistate essay exam (MEE) and multistate performance test (MPT) components. On the MBE, GPT-4 significantly outperforms both human test - takers and prior models, demonstrating a 26% increase over ChatGPT and beating humans in five of seven subject areas. On the MEE and MPT, which have not previously been evaluated by scholars, GPT - 4 scores an average of 4.2/6.0 when compared with much lower scores for ChatGPT. Graded across the UBE components, in the manner in which a human test - taker would be, GPT-4 scores approximately 297 points, significantly in excess of the passing threshold for all UBE jurisdictions. These findings document not just the rapid and remarkable advance of large language model performance generally, but also the potential for such models to support the delivery of legal services in society.", "Warming and parasitism impair the performance of Baltic native and invasive macroalgae and their associated fauna": "Global warming, bioinvasions, and parasitism affect single-species performances and species interactions, substantially impacting the structure and stability of marine ecosystems. In light of accelerated global change,the information derived from studies focusing on single species and single drivers is insuf \ufb01cient, calling for a multi-stressor approach under near-natural conditions. We investigated the effects of warming ( +3 /C14C) on the performance of a benthic community composed of native and invasive macroalgae, consumers and a trematodeparasite in a mesocosm setting. We also assessed the effects of warming and parasitism on the survival and growth of gastropods and mussels and the thermal dependency of trematode performance. Our \ufb01ndings show that warming and grazing by infected gastropods had a large detrimental effect on the invasive macroalgagrowth. Furthermore, the single and interactive effects of parasitism and warming were detrimental to interme- diate host survival and growth, especially to large mussels. Finally, cercarial emergence positively correlated to the natural peaks of summer temperatures, while infection intensity in mussels was higher in larger individuals.Our\ufb01ndings suggest that grazing and warming will be detrimental to the invasive macroalga, favoring the native alga. Moreover, parasitism will enhance grazing, especially in summer, when higher temperatures trigger parasite development. However, parasite-enhanced grazing may be buffered", "ChatGPT by OpenAI: The End of Litigation Lawyers?": "ChatGPT, a revolutionary AI language model developed by OpenAI, can understand instructions with unprecedented efficiency. This study aims to evaluate the extent to which ChatGPT can potentially serve as a replacement for litigation lawyers through an examination of its drafting and research capabilities. The results indicate that ChatGPT has advanced legal drafting skills for various types of documents, including demand letters, without -prejudice letters, and pleadings. ChatGPT was able to elaborate and enhance the contents based on the simple facts inputted into the system and demonstrated the ability to understand simple facts and articulate the legal basis of the claim. Additionally, ChatGPT can identify legal strategies, draft a summary judgment, generate a skeleton argument, cross -examination, and provide simple legal advice. The results also reveal that ChatGPT performed excellently in analysing a more complicated case. However, there were limitations in the data sources used in ChatGPT, which resulted in a weakness in identifying recent case laws. At this stage, the paper suggests that ChatGPT should be viewed as a supplement, rather than a replacement, to the litigation lawyers."}, "Medicine": {"Large Language Models are Few-Shot Clinical Information Extractors": "A long-running goal of the clinical NLP com- munity is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models , such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clin- ical text despite not being trained speci\ufb01cally for the clinical domain. Whereas text classi- \ufb01cation and generation performance have al- ready been studied extensively in such mod- els, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identi\ufb01cation, token-level se- quence classi\ufb01cation, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical in- formation extraction based on a manual re- annotation of the CASI dataset (Moon et al., 2014) for new tasks1. On the clinical extrac- tion tasks we studied, the GPT-3 systems sig- ni\ufb01cantly outperform existing zero- and few- shot baselines.", "Capabilities of GPT-4 on Medical Challenge Problems": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4 [Ope23], a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the United States Medical Licensing Examination (USMLE), a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study calibration of the probabilities, which is of critical importance in high - stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine - tuned on medical knowledge (Med-PaLM, a prompt - tuned version of Flan - PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much - improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively by presenting a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.", "Towards Expert-Level Medical Question Answering with Large Language Models": "Recent arti\ufb01cial intelligence (AI) systems have reached milestones in \u201cgrand challenges\u201d ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed signi\ufb01cant progress in medical question answering; Med-PaLM was the \ufb01rst model to exceed a \u201cpassing\u201d score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested signi\ufb01cant room for improvement, especially when models\u2019 answers were compared to clinicians\u2019 answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain \ufb01netuning, and prompting strategies including a novel ensemble re\ufb01nement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p< 0.001). We also observed signi\ufb01cant improvements compared to Med-PaLM on every evaluation axis (p< 0.001) on newly introduced datasets of 240 long-form \u201cadversarial\u201d questions to probe LLM limitations. While further studies are necessary to validate the e\ufb03cacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question a", "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events": "Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT) and ADE extraction architecture shed light on best practice for biomedical knowledge extraction. Similar gains were attained by distillation for other standard biomedical knowledge extraction tasks such as gene-disease associations and protected health information, further illustrating the promise of this approach.", "ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models": "", "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge": "Objective\nThe primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice.\nMethods\nWe achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases.\nResults\nThe fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses.\nConclusion\nOur proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not o", "The Diagnostic and Triage Accuracy of  the GPT -3 Artificial Intelligence Model": "Importance : Artificial intelligence (AI) applications in health care have been effective in many areas  of medicine , but they are often trained for a single task using labeled data , making deployment and generalizability challenging. Whether a general -purpose AI language model can perform diagnosis and triage is unknown.  Objective : Compare the general -purpose Generative Pre -trained Transformer 3 ( GPT -3) AI model\u2019s diagnostic and triage performance to  attending physicians and lay adults who use the Internet.  Design : We compared the accuracy of  GPT -3\u2019s diagnostic  and triage ability for 48 validated case vignettes of both common (e.g., viral illness) and severe (e.g., heart attack) conditions to lay people  and practicing  physicians . Finally, we examined how well calibrated GPT -3\u2019s confidence was for diagnosis and triage.   Setting and Participants : The GPT -3 model , a nationally  representative sample of lay people ,", "Can large language models reason about medical questions?": "Although large language models often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether closed- and open-source models (GPT-3.5, Llama 2, etc.) can be applied to answer and reason about dif\ufb01cult real-world-based questions. We focus on three popular medical benchmarks(MedQA-US Medical Licensing Examination [USMLE], MedMCQA, and PubMedQA)", "Deciphering clinical abbreviations with a privacy protecting machine learning system": "Physicians write clinical notes with abbreviations and shorthand that are difficult to decipher. Abbreviations can be clinical jargon (writing \u201cHIT\u201d for \u201cheparin induced thrombocytopenia \u201d), ambiguous terms that require expertise to disambiguate (using \u201cMS\u201d for \u201cmultiple sclerosis \u201d or \u201cmental status \u201d), or domain - specific vernacular ( \u201ccb\u201d for \u201ccomplicated by \u201d). Here we train machine learning models on public web data to decode such text by replacing abbreviations with their meanings. We report a single translation model that simultaneously detects and expands thousands of abbreviations in real clinical notes with accuracies ranging from 92. 1% - 97.1% on multiple external test datasets. The model equals or exceeds the performance of board - certified physicians (97.6% vs 88.7% total accuracy). Our results demonstrate a general method to contextually decipher abbreviations and shorthand that is built without any privacy - compromising data.", "ArticleLarge language models encode clinical knowledge": "Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model 1 (PaLM, a 540 - billion parameter LLM) and its instruction - tuned variant, Flan - PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan - PaLM achieves state - of - the - art accuracy on every MultiMedQA multiple - choice dataset (MedQA 3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam - style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter - efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med - PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today\u2019s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinic", "Evaluating large language models on medical evidence summarization": "Recent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on various downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the capabilities and limitations of LLMs, specifically GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization across six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality. Our study demonstrates that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore, informed by our human evaluations, we define a terminology of error types for medical evidence summarization. Our findings reveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain statements, leading to potential harm due to misinformation. Moreover, we find that models struggle to identify the salient information and are more error-prone when summarizing over longer textual contexts.", "A study of generative large language model for medical research and healthcare": "There are enormous enthusiasm and concerns in applying large language models (LLMs) to healthcare. Yet current assumptions are based on general - purpose LLMs such as ChatGPT, which are not developed for medical use. This study develops a generative clinical LLM, GatorTronGPT, using 277 billion words of text including (1) 82 billion words of clinical text from 126 clinical departments and approximately 2 million patients at the University of Florida Health and (2) 195 billion words of diverse general English text. We train GatorTronGPT using a GPT - 3 architecture with up to 20 billion parameters and evaluate its utility for biomedical natural language processing (NLP) and healthcare text generation. GatorTronGPT improves biomedical natural language processing. We apply GatorTronGPT to generate 20 billion words of synthetic text. Synthetic NLP models trained using synthetic text generated by GatorTronGPT outperform models trained using real - world clinical text. Physicians \u2019Turing test using 1 (worst) to 9 (best) scale shows that there are no significant differences in linguistic readability ( p=0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance ( p=0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them ( p< 0.001). This study provides insights into the opportunities and challenges of LLMs for medical research and healthcare."}, "Reasoning": {"Towards automating formalisation of theorem statements using large language models": "Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful input-dependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with about 65% accuracy for 120 theorem statements.", "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models": "", "LARGE LANGUAGE MODELS CANSELF-IMPROVE": "Large Language Models (LLMs) have achieved excellent performances in vari-ous tasks. However, \ufb01ne-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \u201chigh-con\ufb01dence\u201d rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and \ufb01ne-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4% !82.1% on GSM8K, 78.2% !83.0% on DROP, 90.0% !94.4% on OpenBookQA, and 63.4%!67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that \ufb01ne-tuning on reasoning is critical for self-improvement.", "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models": "We have recently witnessed a number of im- pressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intu- itive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT - 3 Davinci models (175B) achieve a dramatic im- provement in both robustness and sensitivity compared to all other GPT variants.", "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems": "Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers. In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning. On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.", "Solving math word problems with process- and outcome-based feedback": "Recent work has shown that asking language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education. We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use process-based supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from 16.8% \u219212.7% final-answer error and 14.0% \u21923.4% reasoning error among final-answer-correct solutions.", "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality": "The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a \u201cbehavorial\u201d study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best - performing existing methods. Algorithms based on GPT - 3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM - based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py - why/pywhy - llm.", "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation.": "Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT\u2019s causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT\u2019s upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events. The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning.", "CAN LARGE LANGUAGE MODELS INFER CAUSATION FROM CORRELATION ?": "Causal inference is one of the hallmarks of human intelligence. While the field of Causal NLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task CORR2CAUSE , which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large - scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re - purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize \u2013 they can only perform causal inference in in - distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out - of - distribution settings generated by perturbing these queries. CORR2CAUSE is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs\u2019 pure reasoning skills and generalizability.", "Large Language Models Still Can\u2019t Plan": "Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs\u2019 reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.", "Passive learning of active causal strategies in agents and language models": "What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that, under certain assumptions, learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high - dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out - of - distribution from otherwise perfectly - confounded training data. Finally, we show that language models, trained only on passive next - word prediction, can generalize causal intervention strategies from a few - shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.", "Emergent analogical reasoning in large language models": "The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero - shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text - davinci - 003 variant of Generative Pre - trained Transformer (GPT) - 3) on a range of analogical tasks, including a non - visual matrix reasoning task based on the rule structure of Raven\u2019s Standard Progressive Matrices. We found that GPT - 3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT - 4 indicated even better performance. Our results indicate that large language models such as GPT - 3 have acquired an emergent ability to find zero - shot solutions to a broad range of analogy problems."}, "Robotics": {"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high - level, temporally extended instructions expressed in natural language. However, a signi\ufb01cant weakness of language models is that they lack real - world experience, which makes it dif\ufb01cult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real - world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model\u2019s \u201chands and eyes,\u201d while the language model supplies high - level semantic knowledge about the task. We show how low - level skills can be combined with large language models so that the language model provides high - level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real - world robotic tasks, where we show the need for real - world grounding and that this approach is capable of completing long - horizon, abstract, natural language instructions on a mobile manipulator. The project\u2019s website, the video, and open sourced code in a tabletop domain can be found at say-", "Inner Monologue: Embodied Reasoning through Planning with Language Models": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent\u2019s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.", "PaLM-E: An Embodied Multimodal Language Model": "", "REFLECT: Summarizing Robot Experiences for FaiLure Explanation and Correc Tion": "The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.", "ChatGPT for Robotics: Design Principles and Model Abilities": "This paper presents an experimental study regarding the use of OpenAI\u2019s ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high - level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT\u2019s ability to use free - form dialog, parse XML tags, and to synthesize code, in addition to the use of task - specific prompting functions and closed - loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open - sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics. Videos and blog: aka.ms/ChatGPT - Robotics PromptCraft, AirSim - ChatGPT code: https://github.com/microsoft/PromptCraft - Robotics", "Code as Policies: Language Model Programs for Embodied Control": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u201cfaster\u201d) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies : a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io", "PROG PROMPT : Generating Situated Robot Task Plans using Large Language Models": "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io", "Statler: Stat e-Maintaining L anguage Models for E mbodied R easoning": "There has been a signi\ufb01cant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may bene\ufb01t robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework signi\ufb01cantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code"}, "SocialScience": {"Who is GPT-3? An Exploration of Personality, Values and Demographics": "Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.", "Susceptibility to  Influence  of Large Language Models": "Two studies test ed the hypothesi s that a Large Language Model (LLM) can be used to model psychological  change  following exposure  to influential  input . The first study  tested  a generic mode of influence  - the Illusory Truth Effect  (ITE) - where earlier  exposure to a statement (through, for example, rating its interest) boosts  a later truthfulness  test rating . Data was collected from 1000 human participan ts using an online experiment , and 1000 simulated participants  using engineered prompt s and LLM  completion . 64 ratings per participant  were collected , using  all exposure -test combinations of the attributes: truth, interest, sentiment and importance.  The results for human participants reconfirm ed the ITE , and demonstrated  an absence  of effect for attributes  other than truth , and when the same attribute  is used for exposure and test. The same pattern of effects  was found for LLM -simulated  participants . The second study concerns a sp ecific mode of influence  \u2013 populist framing of news to increase its persuasion  and political mobilization.  Data from LLM -simulated  participants  was collected and compared to previously  published data from a 15 -country experiment on 7286 human participants . Several  effects previously demonstrated  from the human study were re plicated by the simulated study , including effects  that surprised the author s of the human study  by contradicting their theoretica l expectations  (anti-immigrant framing of news decreases  its persuasion  and mobilization ); but some  significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation  of the participant ) were not present in the LLM  data . Together the two studies support the view  that LLMs have potential to act as models of the effect of influence .", "Generative Agents: Interactive Simulacra of Human Behavior": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher - level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty - five agents using natural language. In an evaluation", "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model\u2019s simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a \u201chyper-accuracy distortion\u201d present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.", "AI Psychometrics: Using psychometric inventories to obtain psychological profiles of large language models": ""}, "Synthetic Training Data": {"GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation": "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classi\ufb01cation eliminates the need for \ufb01ne-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classi\ufb01cation tasks and show that our method hugely outperforms existing text augmentation methods. We also conduct experiments on our newly proposed benchmark to show that the augmentation effect is not only attributed to memorization. Further ablation studies and a qualitative analysis provide more insights into our approach.", "Want To Reduce Labeling Cost? GPT-3 Can Help": "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-speci\ufb01c and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We \ufb01nd that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.", "Distilling Reasoning Capabilities into Smaller Language Models": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, SOCRATIC COT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use SOCRATIC COT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when SOCRATIC COT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT - 2 large) can outperform a 10X larger model (GPT - 3 6B). Our code is available here.", "Is GPT-3 a Good Data Annotator?": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high-quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP1.", "AugGPT: Leveraging ChatGPT for Text Data Augmentation": "Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few - shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely - used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can\u2019t ensure the correct labeling of the generated data (lacking faithfulness) or can\u2019t ensure suf\ufb01cient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few - shot learning text classi\ufb01cation tasks show the superior performance of the proposed AugGPT approach over state - of - the - art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.", "Textbooks Are All You Need": "We introduce phi-1 , a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1 .3B parameters, trained for 4 days on 8 A100s, using a selection of \u201ctextbook quality\u201d data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50 .6% on HumanEval and 55 .5% on MBPP. It also displays surprising emergent properties compared to phi-1-base , our model before our finetuning stage on a dataset of coding exercises, and phi-1-small , a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "InPars: Unsupervised Dataset Generation for Information Retrieval": "The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models [ 45,56]. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models fine-tuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars."}}