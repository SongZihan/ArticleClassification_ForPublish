{
  "Argumentation": {
    "A neural cognitive model of argumentation with application to legal inference and decision making": "Formal models of argumentation have been investigated in several areas, from multi - agent systems and arti\ufb01cial intelligence (AI) to decision making, philosophy and law. In arti\ufb01cial intelligence, logic - based models have been the standard for the representation of argumentative reasoning. More recently, the standard logic - based models have been shown equivalent to standard connectionist models. This has created a new line of research where (i) neural networks can be used as a parallel computational model for argumentation and (ii) neural networks can be used to combine argumentation, quantitative reasoning and statistical learning. At the same time, non - standard logic models of argumentation started to emerge. In this paper, we propose a connectionist cognitive model of argumentation that accounts for both standard and non - standard forms of argumentation. The model is shown to be an adequate framework for dealing with standard and non - standard argumentation, including joint - attacks, argument support, ordered attacks, disjunctive attacks, meta - level attacks, self - defeating attacks, argument accrual and uncertainty. We show that the neural cognitive approach o\ufb00ers an adequate way of modelling all of these di\ufb00erent aspects of argumentation. We have applied the framework to the modelling of a public prosecution charging decision as part of a real legal decision making case study containing many of the above aspects of argumentation. The results show that the model can be a useful tool in the analysis of legal decision making, including the analysis of what - if questions and the analysis of alternative conclusions. The approach opens up two new perspectives in the short - term: the use of neural networks for computing prevaili",
    "Value-based Argumentation Frameworks as Neural-symbolic Learning Systems": "While neural networks have been successfully used in a number of machine learning applications, logical languages have been the standard for the representation of argumentative reasoning. In this paper, we establish a relationship between neural networks and argumentation networks, combining reasoning and learning in the same argumentation framework. We do so by presenting a new neural argumentation algorithm, responsible for translating argumentation networks into standard neural networks. We then show a correspondence between the two networks. The algorithm works not only for acyclic argumentation networks, but also for circular networks, and it enables the accrual of arguments through learning as well as the parallel computation of arguments.",
    "Argumentation-Based Multi-Agent Decision Making with Privacy Preserved": "We consider multi-agent decision making problems in which agents need to communicate with other agents to make socially optimal decisions but, at the same time, have some private information that they do not want to share. Abstract argumentation has been widely used in both single-agent and multi-agent decision making problems, because of its ability for reasoning with incomplete and con\ufb02icting information. In this work, we propose an abstract argumentation-based knowledge representation and communication protocol, such that agents can \ufb01nd socially optimal strategies by only disclosing the `necessary' and `disclosable' information. We prove that our protocol is sound, e\ufb03cient, of perfect information security and guaranteed to terminate.",
    "Extracting Dialogical Explanations for Review Aggregations with Argumentative Dialogical Agents": "The aggregation of online reviews is fast becoming the chosen method of quality control for users in various domains, from retail to entertainment. Consequently, fair, thorough and explainable aggregation of reviews is increasingly sought-after. We consider the movie review domain, and in particular Rotten Tomatoes\u2019 ubiquitous (and arguably over-simplified) aggregation method, the Tomatometer Score (TS). For a movie, this amounts to the percentage of critics giving the movie a positive review. We define a novel form of argumentative dialogical agent (ADA) for explaining the reasoning within the reviews. ADA integrates: 1.) NLP with reviews to extract a Quantitative Bipolar Argumentation Framework (QBAF) for any chosen movie to provide the underlying structure of explanations, and 2.) gradual semantics for QBAFs for deriving a dialectical strength measure for movies, as an alternative to the TS, satisfying desirable properties for obtaining explanations. We evaluate ADA using some prominent NLP methods and gradual semantics for QBAFs. We show that they provide a dialectical strength which is comparable with the TS, while at the same time being able to provide dialogical explanations of why a movie obtained its strength via interactions between the user and ADA.",
    "Toward Artificial Argumentation": "The \feld of computational models of argument is emerging as an important aspect of arti\fcial intelligence research. The reason for this is based on the recognition that if we are to develop robust intelligent systems, then it is imperative that they can handle incomplete and inconsistent information in a way that somehow emulates the way humans tackle such a complex task. And one of the key ways that humans do this is to use argumentation | either internally, by evaluating arguments and counterarguments | or externally, by for instance entering into a",
    "An Argument-Based Multi-Agent System for Information Integration": "In this paper we address the problem of obtaining a consoli- dated view of the knowledge that a community of information agents pos- sesses in the form of private, possibly large, databases. Each agent in the community has independent sources of information and each database could contain information that is potentially inconsistent and incom- plete, both by itself and/or in conjunction with some of the others. These characteristics make the consolidation di\ufb03cult by traditional means. The idea of obtaining a single view is to provide a way of querying the re- sulting knowledge in a skeptical manner, i.e., receiving one answer that re\ufb02ects the perception of the information community. Agents using the proposed system will be able to access multiple sources of knowledge represented in the form of deductive databases as if they were accessing a single one. One application of this schema is a novel ar- chitecture for decision-support systems (DSS) that will combine database technologies, speci\ufb01cally federated databases, which we will cast as in- formation agents, with an argumentation-based framework.",
    "Long Short-Term Memory": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insuf\ufb01cient, decaying error back\ufb02ow. We brie\ufb02y review Hochreiter\u2019s (1991) analysis of this problem, then address it by introducing a novel, ef\ufb01cient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error \ufb02ow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error \ufb02ow. LSTM is local in space and time; its computational complexity per time step and weight is O.1/. Our experiments with arti\ufb01cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti\ufb01cial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
    "Neural-Symbolic Probabilistic Argumentation Machines": "Neural-symbolic systems combine the strengths of neural networks and symbolic formalisms. In this paper, we introduce a neural-symbolic system which combines restricted Boltzmann machines and probabilistic semi-abstract argumentation. We propose to train networks on argument labellings explaining the data, so that any sampled data outcome is associated with an argument labelling. Argument labellings are integrated as constraints within restricted Boltzmann machines, so that the neural networks are used to learn probabilistic dependencies amongst argument labels. Given a dataset and an argumentation graph as prior knowledge, for every example/case K in the dataset, we use a so-called K-maxconsistent labelling of the graph, and an explanation of case K refers to a K-maxconsistent labelling of the given argumentation graph. The abilities of the proposed system to predict correct labellings were evaluated and compared with standard machine learning techniques. Experiments revealed that such argumentation Boltzmann machines can outperform other classification models, especially in noisy settings.",
    "A Roadmap for Neuro-argumentative Learning": "Computational argumentation (CA) has emerged, in recent decades, as a powerful formalism for knowledge representation and reasoning in the presence of conflicting information, notably when reasoning non-monotonically with rules and exceptions. Much existing work in CA has focused, to date, on reasoning with given argumentation frameworks (AFs) or, more recently, on using AFs, possibly automatically drawn from other systems, for supporting forms of XAI. In this short paper we focus instead on the problem of learning AFs from data, with a focus on neuro-symbolic approaches. Specifically, we overview existing forms of neuro-argumentative (machine) learning , resulting from a combination of neural machine learning mechanisms and argumentative (symbolic) reasoning. We include in our overview neuro-symbolic paradigms that integrate reasoners with a natural understanding in argumentative terms, notably those capturing forms of non-monotonic reasoning in logic programming. We also outline avenues and challenges for future work in this spectrum.",
    "Argumentation-based multi-agent distributed reasoning in dynamic and open environments": "This work presents an approach for distributed and contextualized reasoning in multi-agentsystems, considering environments in which agents may have incomplete, uncertain andinconsistent knowledge. Knowledge is represented by defeasible logic with mapping rules,which model the capability of agents to acquire knowledge from other agents during rea-soning. Based on such knowledge representation, an argumentation-based reasoning modelthat enables distributed building of reusable argument structures to support conclusions isproposed. Con\ufb02icts between arguments are resolved by an argument strength calculation thatconsiders the trust among agents and the degree of similarity between knowledge of differentagents, based on the intuition that greater similarity between knowledge de\ufb01ned by differentagents implies in less uncertainty about the validity of the built argument. Contextualized rea-soning is supported through sharing of relevant knowledge by an agent when issuing queriesto other agents, which enable the cooperating agents to be aware of knowledge not known apriori but that is important to reach a reasonable conclusion given the context of the agent thatissued the query. A distributed algorithm is presented and analytically and experimentallyevaluated asserting its computational feasibility. Finally, our approach is compared to relatedwork, highlighting the contributions presented, demonstrating its applicability in a broaderrange of scenarios, and presenting perspectives for future work.",
    "A systematic review of argumentation techniques for multi-agent systems research": "The ability to build arguments that express thoughts is crucial for intelligent inter- actions among human beings. Thus, argumentation techniques have been applied for years in \ufb01elds, such as rhetoric or arti\ufb01cial intelligence. More speci\ufb01cally, the agents paradigm \ufb01ts into the use of these types of techniques because agents shape a society in which they interact to make arrangements or to decide future actions. Those interactions can be modelled using argumentation techniques. Therefore, the application of those techniques in multi-agent systems is an interesting research \ufb01eld. However, no systematic review has been conducted previously, to the best of the authors\u2019 knowledge, to provide an overview of argumentation techniques for multi-agent systems. This paper presents a systematic review of argumentation techniques for multi-agent systems research. The period of time that is included in this review is from 1998 to 2014. The objective of this review is to obtain an overview of the existing approaches and to study their impact on research and practice. The research method has been de\ufb01ned to identify relevant studies based on a prede\ufb01ned search strategy, and it is clearly de\ufb01ned to facilitate the reading of this paper. All of the included studies in this review have been analysed from two different points of view: the Application view and the Multi-Agent System view . A comprehensive analysis of the extracted data is provided in the paper, which is based on a set of research questions that are de\ufb01ned. The results of this review reveal suggestions for further research and practice. The argumentation technology is actually in a phase of internal enhancement and exploration. Moreover, the research interest in this topic has increased in the last years. Furthermore, several interesting \ufb01ndings are presented in the paper."
  },
  "MathematicalReasoning": {
    "A Goal-Driven Tree-Structured Neural Model for Math Word Problems": "Most existing neural models for math word prob- lems exploit Seq2Seq model to generate solution expressions sequentially from left to right, whose results are far from satisfactory due to the lack of goal-driven mechanism commonly seen in hu- man problem solving. This paper proposes a tree- structured neural model to generate expression tree in a goal-driven manner. Given a math word prob- lem, the model \ufb01rst identi\ufb01es and encodes its goal to achieve, and then the goal gets decomposed into sub-goals combined by an operator in a top-down recursive way. The whole process is repeated un- til the goal is simple enough to be realized by a known quantity as leaf node. During the process, two-layer gated-feedforward networks are designed to implement each step of goal decomposition, and a recursive neural network is used to encode ful- \ufb01lled subtrees into subtree embeddings, which pro- vides a better representation of subtrees than the simple goals of subtrees. Experimental results on the dataset Math23K have shown that our tree- structured model outperforms signi\ufb01cantly several state-of-the-art models.",
    "HMS: A Hierarchical Solver with Dependency-Enhanced Understanding for Math Word Problem": "Automatically solving math word problems is a crucial task for exploring the intelligence levels of machines in the general AI domain. It is highly challenging since it requires not only natural language understanding but also mathematical expression inference. Existing solutions usually explore sequence-to-sequence models to generate expressions, where the problems are simply encoded sequentially. However, such models are generally far from enough for understanding problems as similar to humans and lead to incorrect answers. To this end, in this paper, we propose a novel Hierarchical Math Solver (HMS) to make deep understanding and exploitation of problems. In problem understanding, imitating human reading habits, we propose a hierarchical word-clause-problem encoder. Specifically, we first split each problem into several clauses and learn problem semantics from the local clause level to the global problem level. Then, in clause understanding, we propose a dependency-based module to enhance clause semantics with the dependency structure of the problem. Next, in expression inference, we propose a novel tree-based decoder to generate the mathematical expression for the answer. In the decoder, we apply a hierarchical attention mechanism to enhance the problem semantics with context from different levels, and a pointer-generator network to guide the model to copy existing information and infer extra knowledge. Extensive experimental results on two widely used datasets demonstrate that HMS achieves not only better answers but also more reasonable inference.",
    "End-to-End Differentiable Proving": "We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Speci\ufb01cally, we replace symbolic uni\ufb01cation with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free \ufb01rst-order logic rules.",
    "Graph Representations for Higher-Order Logic and Theorem Proving": "This paper presents the \ufb01rst use of graph neural networks (GNNs) for higher-order proof search and demonstrates that GNNs can improve upon state-of-the-art results in this domain. Interactive, higher-order theorem provers allow for the formalization of most mathematical theories and have been shown to pose a signi\ufb01cant challenge for deep learning. Higher-order logic is highly expressive and, even though it is well-structured with a clearly de\ufb01ned grammar and seman- tics, there still remains no well-established method to con- vert formulas into graph-based representations. In this paper, we consider several graphical representations of higher-order logic and evaluate them against the HOList benchmark for higher-order theorem proving.",
    "DEEP LEARNING FOR SYMBOLIC MATHEMATICS": "Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence - to - sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",
    "Learning Reasoning Strategies in End-to-End Differentiable Proving": "Attempts to render deep learning models interpretable, data-ef\ufb01cient, and robust have seen some success through hybridisation with rule-based systems, for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them un\ufb01t for large-scale applications. We present Conditional Theorem Provers (CTPs), an extension to NTPs that learns an optimal rule selection strategy via gradient-based optimisation. We show that CTPs are scalable and yield state-of-the-art results on the CLUTRR dataset, which tests systematic generalisation of neural models by learning to reason over smaller graphs and evaluating on larger ones. Finally, CTPs show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable. All source code and datasets are available online.",
    "Graph-to-Tree Learning for Solving Math Word Problems": "While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree \u2019s effectiveness in translating the MWP text into solution expressions1.",
    "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks": "Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi duality between symbolic equation generation and problem\u2019s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new large-scale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods1.",
    "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems": "Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations like n1+n2 are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.",
    "SOFTENED SYMBOL GROUNDING FOR NEURO - SYMBOLIC SYSTEMS": "Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on sym- bol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distri- bution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning; (2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can es- cape from sub-optimal symbol groundings. Experiments with three representative neuro-symbolic learning tasks demonstrate that, owining to its superior symbol grounding capability, our framework successfully solves problems well beyond the frontier of the existing proposals.",
    "Tree-structured Decoding for Solving Math Word Problems": "Automatically solving math word problems is an interesting research topic that needs to bridge natural language descriptions and formal math equations. Previous studies introduced end - to - end neural network methods, but these approaches did not efficiently consider an important characteristic of the equation, i.e., an abstract syntax tree. To address this problem, we propose a tree - structured decoding method that generates the abstract syntax tree of the equation in a top - down manner. In addition, our approach can automatically stop during decoding without a redundant stop token. The experimental results show that our method achieves single model state - of - the - art performance on Math23K, which is the largest dataset on this task.",
    "Discovering faster matrix multiplication algorithms with reinforcement learning": "Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state - of - the - art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two - level algorithm for the first time, to our knowledge, since its discovery 50 years ago. We further showcase the flexibility of AlphaTensor through different use - cases: algorithms with state - of - the - art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria."
  },
  "ProgrammingSystems": {
    "DEEPCODER : LEARNING TO WRITE PROGRAMS": "We develop a \ufb01rst line of attack for solving programming competition-style prob- lems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network\u2019s predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of dif\ufb01culty comparable to the simplest problems on programming competition websites.",
    "NEURAL SKETCH LEARNING FOR CONDITIONAL PROGRAM GENERATION": "We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a \u201crealistic\u201d relationship between programs and labels, as exempli\ufb01ed by a corpus of labeled programs available during training. Two challenges in such conditional program generation are that the generated programs must satisfy a rich set of syntactic and semantic constraints, and that source code contains many low-level features that impede learning. We address these problems by training a neural generator not on code but on program sketches , or models of program syntax that abstract out names and operations that do not generalize across programs. During generation, we infer a posterior distribution over sketches, then concretize samples from this distribution into type-safe programs using combinatorial techniques. We implement our ideas in a system for generating API-heavy Java code, and show that it can often predict the entire body of a method given just a few API calls or data types that appear in the method.",
    "Learning to Infer Graphics Programs from Hand-Drawn Images": "We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of L ATEX. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a speci\ufb01cation (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network, measure similarity between drawings by use of similar high-level geometric structures, and extrapolate drawings.",
    "Neural Program Generation Modulo Static Analysis": "State-of-the-art neural models of source code tend to be evaluated on the generation of individual expressions and lines of code, and commonly fail on long-horizon tasks such as the generation of entire method bodies. We propose to address this de\ufb01ciency using weak supervision from a static program analyzer. Our neurosym - bolic method allows a deep generative model to symbolically compute, using calls to a static-analysis tool, long-distance semantic relationships in the code that it has already generated. During training, the model observes these relationships and learns to generate programs conditioned on them. We apply our approach to the problem of generating entire Java methods given the remainder of the class that contains the method. Our experiments show that the approach substantially outperforms state-of-the-art transformers and a model that explicitly tries to learn program semantics on this task, both in terms of producing programs free of basic semantic errors and in terms of syntactically matching the ground truth."
  },
  "Question-Answering": {
    "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision": "Harnessing the statistical power of neu- ral networks to perform language under- standing and symbolic reasoning is dif- \ufb01cult, when it requires executing ef\ufb01- cient discrete operations against a large knowledge-base. In this work, we intro- duce a Neural Symbolic Machine (NSM), which contains (a) a neural \u201cprogram- mer\u201d, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to han- dle compositionality (b) a symbolic \u201ccom- puter\u201d, i.e., a Lisp interpreter that performs program execution, and helps \ufb01nd good programs by pruning the search space. We apply REINFORCE to directly opti- mize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE we augment it with an it- erative maximum-likelihood training pro- cess. NSM outperforms the state-of-the- art on the W EBQUESTIONS SP dataset when trained from question-answer pairs only, without requiring any feature engi- neering or domain-speci\ufb01c knowledge.",
    "Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering": "Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.",
    "NEURAL MODULE NETWORKS FOR REASONING OVER TEXT": "Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we \ufb01nd that it is challenging to learn these models for non - synthetic questions on open - domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically - obtained question program and intermediate module output supervision provides suf\ufb01cient inductive bias for accurate learning. Our proposed model signi\ufb01cantly outperforms state - of - the - art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.",
    "Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering": "Recent developments in pre-trained neural language modeling have led to leaps in accuracy on common- sense question-answering benchmarks. However, there is increasing concern that models over\ufb01t to speci\ufb01c tasks, without learning to utilize external knowledge or perform general semantic reasoning. In contrast, zero- shot evaluations have shown promise as a more robust measure of a model\u2019s general reasoning abilities. In this paper, we propose a novel neuro-symbolic framework for zero-shot question answering across commonsense tasks. Guided by a set of hypotheses, the framework studies how to transform various pre-existing knowl- edge resources into a form that is most effective for pre- training models. We vary the set of language models, training regimes, knowledge sources, and data genera- tion strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies. We provide empirical results across \ufb01ve commonsense question- answering tasks with data generated from \ufb01ve external knowledge resources. We show that, while an individ- ual knowledge graph is better suited for speci\ufb01c tasks, a global knowledge graph brings consistent gains across different tasks. In addition, both preserving the structure of the task as well as generating fair and informative questions help language models learn more effectively.",
    "Adaptable and Interpretable Neural Memory Over Symbolic Knowledge": "Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro - symbolic KB in the form of a \u201cfact memory\u201d. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge - intensive question - answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state - of - the - art open - book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re - training, by updating the fact memory.",
    "RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering": "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GRAIL QA and WEBQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GRAIL QA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WEBQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.",
    "A Two-Stage Approach towards Generalization in Knowledge Base Question Answering": "Most existing approaches for Knowledge Base Question An- swering (KBQA) focus on a speci\ufb01c underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base re- quires non-trivial changes. However, many popular knowl- edge bases share similarities in their underlying schemas that can be leveraged to facilitate generalization across knowl- edge bases. To achieve this, we introduce a KBQA frame- work based on a 2-stage architecture that explicitly separates semantic parsing from the knowledge base interaction, facili- tating transfer learning across datasets and knowledge graphs. We show that pretraining on datasets with a different un- derlying knowledge base can nevertheless provide signi\ufb01cant performance gains and reduce sample complexity. Our ap- proach achieves comparable or state-of-the-art performance for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQues- tions (Wikidata) and MetaQA (Wikimovies-KG)."
  },
  "Robotics&Control": {
    "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making": "Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework PEORL that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision - making in a dynamic environment with uncertainties. Symbolic plans are used to guide the agent\u2019s task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.",
    "Neural Task Programming: Learning to Generalize Across Hierarchical Tasks": "In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task speci\ufb01cation (e.g., video demonstration of a task) and recursively decomposes it into \ufb01ner sub-task speci\ufb01cations. These speci\ufb01cations are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives. stanfordvl.github.io/ntp/",
    "Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs": "We present a visually grounded hierarchical planning algorithm for long-horizon manipulation tasks. Our algorithm offers a joint framework of neuro-symbolic task planning and low-level motion generation conditioned on the speci\ufb01ed goal. At the core of our approach is a two-level scene graph representation, namely geometric scene graph andsymbolic scene graph . This hierarchical representation serves as a structured, object-centric abstraction of manipulation scenes. Our model uses graph neural networks to process these scene graphs for predicting high-level task plans and low-level motions. We demonstrate that our method scales to long-horizon tasks and generalizes well to novel task goals. We validate our method in a kitchen storage task in both physical simulation and the real world. Experiments show that our method achieves over 70%success rate and nearly 90 %of subgoal completion rate on the real robot while being four orders of magnitude faster in computation time compared to standard search-based task-and-motion planner.",
    "Learning Neuro-Symbolic Skills for Bilevel Planning": "Decision-making is challenging in robotics environments with con- tinuous object-centric states, continuous actions, long horizons, and sparse feed- back. Hierarchical approaches, such as task and motion planning (TAMP), ad- dress these challenges by decomposing decision-making into two or more lev- els of abstraction. In a setting where demonstrations and symbolic predicates are given, prior work has shown how to learn symbolic operators and neural samplers for TAMP with manually designed parameterized policies. Our main contribution is a method for learning parameterized polices in combination with operators and samplers. These components are packaged into modular neuro- symbolic skills and sequenced together with search-then-sample TAMP to solve new tasks. In experiments in four robotics domains, we show that our ap- proach \u2014 bilevel planning with neuro-symbolic skills \u2014 can solve a wide range of tasks with varying initial states, goals, and objects, outperforming six base- lines and ablations. Video: https://youtu.be/PbFZP8rPuGg Code: https://tinyurl.com/skill-learning",
    "Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design": "As a promising topic in cognitive robotics, neuro-symbolic model- ing integrates symbolic reasoning and neural representation altogether. However, previous neuro-symbolic models usually wire their structures and the connec- tions manually, making the underlying parameters sub-optimal. In this work, we propose the Neuro-Symbolic Program Search (NSPS) to improve the au- tonomous driving system design. NSPS is a novel automated search method that synthesizes the Neuro-Symbolic Programs. It can produce robust and ex- pressive Neuro-Symbolic Programs and automatically tune the hyper-parameters. We validate NSPS in the CARLA driving simulation environment. The resulting Neuro-Symbolic Decision Programs successfully handle multiple traf\ufb01c scenar- ios. Compared with previous neural-network-based driving and rule-based meth- ods, our neuro-symbolic driving pipeline achieves more stable and safer behav- iors in complex driving scenarios while maintaining an interpretable symbolic decision-making process."
  },
  "ScientificDiscovery": {
    "SYNTAX -DIRECTED VARIATIONAL AUTOENCODER FOR STRUCTURED DATA": "Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete struc- tures with formal grammars and semantics, e.g., computer programs and molec- ular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-V AE) by introduc- ingstochastic lazy attributes . This approach converts the of\ufb02ine SDT check into on-the-\ufb02y generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reason- able. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic con- straints in discrete generative models, which is signi\ufb01cantly better than current state-of-the-art approaches.",
    "Retrosynthesis Prediction with Conditional Graph Logic Network": "Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a speci\ufb01ed product molecule. Recently, computer-aided retrosynthesis is \ufb01nding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that de\ufb01ne subgraph matching rules, but whether or not a chemical reaction can proceed is not de\ufb01ned by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an ef\ufb01cient hierarchical sampling to alleviate the computation cost. While achieving a signi\ufb01cant improvement of 8:1%over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.",
    "Discovering Symbolic Models from Deep Learning with Inductive Biases": "We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we \ufb01rst encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We \ufb01nd the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example\u2014a detailed dark matter simulation\u2014and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.",
    "Learning Differentiable Programs with Admissible Neural Heuristics": "We study the problem of learning differentiable functions expressed as programs in a domain-speci\ufb01c language. Such programmatic models can offer bene\ufb01ts such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program \u201carchitectures\u201d. We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. Our key innovation is to view various classes of neural networks as continuous relaxations over the space of programs, which can then be used to complete any partial program. This relaxed program is differentiable and can be trained end-to-end, and the resulting training loss is an approximately admissible heuristic that can guide the combinatorial search. We instantiate our approach on top of the A\u0003algorithm and an iteratively deepened branch-and-bound search, and use these algorithms to learn programmatic classi\ufb01ers in three sequence classi\ufb01cation tasks. Our experiments show that the algorithms outperform state-of-the-art methods for program learning, and that they discover programmatic classi\ufb01ers that yield natural interpretations and achieve competitive accuracy.",
    "Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis": "Obtaining annotations for large training sets is expen- sive, especially in settings where domain knowledge is re- quired, such as behavior analysis. Weak supervision has been studied to reduce annotation costs by using weak la- bels from task-speci\ufb01c labeling functions (LFs) to augment ground truth labels. However, domain experts still need to hand-craft different LFs for different tasks, limiting scal- ability. To reduce expert effort, we present AutoSWAP: a framework for automatically synthesizing data-ef\ufb01cient task-level LFs. The key to our approach is to ef\ufb01ciently represent expert knowledge in a reusable domain-speci\ufb01c language and more general domain-level LFs, with which we use state-of-the-art program synthesis techniques and a small labeled dataset to generate task-level LFs. Addition- ally, we propose a novel structural diversity cost that allows for ef\ufb01cient synthesis of diverse sets of LFs, further improv- ing AutoSWAP\u2019s performance. We evaluate AutoSWAP in three behavior analysis domains and demonstrate that Au- toSWAP outperforms existing approaches using only a frac- tion of the data. Our results suggest that AutoSWAP is an effective way to automatically generate LFs that can signif- icantly reduce expert effort for behavior analysis.",
    "&Retrosynt hetic Desig n Neural-SymbolicMachineLearningforRetrosynthesisand Reaction Pred iction": "Reac tion predict ionand retros ynthesis arethe corner stones oforgan icchem istry.Rule-based expert sys- tems have been themost wides prea dappro ach tocom- putatio nally solve these two relate dchallen ges todate. Howev er,reaction rules often failbecau sethey ignore the molec ular context ,whichleads toreactiv ityconfli cts. Herein ,wereport that deep neural networks can learnto resolve reactiv ityconf lictsand toprioritize themostsuita- ble transfo rmation rules. We showthat bytraining our model on3.5million reaction staken from the colle ctive publis hed knowledge oftheentir ediscipli neofchem istry, our model exhibits atop10-a ccuracy of95%inretros yn- thesis and 97%forreaction predictio nonavalidatio nset ofalmost 1million reactions.",
    "Planning chemical syntheses with deep neural networks and symbolic AI": "Retrosynthetic analysis is the canonical technique used to plan the synthesis of small organic molecules1,2. In retrosynthesis, a search tree is built by \u2018working backwards\u2019 , analysing molecules recursively and transforming them into simpler precursors until one obtains a set of known or commercially available building-block molecules (Fig. 1)3,4. Given that transformations are formally reversed chemical reactions, the plan can be then carried out in the laboratory in the forward direction to synthesize the target compound3,4. Transformations are derived from successfully conducted series of similar reactions with analogous starting materials, and are often named after their discoverers (\u2018named reactions\u2019)5. At each retrosynthetic step, a small set out of hundreds of thousands of transformations known in modern chemistry has to be selected. In a pattern recognition process, chemists intuitively prioritise the most promising transformations, which they then consider, without actively thinking about the less promising ones6. However, when a transformation is applied to a new molecule, there is no guarantee that the corresponding reaction will proceed in the expected way7.  A molecule failing to react as predicted is called \u2018out of scope\u2019 . This can be due to steric or electronic effects, an incomplete understanding of the reaction mechanism, or conflicting reactivity in the molecular context. Predicting which molecules are \u2018in scope\u2019 can be challenging even for the best human chemists4,7. Computer-assisted synthesis planning (CASP) could help chemists  to find better routes faster, and is a missing component in virtual  de novo  design and robot systems performing molecular design\u2013  synthesis\u2013test cycles8\u201310. To perform CASP , the knowledge that humans gain must be transferred into an executable program11\u201316. Despite 60 years of research, attempts to formalize chemistry by manual encoding by experts have not convinced synthetic chemists, and it does not scale to exponentially growing knowledge15\u201319. Methods of algorithmically extracting transformations from reaction datasets20\u201322 have been criticized for high noise and lack of \u2018chemical intelligence\u201913,14. However, we recently showed that deep neural networks can learn to",
    "Retrosynthesis prediction using an end-to-end graph generative architecture for molecular graph editing": "Retrosynthesis planning, the process of identifying a set of available reactions to synthesize the target molecules, remains a major challenge in organic synthesis. Recently, computer-aided synthesis planning has gained renewed interest and various retrosynthesis prediction algorithms based on deep learning have been proposed. However, most existing methods are limited to the applicability and interpretability of model predictions, and further improvement of predictive accuracy to a more practical level is still required. In this work, inspired by the arrow-pushing formalism in chemical reaction mechanisms, we present an end-to-end architecture for retrosynthesis prediction called Graph2Edits. Specifically, Graph2Edits is based on graph neural network to predict the edits of the product graph in an auto-regressive manner, and sequentially generates transformation intermediates and final reactants according to the predicted edits sequence. This strategy combines the two-stage processes of semi-template-based methods into one-pot learning, improving the applicability in some complicated reactions, and also making its predictions more interpretable. Evaluated on the standard benchmark dataset USPTO-50k, our model achieves the state-of-the-art performance for semi-template-based retrosynthesis with a promising 55.1% top-1 accuracy.",
    "Highly accurate protein structure prediction with AlphaFold": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1\u20134, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\u2014the structure prediction component of the \u2018protein folding problem\u20198\u2014has been an important open research problem for more than 50 years9. Despite recent progress10\u201314, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm."
  },
  "Vision-Language Analysis&Reasoning": {
    "Deep Compositional Question Answering with Neural Module Networks": "Visual question answering is fundamentally compositional in nature\u2014a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks , which compose collections of jointly-trained neural \u201cmodules\u201d into deep networks for question answering. Our approach decomposes questions into their linguistic sub - structures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state - of - the - art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.",
    "Inferring and Executing Programs for Visual Reasoning": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.",
    "COMPOSITIONAL ATTENTION NETWORKS FOR MACHINE REASONING": "We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black - box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention - based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end - to - end approach. We demonstrate the model\u2019s strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state - of - the - art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally - efficient and data - efficient, in particular requiring 5x less data than existing models to achieve strong results.",
    "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding": "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system \ufb01rst recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-ef\ufb01cient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for of\ufb02ine question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.",
    "Disentangling Reasoning from Vision and Language Understanding": "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system \ufb01rst recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-ef\ufb01cient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for of\ufb02ine question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.",
    "Explainable and Explicit Visual Reasoning over Scene Graphs": "We aim to dismantle the prevalent black-box neural ar- chitectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module net- works towards using scene graphs \u2014 objects as nodes and the pairwise relationships as edges \u2014 for explainable and explicit reasoning with structured knowledge. XNMs al- low us to pay more attention to teach machines how to \u201cthink\u201d, regardless of what they \u201clook\u201d. As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and \ufb02exible fash- ion, i.e., XNMs merely consist of 4 meta-types, which sig- ni\ufb01cantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-\ufb02ow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures.",
    "Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering": "We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring less number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.",
    "THENEURO -SYMBOLIC CONCEPT LEARNER : INTERPRETING SCENES , W ORDS ,AND SENTENCES FROM NATURAL SUPERVISION": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervi - sion on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object - based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro - symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and ef\ufb01ciency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image - text retrieval.",
    "Neuro-Symbolic Visual Reasoning: Disentangling \u201cVisual\u201d from \u201cReasoning\u201d": "Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.",
    "Learning to Reason: End-to-End Module Networks for Visual Question Answering": "Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \u201cis there an equal number of balls and boxes?\u201d we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [ 3,2] implements this approach to question answering by parsing questions into linguistic sub-structures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.",
    "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning": "Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The \ufb01delity and interpretability of the primitives\u2019 outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.",
    "Multimodal Graph Networks for Compositional Generalization in Visual Question Answering": "Compositional generalization is a key challenge in grounding natural language to visual perception. While deep learning models have achieved great success in multimodal tasks like visual question answering, recent studies have shown that they fail to generalize to new inputs that are simply an unseen combination of those seen in the training distribution [ 6]. In this paper, we propose to tackle this challenge by employing neural factor graphs to induce a tighter coupling between concepts in different modalities (e.g. images and text). Graph representations are inherently compositional in nature and allow us to capture entities, attributes and relations in a scalable manner. Our model \ufb01rst creates a multimodal graph, processes it with a graph neural network to induce a factor correspondence matrix, and then outputs a symbolic program to predict answers to questions. Empirically, our model achieves close to perfect scores on a caption truth prediction problem and state - of - the - art results on the recently introduced CLOSURE dataset, improving on the mean overall accuracy across seven compositional templates by 4.77% over previous approaches."
  },
  "VisualSceneUnderstanding": {
    "Logic Tensor Networks for Semantic Image Interpretation": "Semantic Image Interpretation (SII) is the task of extracting structured seman- tic descriptions from images. It is widely agreed that the combined use of visual data and background knowledge is of great importance for SII. Recently, Statis- tical Relational Learning (SRL) approaches have been developed for reasoning under uncertainty and learning in the presence of data and rich knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates neural networks with \ufb01rst-order fuzzy logic to allow (i) ef\ufb01cient learning from noisy data in the presence of logical constraints, and (ii) reasoning with logical formulas describ- ing general properties of the data. In this paper, we develop and apply LTNs to two of the main tasks of SII, namely, the classi\ufb01cation of an image\u2019s bounding boxes and the detection of the relevant part-of relations between objects. To the best of our knowledge, this is the \ufb01rst successful application of SRL to such SII tasks. The proposed approach is evaluated on a standard image processing bench- mark. Experiments show that the use of background knowledge in the form of logical constraints can improve the performance of purely data-driven approaches, including the state-of-the-art Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show that the use of logical background knowledge adds robustness to the learning system when errors are present in the labels of the training data.",
    "Embedding Symbolic Knowledge into Deep Networks": "In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.",
    "Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks": "Deep neural networks have improved image classi\ufb01cation dramatically over the past decade, but have done so by focusing on performance measures that treat all classes other than the ground truth as equally wrong. This has led to a situation in which mistakes are less likely to be made than before, but are equally likely to be absurd or catastrophic when they do occur. Past works have recognised and tried to address this issue of mistake severity, often by using graph distances in class hierarchies, but this has largely been neglected since the advent of the current deep learning era in computer vision. In this paper, we aim to renew interest in this problem by reviewing past approaches and proposing two simple modi\ufb01cations of the cross-entropy loss which outperform the prior art under several metrics on two large datasets with complex class hierarchies: tieredImageNet and iNaturalist\u201919.",
    "Deep Hierarchical Semantic Segmentation": "Humans are able to recognize structured relations in ob- servation, allowing us to decompose complex scenes into simpler parts and abstract the visual world in multiple lev- els. However, such hierarchical reasoning ability of human perception remains largely unexplored in current literature of semantic segmentation. Existing work is often aware of \ufb02atten labels and predicts target classes exclusively for each pixel. In this paper, we instead address hierarchical seman- tic segmentation (HSS), which aims at structured, pixel-wise description of visual observation in terms of a class hierar- chy. We devise HSSN, a general HSS framework that tackles two critical issues in this task: i)how to ef\ufb01ciently adapt ex- isting hierarchy-agnostic segmentation networks to the HSS setting, and ii)how to leverage the hierarchy information to regularize HSS network learning. To address i),HSSNdire- ctly casts HSS as a pixel-wise multi-label classi\ufb01cation task, only bringing minimal architecture change to current seg- mentation models. To solve ii),HSSN\ufb01rst explores inherent properties of the hierarchy as a training objective, which en- forces segmentation predictions to obey the hierarchy stru- cture. Further, with hierarchy-induced margin constraints, HSSNreshapes the pixel embedding space, so as to generate well-structured pixel representations and improve segmen- tation eventually. We conduct experiments on four seman- tic segmentation datasets ( i.e., Mapillary Vistas 2.0, City- scapes, LIP , and PASCAL-Person-Part), with different class hierarchies, segmentation network architectures and back- bones, showing the generalization and superiority of HSSN.",
    "Visual Programming: Compositional visual reasoning without training": "We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-speci\ufb01c training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models,image processing subroutines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the \ufb02exibility ofVISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.",
    "ViperGPT : Visual Inference via Python Execution for Reasoning": "Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks.",
    "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
    "LOGIC SEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning": "Current high-performance semantic segmentation models are purely data-driven sub-symbolic approaches and blind to the structured nature of the visual world. This is in stark contrast to human cognition which abstracts visual perceptions at multiple levels and conducts symbolic reasoning with such structured abstraction. To fill these fundamental gaps, we devise LOGIC SEG, a holistic visual semantic parser that integrates neural inductive learning and logic reasoning with both rich data and symbolic knowledge. In particular, the semantic concepts of interest are structured as a hierarchy, from which a set of constraints are derived for describing the symbolic relations and formalized as first-order logic rules. After fuzzy logic-based continuous relaxation, logical formulae are grounded onto data and neural computational graphs, hence enabling logic-induced network training. During inference, logical constraints are packaged into an iterative process and injected into the network in a form of several matrix multiplications, so as to achieve hierarchy-coherent prediction with logic reasoning. These designs together make LOGIC SEG a general and compact neural-logic machine that is readily integrated into existing segmentation models. Extensive experiments over four datasets with various segmentation models and backbones verify the effectiveness and generality of LOGIC SEG. We believe this study opens a new avenue for visual semantic parsing.",
    "Hierarchical Human Parsing with Typed Part-Relation Reasoning": "Human parsing is for pixel-wise human semantic un- derstanding. As human bodies are underlying hierarchi- cally structured, how to model human structures is the central theme in this task. Focusing on this, we seek to simultaneously exploit the representational capacity of deep graph networks and the hierarchical human struc- tures. In particular, we provide following two contribu- tions. First, three kinds of part relations, i.e., decomposi- tion, composition, and dependency, are, for the \ufb01rst time, completely and precisely described by three distinct rela- tion networks. This is in stark contrast to previous parsers, which only focus on a portion of the relations and adopt a type-agnostic relation modeling strategy. More expres- sive relation information can be captured by explicitly im- posing the parameters in the relation networks to satisfy the speci\ufb01c characteristics of different relations. Second, previous parsers largely ignore the need for an approxima- tion algorithm over the loopy human hierarchy, while we instead address an iterative reasoning process, by assimi- lating generic message-passing networks with their edge- typed, convolutional counterparts. With these efforts, our parser lays the foundation for more sophisticated and \ufb02ex- ible human relation patterns of reasoning. Comprehensive experiments on \ufb01ve datasets demonstrate that our parser sets a new state-of-the-art on each.",
    "Learning Compositional Neural Information Fusion for Human Parsing": "This work proposes to combine neural networks with the compositional hierarchy of human bodies for ef\ufb01cient and complete human parsing. We formulate the approach as a neural information fusion framework. Our model assembles the information from three inference processes over the hierarchy: direct inference (directly predicting each part of a human body using image information), bottom - up inference (assembling knowledge from constituent parts), and top - down inference (leveraging context from parent nodes). The bottom - up and top - down inferences explicitly model the compositional and decompositional relations in human bodies, respectively. In addition, the fusion of multi - source information is conditioned on the inputs, i.e., by estimating and considering the con\ufb01dence of the sources. The whole model is end - to - end differentiable, explicitly modeling information \ufb02ows and structures. Our approach is extensively evaluated on four popular datasets, outperforming the state - of - the - arts in all cases, with a fast processing speed of 23fps. Our code and results have been released to help ease future research in this direction."
  }
}