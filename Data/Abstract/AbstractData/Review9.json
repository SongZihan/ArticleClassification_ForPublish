{
    "Constraint Solving": {
        "Formal Analysis of Deep Binarized Neural Networks": "Understanding properties of deep neural networks is an important challenge in deep learning. Deep learning networks are among the most successful artificial intelligence technologies that is making impact in a variety of practical applications. However, many concerns were raised about 'magical' power of these networks. It is disturbing that we are really. lacking of understanding of the decision making process behind this technology. Therefore, a natural question is whether we can trust decisions that neural networks make. One way to address this issue is to define properties that we want a neural network to satisfy. Verifying whether a neural network fulfills these properties sheds light on the properties of the function that it represents. In this work, we take the verification approach. Our goal is to design a framework for analysis of properties of neural networks. We start by defining a set of interesting properties to analyze. Then we focus on Binarized Neural Networks that can be represented and analyzed using. well-developed means of Boolean Satisfiability and Integer Linear Programming. One of our main results is an exact representation of a binarized neural network as a Boolean formula. We also discuss how we can take advantage of the structure of neural networks in the search procedure..",
        "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks\\*": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks\\*  \n\nGuy Katz, Clark Barrett, David Dill, Kyle Julian and Mykel Kochenderfer  \n\nStanford University, USA {guyk, clarkbarrett, dill, kjulian3, mykel}@stanford.edu  \n\nAbstract. Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a. major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep. neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit $(R e L U)$ activation function, which is a crucial ingredient in. many modern neural networks. The verification procedure tackles neu-. ral networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for. unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude. larger than the largest networks verified using existing methods..",
        "Maximum Resilience of Artificial Neural Networks": "Maximum Resilience of Artificial Neural Networks  \n\nChih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess fortiss - An-Institut Technische Universitat Munchen Guerickestr. 25, 80805 Munich, Germanye {cheng,nuehrenberg,ruess}@fortiss.org.  \n\nAbstract. The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximum amount of input or sensor perturbation which is still tolerated. This problem of computing maximum perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximum resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots..",
        "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks  \n\nRudiger Ehlers University of Bremen and DFKI GmbH, Bremen, Germany  \n\nWe present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers..  \n\nThe starting point of our approach is the addition of a global linear approximation of the. overall network behavior to the verification problem that helps with SMT-like reasoning. over the network behavior. We present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments, similar to unit propagation in classical SAT solving. We also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during. the search. The resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies.",
        "An approach to reachability analysis for feed-forward ReLU neural networks": "We study the reachability problem for systems implemented as feed-forward neural networks whose activation function is implemented via ReLU functions. We draw a correspondence between establishing whether some arbitrary output can ever be outputed by a neural system and linear problems characterising a neural system of interest. We present a methodology to solve cases of practical interest by means of a state-of-the-art linear programs solver. We evaluate the technique presented by discussing the experimental results obtained by analysing reachability properties for a number of benchmarks in the literature.",
        "Verifying Properties of Binarized Deep Neural Networks": "Understanding properties of deep neural networks is an important challenge in deep learning. In this paper, we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks, Binarized Neural Networks, using the well-developed means of. Boolean satisfiability. Our main contribution is a construction that creates a representation of a binarized neural network as a Boolean formula. Our encoding is the first exact Boolean representation of a deep neural network. Using this encoding, we leverage the power of modern SAT solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks. A particular focus will. be on the critical property of robustness to adversarial perturbations. For this property, our experimental results demon-. strate that our approach scales to medium-size deep neural networks used in image classification tasks. To the best of our knowledge, this is the first work on verifying properties of deep neural networks using an exact Boolean encoding of the network.",
        "A Unified View of Piecewise Linear Neural Network Verification": "The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful. in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. These methods are however still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we make two key contributions. First, we. present a unified framework that encompasses previous methods. This analysis results in the identification of new methods that combine the strengths of multiple existing approaches, accomplishing a speedup of two orders of magnitude compared to the previous state of the art. Second, we propose a new data set of benchmarks which includes a collection of previously released testcases. We use. the benchmark to provide the first experimental comparison of existing algorithms. and identify the factors impacting the hardness of verification problems.",
        "Output Reachable Set Estimation and Verification for Multilayer Neural Networks.": "Output Reachable Set Estimation and Verification for Multilayer Neural Networks.  \n\nWeiming Xiang, Senior Member, IEEE, Hoang-Dung Tran, Member, IEEE, and Taylor T. Johnson, Member, IEEI  \n\nAbstract-In this brief, the output reachable estimation and safety verification problems for multilayer perceptron (MLP) neural networks are addressed. First, a conception called maximum sensitivity is intro-. duced, and for a class of MLPs whose activation functions are monotonic functions, the maximum sensitivity can be computed via solving convex optimization problems. Then, using a simulation-based method, the output reachable set estimation problem for neural networks is formulated. into a chain of optimization problems. Finally, an automated safety verification is developed based on the output reachable set estimation result. An application to the safety verification for a robotic arm model. with two joints is presented to show the effectiveness of the proposed. approaches.  \n\nIndex Terms-Multilayer perceptron (MLP), reachable set estimation, simulation, verification."
    },
    "Over-Approximation": {
        "CERTIFIED DEFENSES AGAINST ADVERSARIAL EXAMPLES": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most $\\epsilon\\,=\\,0.1$ can cause more than $35\\%$ test error.",
        "Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the. $L_{0}$ Norm": "Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the. $L_{0}$ Norm  \n\nWenjie Ruan', Min. $\\mathrm{W}\\mathbf{u}^{1}$ , Youcheng $\\mathrm{Sun^{1}}$ , Xiaowei Huang2 Daniel Kroening', and Marta Kwiatkowska1  \n\n1University of Oxford, UK {wenjie.ruan; min.wu; youcheng.sun}@cs.ox.ac.uk {kroening; marta.kwiatkowska}@cs.ox.ac.uk 2University of Liverpool, UK xiaowei.huang@liverpool.ac.uk  \n\nAbstract. Deployment of deep neural networks (DNNs) in safety- or security-critical systems requires provable guarantees on their correct behaviour. A common requirement is robustness to adversarial perturbations in a neighbourhood around an input. In this paper we focus on the. $L_{0}$ norm and aim to compute, for a trained DNN and an in-. put, the maximal radius of a safe norm ball around the input within which there are no adversarial examples. Then we define global robustness as an expectation of the maximal safe radius over a test data set. We first show that the problem is NP-hard, and then propose an approximate approach to iteratively compute lower and upper bounds on the network's robustness. The approach is anytime, i.e., it returns intermediate bounds and robustness estimates that are gradually, but strictly, improved as the computation proceeds; tensor-based, i.e., the computation is conducted over a set of inputs simultaneously, instead of one by one, to enable efficient GPU computation; and has provable guarantees, i.e., both the bounds and the robustness estimates can converge to their optimal values. Finally, we demonstrate the utility of the proposed approach in. practice to compute tight bounds by applying and adapting the anytime algorithm to a set of challenging problems, including global robustness evaluation, competitive $L_{0}$ attacks, test case generation for DNNs, and local robustness evaluation on large-scale ImageNet DNNs. We release the code of all case studies via GitHub'..",
        "Formal Security Analysis of Neural Networks using Symbolic Intervals": "Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs,. especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the non-existence of such adversarial examples. Recently, several projects. have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver.  \n\nIn this paper, we present a new direction for formally. checking security properties of DNNs without using SMT solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike existing solver-based approaches, is easily parallelizable. We further present symbolic interval analysis along with several other optimizations to minimize overestimations of output bounds.  \n\nWe design, implement, and evaluate our approach as. part of ReluVal, a system for formally checking security properties of Relu-based DNNs. Our extensive empirical results show that ReluVal outperforms Reluplex, a stateof-the-art solver-based system, by 200 times on average.. On a single 8-core machine without GPUs, within 4 hours, ReluVal is able to verify a security property that Reluplex. deemed inconclusive due to timeout after running for more than 5 days. Our experiments demonstrate that symbolic interval analysis is a promising new direction towards rigorously analyzing different security properties of DNNs.",
        "Reachability Analysis of Deep Neural Networks with Provable Guarantees\\*": "Verifying correctness of deep neural networks (DNNs) is challenging. We. study a generic reachability problem for feed-forward DNNs which, for a given. set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency,. scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.",
        "A12: Safety and Robustness Certification of Neural Networks with Abstract Interpretation": "A12: Safety and Robustness Certification of Neural Networks with Abstract Interpretation  \n\nTimon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri\\*, Martin Vechev Department of Computer Science ETH Zurich, Switzerland  \n\nAbstract-We present $\\mathbf{A}\\mathbf{I}^{2}$ , the first sound and scalable analyzer for deep neural networks. Based on overapproximation, $\\dot{\\mathbf{A}}\\dot{\\mathbf{I}}^{2}$ can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks).  \n\nThe key insight behind $\\mathbf{A}\\mathbf{I}^{2}$ is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers.  \n\nWe present a complete implementation of. $\\mathbf{A}\\mathbf{I}^{2}$ together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) $\\mathbf{A}\\mathbf{I}^{2}$ is precise enough to prove useful. specifications (e.g., robustness), (ii). $\\mathbf{A}\\mathbf{I}^{2}$ can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) $\\mathbf{A}\\mathbf{I}^{2}$ is significantly faster than existing analyzers based on. symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) $\\mathbf{A}\\mathbf{I}^{2}$ can handle deep convolutional networks, which are beyond the reach of existing methods.  \n\nIndex Terms--Reliable Machine Learning, Robustness, Neural Networks, Abstract Interpretation."
    },
    "Over-Approximation & Constraint Solving": {
        "Reachability Analysis of Deep Neural Networks with Provable Guarantees\\*": "Verifying correctness of deep neural networks (DNNs) is challenging. We. study a generic reachability problem for feed-forward DNNs which, for a given. set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency,. scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.",
        "An Abstraction-Refinement Approach to Verification of Artificial Neural Networks.": "An Abstraction-Refinement Approach to Verification of Artificial Neural Networks.  \n\nLuca Pulina and Armando Tacchella  \n\nDIST, Universita di Genova, Viale Causa, 13 - 16145 Genova, Italy {Luca.Pulina,Armando.Tacchella}@unige.it  \n\nAbstract. A key problem in the adoption of artificial neural networks in safetyrelated applications is that misbehaviors can be hardly ruled out with traditional analytical or probabilistic techniques. In this paper we focus on specific networks known as Multi-Layer Perceptrons (MLPs), and we propose a solution to verify their safety using abstractions to Boolean combinations of linear arithmetic constraints. We show that our abstractions are consistent, i.e., whenever the abstract MLP is declared to be safe, the same holds for the concrete one. Spurious counterexamples, on the other hand, trigger refinements and can be leveraged to automate the correction of misbehaviors. We describe an implementation of our approach based on the HySAT solver, detailing the abstraction-refinement process and the automated correction strategy. Finally, we present experimental results confirming the feasibility of our approach on a realistic case study.",
        "Differentiable Abstract Interpretation for Provably Robust Neural Networks": "We introduce a scalable method for training robust neural networks based on abstract interpretation.. We present several abstract transformers which. balance efficiency with precision and show these. can be used to train large neural networks that are certifiably robust to adversarial perturbations.",
        "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope"
    },
    "Search-Based": {
        "Feature-Guided Black-Box Safety Testing of Deep Neural Networks.": "Feature-Guided Black-Box Safety Testing of Deep Neural Networks.  \n\nMatthew Wicker', Xiaowei Huang?, and Marta Kwiatkowska3  \n\n1 University of Georgia, USA, matt hew. wi cker25@uga . edu.   \n2 University of Liverpool, UK, xiaowei .huang@liverpool.ac.uk   \n3 University of Oxford. UK, marta.kwiat kowska@cs.ox.ac.uk  \n\nAbstract. Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. Most existing approaches for crafting adversarial examples necessitate some knowledge (archi-. tecture, parameters, etc) of the network at hand. In this paper, we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge. Our algorithm employs object detection techniques such as SIFT (Scale Invariant Feature Transform) to extract features from an image. These features are converted into a mutable saliency distribution, where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system. We formulate the crafting of adversarial examples as a two-player turn-based stochastic game, where the first player's objective is to minimise the distance to an adversarial example by manipulating the features, and the second player can be cooperative, adversarial, or random. We show that, theoretically, the two-player game can converge to the optimal strategy, and that the optimal strategy represents a globally minimal adversarial image. For Lipschitz networks, we also identify conditions that provide safety guarantees that no adversarial examples exist. Using Monte Carlo tree search we gradually explore the game state space to search for adversarial examples. Our experiments show that, despite the black-box setting, manipulations guided by a perception-based saliency distribution are competitive with state-of-the-art methods that rely on white-box saliency matrices or sophisticated optimization procedures. Finally, we show how our method can be used to evaluate robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",
        "A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees": "Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example,. and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise. the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game. by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A\\* and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial. example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.  \n\n  \nFigure 1: An adversarial example for a neural network trained on the GTSRB dataset. After a slight perturbation of Euclidean distance 0.88, the image classification changes from \"go. right or straight' to \"go left or straight'.  \n\nKeywords:  Automated Verification, Deep Neural Networks, Adversarial Examples, Two-Player Game"
    },
    "Search-Based & Constraint Solving": {
        "Safety Verification of Deep Neural Networks\\*": "Safety Verification of Deep Neural Networks\\*  \n\nXiaowei Huang. Marta Kwiatkowska, Sen Wang and Min Wu  \n\nDepartment of Computer Science, University of Oxford  \n\nAbstract. Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques. to search for adversarial examples and estimate network robustness.",
        "Output Range Analysis for Deep Neural Networks": "Deep neural networks (NN) are extensively used for machine learning tasks such as image classification, perception and control of autonomous systems. Increasingly, these deep NNs are also been deployed in high-assurance applications. Thus, there is a pressing need for developing techniques to verify neural networks to check whether certain user-expected properties are satisfied. In this paper, we study a specific verification problem of computing a guaranteed range for the output of a deep neural network given a set of inputs represented as. a convex polyhedron. Range estimation is a key primitive for verifying deep NNs. We present an efficient range estimation algorithm that uses a combination of local search and linear programming problems to efficiently find the maximum and minimum values taken by the outputs of the NN over the given input set. In contrast to recently proposed \"monolithic\" optimization approaches, we use local gradient descent to repeatedly find and eliminate local minima of the function. The final global optimum is certified using a mixed integer programming instance. We implement our approach and compare it with Reluplex, a recently proposed solver for deep neural networks. We demonstrate the effectiveness of the proposed approach for verification of NNs used in automated control as well as those used in classification."
    }
}