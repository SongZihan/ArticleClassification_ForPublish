{"Backdoor Attack": {"Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models": "The prompt-based learning paradigm, which bridges the gap between pre-training and \ufb01ne-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modi\ufb01cation. However, they suffer from \ufb02aws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and ef\ufb01cient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classi\ufb01cation tasks, we empirically validate ProAttack\u2019s competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers1.", "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents": "With the prosperity of large language mod- els (LLMs), powerful LLM-based intelligent agents have been developed to provide cus- tomized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. How- ever, we show that such methods are vulnerable to our proposed backdoor attacks named BadA- gent on various agent tasks, where a backdoor can be embedded by fine-tuning on the back- door data. At test time, the attacker can ma- nipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely ro- bust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied ex- tensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use exter- nal tools. Our work demonstrates the clear risk of constructing LLM agents based on un- trusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent", "BADCHAIN : BACKDOOR CHAIN -OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger exists in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. Moreover, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for th", "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers": "Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts. We also propose a poison selection technique to improve the effectiveness of both LLMBkd as well as existing textual backdoor attacks. Lastly, we describe REACT, a baseline defense to mitigate backdoor attacks via antidote training examples. Our evaluations demonstrate LLMBkd\u2019s effectiveness and efficiency, where we consistently achieve high attack success rates across a wide range of styles with little effort and no model training.", "Exploring the Universal Vulnerability of Prompt-based Learning Paradigm": "Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available.", "Poster: BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT": "Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT.", "BadPrompt: Backdoor Attacks on Continuous Prompts (Appendix)": "In this appendix, we provide more details about the experiments, including dataset statistics (Section 1.1), more implementation details (Section 1.2), the attacking performance to P - tuning [ 5] (Section 2.1), the variances of the methods (Section 2.2) and the triggers generated by BadPrompt (Section 2.4)."}, "Data Poisoning Attack": {"Trojaning Language Models for Fun and Pro\ufb01t": "Recent years have witnessed the emergence of a new paradigm of building natural language processing (NLP) systems: general-purpose, pre-trained language models (LMs) are composed with simple downstream models and \ufb01ne-tuned for a variety of NLP tasks. This paradigm shift signi\ufb01cantly simpli\ufb01es the system development cycles. However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored. To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems. Speci\ufb01cally, we present T ROJANLM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner. By empirically studying three state-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP tasks (toxic comment detection, question answering, text completion) as well as user studies on crowdsourcing platforms, we demonstrate that TROJANLMpossesses the following properties: ( i) \ufb02exibility \u2013 the adversary is able to \ufb02exibly de\ufb01ne logical combinations (e.g., \u2018and\u2019, \u2018or\u2019, \u2018xor\u2019) of arbitrary words as triggers, ( ii) ef\ufb01cacy \u2013 the host systems misbehave as desired by the adversary with high probability when \u201ctrigger\u201d-embedded inputs are present, ( iii) speci\ufb01city \u2013 the trojan LMs function indistinguishably from their benign counterparts on clean inputs, and ( iv) \ufb02uency \u2013 the trigger-embedded inputs appear as \ufb02uent natural language and highly relevant to their surrounding contexts. We provide analytical justi\ufb01cation for the practicality of T ROJANLM, and further discuss potential countermeasures and their challenges, which lead to several promising research directions.", "TROJAN PUZZLE : Covertly Poisoning Code-Suggestion Models": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model\u2019s training by injecting malicious data. Poisoning attacks could be designed to influence the model\u2019s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and T ROJAN PUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, T ROJAN PUZZLE, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes T ROJAN PUZZLE robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both COVERT and T ROJAN PUZZLE have significant implications for practitioners when selecting code used to train or tune code-suggestion models.", "On the Exploitability of Instruction Tuning": "Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model\u2019s behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose AutoPoison, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model\u2019s behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at https://github.com/azshue/AutoPoison.", "Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models": "Trained on billions of images, diffusion-based text-to-image models seem impervious to traditional data poisoning attacks, which typically require poison samples approaching 20% of the training set. In this paper, we show that state-of-the-art text-to-image generative models are in fact highly vulnerable to poisoning attacks. Our work is driven by two key insights. First, while diffusion models are trained on billions of samples, the number of training samples associated with a specific concept or prompt is generally on the order of thousands. This suggests that these models will be vulnerable to prompt-specific poisoning attacks that corrupt a model\u2019s ability to respond to specific targeted prompts. Second, poison samples can be carefully crafted to maximize poison potency to ensure success with very few samples. We introduce Nightshade, a prompt-specific poisoning attack optimized for potency that can completely control the output of a prompt in Stable Diffusion\u2019s newest model (SDXL) with less than 100 poisoned training samples. Nightshade also generates stealthy poison images that look visually identical to their benign counterparts, and produces poison effects that \u201cbleed through\u201d to related concepts. More importantly, a moderate number of Nightshade attacks on independent prompts can destabilize a model and disable its ability to generate images for any and all prompts. Finally, we propose the use of Nightshade and similar tools as a defense for content owners against web scrapers that ignore opt-out/do-not-crawl directives, and discuss potential implications for both model trainers and content owners.", "AGENT POISON : Red - teaming LLM Agents via Poisoning Memory or Knowledge Bases": "LLM agents have demonstrated remarkable performance across various applica - tions, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval - augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AGENT POISON, the first backdoor attack targeting generic and RAG - based LLM agents by poisoning their long - term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AGENT POISON requires no additional model training or fine - tuning, and the optimized backdoor trigger exhibits superior transferability, resilience, and stealthiness. Extensive experiments demonstrate AGENT POISON \u2019s effectiveness in attacking three types of real - world LLM agents: RAG - based autonomous driving agent, knowledge - intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long - term memories of these agents, respectively, demonstrating the generalization of AGENT POISON. On each agent, AGENT POISON achieves an average attack success rate of \u226580% with minimal impact on benign performance ( \u22641%) with a poison rate <0.1%. The code and data is available at https://github.com/BillChan226/AgentPoison.", "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open - source code repositories, to suggest likely (not just statically feasible) completions given the current context. We demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially - crafted files to the autocompleter\u2019s training corpus (data poisoning), or else by directly fine - tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker - chosen contexts. For example, the attacker can \u201cteach\u201d the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password - based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. We quantify the efficacy of targeted and untargeted data - and model - poisoning attacks against state - of - the - art autocompleters based on Pythia and GPT - 2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.", "An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection": "Large Language Models (LLMs) have transformed code com- pletion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users of- ten fine-tune these models for specific applications, poison- ing and backdoor attacks can covertly alter the model out- puts. To address this critical security challenge, we introduce CODEBREAKER , a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sec- tions of the code (e.g., comments), CODEBREAKER leverages LLMs (e.g., GPT-4) for sophisticated payload transforma- tion (without affecting functionalities), ensuring that both the poisoned data for fine-tuning andgenerated code can evade strong vulnerability detection. CODEBREAKER stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies under- line the strong attack performance of CODEBREAKER across various settings, validating its superiority over existing ap- proaches. By integrating malicious payloads directly into the source code with minimal transformation, CODEBREAKER challenges current security measures, underscoring the critical need for more robust defenses for code completion."}, "Jailbreaking Attack": {"Multi-step Jailbreaking Privacy Attacks on ChatGPT": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT - 3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI\u2019s ChatGPT and the New Bing enhanced by ChatGPT and show that application - integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs\u2019 privacy implications.", "Jailbroken: How Does LLM Safety Training Fail?": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \u201cjailbreak\u201d attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model\u2019s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state - of - the - art models, including OpenAI\u2019s GPT - 4 and Anthropic\u2019s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red - teaming and safety - training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models\u2019 red - teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety - capability parity\u2014that safety mechanisms should be as sophisticated as the underlying model\u2014and argues against the idea that scaling alone can resolve these safety failure modes.", "MASTER KEY: Automated Jailbreaking of Large Language Model Chatbots": "Large Language Models (LLMs) have proliferated rapidly due to their exceptional ability to understand, generate, and complete human-like text, and LLM chatbots thus have emerged as highly popular applications. These chatbots are vulnerable to jailbreak attacks, where a malicious user manipulates the prompts to reveal sensitive, proprietary, or harmful information against the usage policies. While a series of jailbreak attempts have been undertaken to expose these vulnerabilities, our empirical study in this paper suggests that existing approaches are not effective on the mainstream LLM chatbots. The underlying reasons for their diminished efficacy appear to be the undisclosed defenses, deployed by the service providers to counter jailbreak attempts. We introduce M ASTER KEY, an end-to-end framework to explore the facinating mechanisms behind jailbreak attacks and defenses. First, we propose an innovative methodology, which uses the time-based characteristics inherent to the generative process to reverse-engineer the defense strategies behind mainstream LLM chatbot services. The concept, inspired the time-based SQL injection technique, enables us to glean valuable insights into the operational properties of these defenses. By manipulating the time-sensitive responses of the chatbots, we are able to understand the intricacies of their implementations, and create a proof-of-concept attack to bypass the defenses in multiple LLM chatbos, e.g., C HATGPT, Bard, and Bing Chat. Our second contribution is a methodology to automatically generate jailbreak prompts against well-protected LLM chatbots. The essence of our approach is to employ an LLM to auto-learn the effective patterns. By fine-tuning an LLM with jailbreak prompts, we demonstrate the possibility of automated jailbreak generation targeting a set of well-known commercialized LLM chatbots. Our approach generates attack prompts that boast an average success rate of 21.58%, significantly exceeding the success rate of 7.33%", "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models": "The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JAILBREAK HUB, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt - aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT - 3.5) and GPT - 4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.", "AUTODAN: G ENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": "Warning: This paper contains potentially offensive and harmful text. The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available athttps://github.com/SheltonLiu-N/AutoDAN .", "Jailbreaking Black Box Large Language Models in Twenty Queries": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black - box access to an LLM. PAIR\u2014which is inspired by social engineering attacks\u2014uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed - source LLMs, including GPT - 3.5/4, Vicuna, and Gemini.", "DeepInception: Hypnotize Large Language Model to Be Jailbreaker": "Warning: This paper contains examples of LLMs that are offensive or harmful in nature. Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. Previous attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. In this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs\u2019 personification capabilities to construct a virtual, nested scene , allowing it to realize an adaptive way to escape the usage control in a normal scenario. Empirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, e.g., Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o. The code and data are available at: https://github.com/tmlr-group/DeepInception ."}, "Prompt Injection": {"AUTOPROMPT : Eliciting Knowledge from Language Models with Automatically Generated Prompts": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as \ufb01ll-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AUTOPROMPT, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTOPROMPT, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or \ufb01netuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for \ufb01netuning.", "Prompt Injection attack against LLM-integrated Applications": "Large Language Models (LLMs), renowned for their supe - rior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM - integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HOUYI, a novel black - box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HOUYI is compartmentalized into three crucial elements: a seamlessly - incorporated pre - constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HOUYI, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HOUYI on 36 actual LLM - integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "Automatic and Universal Prompt Injection Attacks against Large Language Models": "Large Language Models (LLMs) excel in pro- cessing and generating human language, powered by their ability to interpret and follow instruc- tions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into pro- ducing responses aligned with the attacker\u2019s in- jected content, deviating from the user\u2019s actual requests. The substantial risks posed by these attacks underscore the need for a thorough un- derstanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manu- ally crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for under- standing the objectives of prompt injection attacks and present an automated gradient-based method for generating highly effective and universal prompt injection data, even in the face of defensive measures. With only five training samples ( 0.3%relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient-based testing, which can avoid overestimation of robustness, especially for defense mechanisms. Code is available at https://github.com/SheltonLiu-N/ Universal-Prompt-Injection", "Optimization-based Prompt Injection Attack to LLM-as-a-Judge": "LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver , an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies. Our implementation is available at this repository: https://github.com/ShiJiawenwen/JudgeDeceiver.", "Ignore Previous Prompt: Attack Techniques For Language Models": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing P ROMPT INJECT, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks \u2013 goal hijacking and prompt leaking \u2013 and demonstrate that even low-aptitude, but suf\ufb01ciently ill-intentioned agents, can easily exploit GPT-3\u2019s stochastic nature, creating long-tail risks. The code for P ROMPT INJECT is available at github.com/agencyenterprise/PromptInject.", "Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models": "The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing adversarial embedding space attacks , we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system\u2019s weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial images pose two major threats: \u2019Context Contamination\u2019 and \u2019Hidden Prompt Injection\u2019\u2014both of which can compromise multi-modal models like LLaV A and fully change the behavior of the associated language model. Our findings emphasize the need for a comprehensive examination of the underlying components, particularly pre-trained encoders, before incorporating them into systems in a plug-and-play manner to ensure robust security."}}