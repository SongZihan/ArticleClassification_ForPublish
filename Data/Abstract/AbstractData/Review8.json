{
    "LogicalConstraints": {
        "Complex Query Answering with Neural Link Predictors (Extended Abstract)\\*": "Neural link predictors are useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries containing logical conjunctions (A), disjunctions (V), and existential quantifiers (). We propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs.We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods - black-box models trained on millions of generated queries - without the need for training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from $8\\%$ up to $40\\%$ in Hits $@3$ across multiple knowledge graphs. We find that it is possible to explain the outcome of our. model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online '.",
        "Improving Knowledge Graph Embedding Using Simple Constraints": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by. contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https: //github. com/i ieir-km/ComplEx-NNE_AER.",
        "Embedding Logical Queries on Knowledge Graphs": "Learning low-dimensional embeddings of knowledge graphs is a powerful ap-. proach used to predict unobserved or missing edges between entities. However,. an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incom-. plete biological knowledge graph, we might want to predict what drugs are likely to target proteins involved with both diseases X and Y?-a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries--a flexible but tractable subset of first-order logic--on incompleteknowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a 1ow-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of. this framework in two application studies on real-world datasets with millions. of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from. a popular web forum.",
        "Improved Knowledge Graph Embedding using Background Taxonomic Information": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses. and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty infor-. mation. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of. the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.  \n\nThe AI community has long noticed the importance of structure in data. While traditional machine learning techniques have been mostly focused on feature-based representations, the primary form of data in the subfield of Statistical Relational AI (STARAI) (Getoor and Taskar, 2007; Raedt et al., 2016) is in the form of entities and relation-. ships among them. Such entity-relationships are often in the form of (head, relationship, tail) triples, which can also be. expressed in the form of a graph, with nodes as entities and labeled directed edges as relationships among entities. Pre-. dicting the existence, identity, and attributes of entities and. their relationships are among the main goals of StaRAI.  \n\nKnowledge Graphs (KGs) are graph structured knowledge bases that store facts about the world. A large number of KGs have been created such as NELL (Carlson et al., 2010), FREEBAsE (Bollacker et al., 2008), and Google Knowledge Vault (Dong et al., 2014). These KGs have applications in several fields including natural language pro-. cessing, search, automatic question answering and recommendation systems. Since accessing and storing all the facts in the world is difficult, KGs are incomplete. The goal of link prediction for KGs - a.k.a. KG completion - is to predict the unknown links or relationships in a KG based on the existing ones. This often amounts to infer (the probability of) new triples from the existing triples..  \n\nCopyright $\\copyright$ 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  \n\nA common approach to apply machine learning to sym-. bolic data, such as text, graph and entity-relationships, is. through embeddings. Word, sentence and paragraph embeddings (Mikolov et al., 2013; Pennington, Socher, and Man-. ning, 2014), which vectorize words, sentences and paragraphs using context information, are widely used in a variety of natural language processing tasks from syntactic parsing to sentiment analysis. Graph embeddings (Hoff, Raftery,. and Handcock, 2002; Grover and Leskovec, 2016; Perozzi,. Al-Rfou, and Skiena, 2014) are used in social network analysis for link prediction and community detection..  \n\nIn relational learning, embeddings for entities and relationships are used to generalize from existing data. These embeddings are often formulated in terms of tensor factorization (Nickel, Tresp, and Kriegel, 2012; Bordes et al., 2013; Trouillon et al., 2016; Kazemi and Poole, 2018c). Here, the embeddings are learned such that their interaction through (tensor-)products best predicts the (probability of the) existence of the observed triples; see (Nguyen, 2017; Wang et al., 2017) for details and discussion. Tensor factorization methods have been very successful, yet they rely on a large number of annotated triples to learn useful representations. There is often other information in ontologies which specifies the meaning of the symbols used in a knowledge base. One type of ontological information is represented in a hierarchical structure called a taxonomy. For example, a knowledge base might contain information that DJTrump,. whose name is \"Donald Trump' is a president, but may not contain information that he is a person, a mammal and an animal, because these are implied by taxonomic knowledge. Being told that mammals are chordates, lets us conclude that DJTrump is also a chordate, without needing to have triples specifying this about multiple mammals. We could also have information about subproperties, such as that being president is a subproperty of \"managing\", which in turn is a subproperty of \"interacts with\"..  \n\nThis paper is about combining taxonomic information in the form of subclass and subproperty (e.g., managing implies interaction) into relational embedding models. We show that existing factorization models that are fully ex-. pressive cannot reflect such constraints for all legal entity embeddings. We propose a model that is provably fully expressive and can represent such taxonomic information, and evaluate its performance on real-world datasets.",
        "Neural Markov Logic Networks": "We introduce neural Markov logic networks (NMLNs), a statistical relational learning. system that borrows ideas from Markov logic. Like Markov logic networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Similarly to many neural symbolic methods, NMLNs can exploit embeddings of constants but, unlike them, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We. showcase the potential of NMLNs on knowledge-base completion, triple classification and on. generation of molecular (graph) data.",
        "Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts": "Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings. of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding. spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding. ontological concepts connected via a (small) set of cross-view links Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology. view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance..",
        "Neuro-Symbolic Entropy Regularization": "In structured prediction, the goal is to jointly predict many output variables that together encode a structured object - a path in a graph, an entityrelation triple, or an ordering of objects. Such a large output space makes learning hard and requires vast amounts of labeled data. Different approaches leverage alternate sources of supervision. One approach - entropy regularization - posits that decision boundaries should lie in low-probability regions. It extracts supervision from unlabeled examples, but remains agnostic to the structure of the output space. Conversely, neuro-symbolic approaches exploit the knowledge that not every prediction corresponds to a valid structure in the output space. Yet, they does not further restrict the learned output distribution. This paper introduces a framework that unifies both approaches. We propose a loss, neuro-symbolic entropy regularization, that encourages the model to confidently predict a valid object. It is obtained by restricting entropy regularization to the distribution over only valid structures. This loss is efficiently computed when the output constraint is expressed as a tractable logic circuit. Moreover, it seamlessly integrates with other neuro-symbolic losses that eliminate invalid predictions. We demonstrate the efficacy of our approach on a series of semi-supervised and fully-supervised structured-prediction experiments, where we find that it leads to models whose predictions are more accurate and more likely to be valid.",
        "Adapting Neural Link Predictors for Data-Efficient Complex Query Answering": "Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Prior work in the literature has proposed to address this problem by designing architectures trained end-to-end for the complex query answering task with a reasoning process that is hard to interpret while requiring data and resource-intensive training. Other lines of research have proposed re-using simple neural link predictors to answer complex queries, reducing the amount of training data by orders of magnitude while providing interpretable answers. The neural link predictor used in such approaches is not explicitly optimised for the complex query answering task, implying that its scores are not calibrated to interact together. We propose to address these problems via $\\mathrm{CQD}^{A}$ , a parameter-efficient score adaptation model optimised to re-calibrate neural link prediction scores for the complex query answering task. While the neural link predictor is frozen, the adaptation component -- which only increases the number of model parameters by $0.0{\\bar{3}}\\%$ - is trained on the downstream complex query answering task. Furthermore, the calibration component enables us to support reasoning over queries that include atomic negations, which was previously impossible with link predictors. In our experiments, $\\mathrm{CQD}^{A}$ produces significantly more accurate results than current stateof-the-art methods, improving from 34.4 to 35.1 Mean Reciprocal Rank values averaged across all datasets and query types while using $\\leq3\\bar{0}\\%$ of the available training query types. We further show that $\\mathrm{CQD}^{A}$ is data-efficient, achievinge competitive results with only $1\\%$ of the training complex queries, and robust in out-of-domain evaluations.",
        "Knowledge Enhanced Graph Neural Networks": "Knowledge Enhanced Graph Neural Networks  \n\n$1^{\\mathrm{st}}$ Luisa Werner Universite Grenoble Alpes, INRIA Grenoble, France. luisa.werner@inria.fr  \n\n$2^{\\mathrm{nd}}$ Nabil Layaida INRIA Grenoble, France nabil.layaida @ inria.fr  \n\n$3^{\\mathrm{rd}}$ Pierre Geneves   \nCNRS, INRIA   \nGrenoble, France.   \npierre.geneves @inria.fr  \n\n4th Sarah Chlyah INRIA Grenoble, France sarah.chlyah@ inria.fr  \n\nAbstract-Graph data is omnipresent and has a wide variety of applications, such as in natural science, social networks, or the semantic web. However, while being rich in information, graphs are often noisy and incomplete. As a result, graph completion tasks, such as node classification or link prediction, have gained attention. On one hand, neural methods, such as graph neural networks, have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose Knowledge Enhanced Graph Neural Networks (KeGNN), a neurosymbolic framework for graph completion that combines both paradigms as it allows for the integration of prior knowledge into a graph neural network model. Essentially, KeGNN consists of a graph neural network as a base upon which knowledge. enhancement layers are stacked with the goal of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two well-known graph neural networks, Graph Convolutional Networks and Graph Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node classification.  \n\nIndex Terms-neuro-symbolic integration, graph neural networks, relational learning, knowledge graphs, fuzzy logic",
        "ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction": "Relation Extraction (RE) is the task of extracting semantic relationships between entities in a sentence and aligning them to relations defined in a vocabulary, which is generally in the form of a. Knowledge Graph (KG) or an ontology. Various approaches have been proposed so far to address this task. However, applying these techniques to biomedical text often yields unsatisfactory results because it is hard to infer relations directly from sentences due to the nature of the biomedical relations.  To address these issues, we present a novel technique called ReOnto, that makes use of neuro symbolic knowledge for the RE task. ReOnto employs a graph neural network to acquire the sentence representation and leverages publicly accessible ontologies as prior knowledge to identify the sentential relation between two entities. The approach involves extracting the relation path between the two entities from the ontology. We evaluate the effect of using symbolic knowledge from ontologies with graph neural networks.  Experimental results on two public biomedical datasets, BioRel and ADE, show that our method outperforms all the baselines (approximately by $3\\%$",
        "Knowledge Graph Embedding Preserving Soft Logical Regularity.": "Embedding knowledge graphs (KGs) into continuous vector spaces is currently an active research area. Soft rules, despite their uncertainty, are highly beneficial to KG embedding. However, they have not been studied enough in recent methods. A major challenge here is how to devise a principled framework, which efficiently and effectively integrates such soft logical information into embedding models. This paper proposes a highly scalable and effective method for preserving soft logical regularities by imposing soft rule constraints on relation representations. Specifically, we first represent relations as bilinear forms and map entity representations into a non-negative and bounded space. Then we derive a rule-based regularization that merely enforces relation representations to satisfy constraints introduced by soft rules. The proposed method has the following advantages: 1) it regularizes relations directly with the complexity of rule learning independent of entity set size, improving scalability; 2) it imposes prior logical information upon the structure of the embedding space, and would be beneficial to knowledge reasoning. Evaluation in link prediction on Freebase and DBpedia shows the effectiveness of our approach over many competitive baselines. Code and datasets are available at https://github.com/StudvGroup-lab/SLRE.",
        "Graph Collaborative Reasoning": "Graphs can represent relational information among entities and graph structures are widely used in many intelligent tasks such as search, recommendation, and question answering. However, most of the graph-structured data in practice suffer from incompleteness, and thus link prediction becomes an important research problem. Though many models are proposed for link prediction, the following two problems are still less explored: (1) Most methods model each link independently without making use of the rich information from relevant links, and (2) existing models are mostly designed based on associative learning and do not take reasoning into consideration. With these concerns, in this paper, we propose Graph Collaborative Reasoning (GCR), which can use the neighbor link. information for relational reasoning on graphs from logical reasoning perspectives. We provide a simple approach to translate a graph structure into logical expressions so that the link prediction task can be converted into a neural logic reasoning problem. We apply logical constrained neural modules to build the network architecture according to the logical expression and use backpropagation to efficiently learn the model parameters, which bridges differentiable learning and symbolic reasoning in a unified architecture. To show the effectiveness of our work, we conduct experiments on graphrelated tasks such as link prediction and recommendation based on commonly used benchmark datasets, and our graph collaborative reasoning approach achieves state-of-the-art performance.",
        "Regularizing Knowledge Graph Embeddings via Equivalence and Inversion Axioms": "Regularizing Knowledge Graph Embeddings via Equivalence and Inversion Axioms  \n\nPasquale Minervini $1(\\boxtimes)$ , Luca Costabello $^2$ , Emir Munoz $^{1,2}$ , Vit Novacek $\\cdot^{1}$ and Pierre-Yves Vandenbussche?  \n\nInsight Centre for Data Analytics, National University of Ireland, Galway, Ireland {pasquale.minervini,emir.munoz,vit.novacek}@insight-centre.org 2 Fujitsu Ireland Ltd., Galway, Ireland   \n{luca.costabello,emir.munoz,pierre-yves.vandenbussche}@ie.fujitsu.com  \n\nAbstract. Learning embeddings of entities and relations using neural architectures is an effective method of performing statistical learning on large-scale relational data, such as knowledge graphs. In this paper, we consider the problem of regularizing the training of neural knowledge graph embeddings by leveraging external background knowledge. We propose a principled and scalable method for leveraging equivalence and inversion axioms during the learning process, by imposing a set of model-dependent soft constraints on the predicate embeddings. The method has several advantages: (i) the number of introduced constraints does not depend on the number of entities in the knowledge base; (ii) regularities in the embedding space effectively reflect available background knowledge; (ii) it yields more accurate results in link prediction tasks over non-regularized methods; and (iv) it can be adapted to a variety of models, without affecting their scalability properties. We demonstrate the effectiveness of the proposed method on several large knowledge graphs. Our evaluation shows that it consistently improves the predictive accuracy of several neural knowledge graph embedding models (for instance, the MRR of TRANsE on WoRDNET increases by $11\\%$ ) without compromising their scalability properties.",
        "Jointly Embedding Knowledge Graphs and Logical Rules": "Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption, while rules represented as complex. formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex formulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and. triple classification tasks.  Experimental results show that joint embedding brings significant and consistent improvements over stateof-the-art methods. Particularly, it enhances the prediction of new facts which cannot even. be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings..",
        "Injecting Logical Background Knowledge into Embeddings for Relation Extraction.": "Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate for relations with sparse data. Rule-based extractors, on the other hand, can be easily extended to novel relations and improved for existing but inaccurate relations, through first-order formulae that capture auxiliary domain knowledge. However, usually a large set of such formulae is necessary to achieve generalization.  \n\nIn this paper, we introduce a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with first-order logic domain knowledge. We introduce simple approaches for estimating such embeddings, as well as a novel training algorithm to jointly optimize over factual and first-order logic information. Our results show that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae.",
        "QLogicE: Quantum Logic Empowered Embedding for Knowledge Graph Completion": " \n\nKnowledge graph completion (KGC) is an important technique for implicitly identifying missing entities. or relations in knowledge graphs (KGs) that are employed in various real-world applications such as. question answering, information retrieval, and making recommendations. Knowledge graph embedding (KGE) is a typical KGC approach that embeds entity and relation vectors into a low-dimensional vector space for this purpose. In this paper, we present a novel KGE approach called QLogicE. It. integrates translation and quantum embedding to capture features of elements within a fact, in the form of word embedding and the logical relationship among facts over KG, in the form of quantum. logic. Extensive experimental results on challenging benchmark datasets confirm that the proposed approach QLogicE achieves impressive and (sometimes) surprising performance on 8 embedding dimensions, whereas state-of-the-art KGE approaches typically achieve their best performance at approximately 200 embedding dimensions. In addition, the proposed model achieves. $94.84\\%$ Hits $@1$ on the challenging dataset FB15k237, which is almost twice as good as the best performance reported in this metric.  \n\n$\\copyright$ 2021 Elsevier B.V. All rights reserved."
    },
    "LogicallyInformedEmbedding": {
        "Fast and scalable learning of neuro-symbolic representations of biomedical knowledge.": "Fast and scalable learning of neuro-symbolic representations of biomedical knowledge.  \n\nAsan Agibetov $^{1}$ and Matthias Samwald1  \n\n$^{\\mathrm{~1~}}{}$ Section for Artificial Intelligence and Decision Support; Center for Medical Statistics, Informatics, and Intelligent Systems; Medical University of Vienna, Austria asan. agibetov@meduniwien.ac.at  \n\nAbstract. In this work we address the problem of fast and scalable learning of neuro-symbolic representations for general biological knowledge. Based on a recently published comprehensive biological knowledge graph (Alshahrani, 2017) that was used for demonstrating neurosymbolic representation learning, we show how to train fast (under 1 minute) log-linear neural embeddings of the entities. We utilize these representations as inputs for machine learning classifiers to enable important tasks such as biological link prediction. Classifiers are trained by concatenating learned entity embeddings to represent entity relations, and training classifiers on the concatenated embeddings to discern true relations from automatically generated negative examples. Our simple embedding methodology greatly improves on classification error compared to previously published state-of-the-art results, yielding a maximum increase of $+0.28$ F-measure and $+0.22$ ROC AUC scores for the most difficult biological link prediction problem. Finally, our embedding approach is orders of magnitude faster to train ( $\\leq1$ minute vs. hours), much more economical in terms of embedding dimensions ( $d\\,=\\,50$ Vs. $d\\,=\\,512$ ), and naturally encodes the directionality of the asymmetric biological relations, that can be controlled by the order with which we concatenate the embeddings.  \n\nKeywords: knowledge graphs, neural embeddings, biological link pre diction",
        "An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data: Experience and Practice": "The autonomous driving (AD) industry is exploring the use of knowledge graphs (KGs) to manage the vast amount of heterogeneous data generated from vehicular sensors. The various types of equipped sensors include video, LIDAR and RADAR. Scene understanding is an important topic in AD which requires consideration of various aspects of a scene, such as detected objects, events, time and location. Recent work on knowledge graph embeddings (KGEs) - an approach that facilitates neuro-symbolic fusion - has shown to improve the predictive performance of machine learning models. With the expectation that neuro-symbolic fusion through KGEs will improve scene understanding, this research explores the generation and evaluation of KGEs for autonomous driving data. We also present an investigation of the relationship between the level of informational detail in a KG and the quality of its derivative embeddings. By systematically evaluating KGEs along four dimensions - i.e. quality metrics, KG informational detail, algorithms, and datasets - we show that (1) higher levels of informational detail in KGs lead to higher quality embeddings, (2) type and relation semantics are better captured by the semantic transitional distance-based TransE algorithm, and (3) some metrics, such as coherence measure, may not be suitable for intrinsically evaluating KGEs in this domain. Additionally, we also present an (early) investigation of the usefulness of KGEs for two use-cases in the AD domain.",
        "UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference.": "Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic models to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice,. making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness.",
        "Towards Loosely-Coupling Knowledge Graph Embeddings and Ontology-based Reasoning": "Knowledge graph completion (a.k.a. link prediction), i.e., the task of inferring missing information from knowledge graphs, is a widely used task in many applications, such as product recommendation and question answering. The state-of-the-art approaches of knowledge graph embeddings and/or rule mining and reasoning are datadriven and, thus, solely based on the information the input knowledge graph contains. This leads to unsatisfactory prediction results which make such solutions inapplicable to crucial domains such as healthcare. To further enhance the accuracy of knowledge graph completion we propose to loosely-couple the data-driven power of knowledge graph embeddings with domain-specific reasoning stemming from experts or entailment regimes (e.g., OwL2). In this way, we not only enhance the prediction accuracy with domain knowl-. edge that may not be included in the input knowledge graph but also allow users to plugin their own knowledge graph embedding and reasoning method. Our initial results show that we enhance the MRR accuracy of vanilla knowledge graph embeddings by up to 3x and outperform hybrid solutions that combine knowledge graph embeddings with rule mining and reasoning up to. $3.5\\mathrm{x}$ MRR.",
        "Knowledge Graph Embedding with. Iterative Guidance from Soft Rules": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Combining such an embedding model with logic rules has recently attracted increasing attention. Most previous attempts made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. And they focused only on hard rules, which always hold with no exception and usually require extensive manual effort to create or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with various confidence levels extracted automatically from the KG. In. the learning process, RUGE iteratively queries rules to obtain soft labels for unlabeled triples, and integrates such newly labeled triples to update the embedding model. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings. We evaluate RUGE in link prediction on Freebase and YAGO. Experimental results show that: 1) with rule knowledge injected iteratively, RUGE achieves significant and consistent improvements over state-of-the-art baselines; and 2) despite their uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels. The code and data used for this paper can be obtained from https://github.com/iieir-km/RUGE.  \n\nRecently, a new research direction termed as knowledge graph embedding has been proposed and quickly received massive attention (Nickel, Tresp, and Kriegel 2011; Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015b; Yang et al. 2015; Nickel, Rosasco, and Poggio 2016; Trouillon et al. 2016). The key idea is to embed entities and relations in a KG into a low-dimensional continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG. Such embeddings contain rich semantic information, and can benefit a broad range of downstream applications (Weston et al. 2013; Bordes et al. 2014; Zhang et al. 2016; Xiong, Power, and Callan 2017).  \n\nTraditional methods performed embedding based solely on triples observed in a KG. But considering the power of logic rules in knowledge acquisition and inference, combining embedding models with logic rules has become a focus of current research (Rocktaschel et al. 2014; Vendrov et al. 2015; Wang and Cohen 2016; Hu et al. 2016). Wang et al. (2015) and Wei et al. (2015) tried to use embedding models and logic rules for KG completion. But in their work, rules are modeled separately from embedding models, and would not help to learn more predictive embeddings. Rocktaschel et al. (2015) and Guo et al. (2016) then devised joint learning paradigms which can inject first-order logic (FOL) into KG embedding. Demeester et al. (2016) further proposed lifted rule injection to avoid the costly propositionalization of FOL rules. Although these joint models are able to learn better embeddings after integrating logic rules, they still have their drawbacks and restrictions.",
        "Improving Knowledge Graph Embeddings with Ontological Reasoning": "Improving Knowledge Graph Embeddings with Ontological Reasoning  \n\nNitisha Jain $\\mathbf{\\Phi}_{\\mathrm{l}}^{1,2(\\boxtimes\\triangleleft)}\\oplus$ , Trung-Kien Tran', Mohamed H. Gad-Elrab $\\mathbf{\\Sigma}^{1}\\oplus$ and Daria Stepanova ${^1\\mathbb{\\Phi}}$  \n\n' Bosch Center for Artificial Intelligence, Renningen, Germanyd {nitisha. jain, trung-kien. tran,mohamed.gad-elrab}@de.bosch. com 2 Hasso Plattner Institute, University of Potsdam, Potsdam, Germanyd nitisha.jain@hpi.de  \n\nAbstract. Knowledge graph (KG) embedding models have emerged as powerful means for KG completion. To learn the representation of KGs, entities and relations are projected in a low-dimensional vector space so that not only existing triples in the KG are preserved but also new triples can be predicted. Embedding models might learn a good representation of the input KG, but due to the nature of machine learning approaches, they often lose the semantics of entities and relations, which might lead to nonsensical predictions. To address this issue we propose to improve the accuracy of embeddings using ontological reasoning. More specifically, we present a novel iterative approach ReasonKGE that identifies dynamically via symbolic reasoning inconsistent predictions produced by a given embedding model and feeds them as negative samples for retraining this model. In order to address the scalability problem that arises when integrating ontological reasoning into the training process, we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining. Experimental results demonstrate the improvements in accuracy of facts produced by our method compared to the state-of-the-art.",
        "Article Enhanced Knowledge Graph Embedding by Jointly Learning Soft Rules and Facts": "Article Enhanced Knowledge Graph Embedding by Jointly Learning Soft Rules and Facts  \n\n|indou ZhangD and Jing Li \\*   \nSchool of Computer Science and Technology, University of Science and Technology of China, Hefei 230026,   \nChina; jindou@mail.ustc.edu.cn   \n\\* Correspondence: lj@ustc.edu.cn  \n\nReceived: 19 November 2019; Accepted: 7 December 2019; Published: 10 December 2019  \n\nCombining first order logic rules with a Knowledge Graph (KG) embedding model has recently gained increasing attention, as rules introduce rich background information. Among such studies, models equipped with soft rules, which are extracted with certain confidences, achieve state-of-the-art performance. However, the existing methods either cannot support the transitivity and composition rules or take soft rules as regularization terms to constrain derived facts, which is incapable of encoding the logical background knowledge about facts contained in soft rules. In addition, previous works performed one time logical inference over rules to generate valid groundings for modeling rules, ignoring forward chaining inference, which can further generate more valid groundings to better model rules. To these ends, this paper proposes Soft Logical rules enhanced Embedding (SoLE), a novel KG embedding model equipped with a joint training algorithm over soft rules and KG facts to inject the logical background knowledge of rules into embeddings, as well as forward chaining inference over rules. Evaluations on Freebase and DBpedia show that SoLE not only achieves improvements of $11.6\\%/5.9\\%$ in Mean Reciprocal Rank (MRR) and $18.4\\%/15.9\\%$ in $\\mathrm{HIT}{\\cal S}@1$ compared to the model on which SoLE is based, but also significantly and consistently outperforms the state-of-the-art baselines in the link prediction task.  \n\nKeywords: knowledge graph; knowledge graph embedding; logical rule; link prediction",
        "Databases and ontologies": "Motivation: Biological data and knowledge bases increasingly rely on Semantic Web technologies and the use of knowledge graphs for data integration, retrieval and federated queries. In the past years, feature learning methods that are applicable to graph-structured data are becoming available, but have not yet widely been applied and evaluated on structured biological knowledge. Results: We develop a novel method for feature learning on biological knowledge graphs. Our method combines symbolic methods, in particular knowledge representation using symbolic logic and automated reasoning, with neural networks to generate embeddings of nodes that encode for related information within knowledge graphs. Through the use of symbolic logic, these embeddings contain both explicit and implicit information. We apply these embeddings to the prediction of edges in the knowledge graph representing problems of function prediction, finding candidate genes of diseases, protein-protein interactions, or drug target relations, and demonstrate perform-. ance that matches and sometimes outperforms traditional approaches based on manually crafted features. Our method can be applied to any biological knowledge graph, and will thereby open up the increasing amount of Semantic Web based knowledge bases in biology to use in machine learning and data analytics.  \n\nAvailability and implementation: https://github.com/bio-ontology-research-group/walking-rdf-and-owl Contact: robert.hoehndorf@kaust.edu.sa  \n\nSupplementary information: Supplementary data are available at Bioinformatics online."
    },
    "RuleLearning": {
        "Differentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphs": "Knowledge graph (KG) reasoning utilizes two primary techniques, i.e., rule-based and KG-embedding based. The former provides precise inferences, but inferring. via concrete rules is not scalable. The latter enables efficient reasoning at the cost of ambiguous inference accuracy. Neuro-symbolic reasoning seeks to amalgamate the advantages of both techniques. The crux of this approach is replacing the predicted existence of all possible triples (i.e., truth scores inferred from rules) with a suitable approximation grounded in embedding representations. However, constructing an effective approximation of all possible triples' truth scores is a challenging task, because it needs to balance the tradeoff between accuracy and efficiency, while compatible with both the rule-based and KG-embedding models. To this end, we proposed a differentiable framework - DiffLogic. Instead of. directly approximating all possible triples, we design a tailored filter to adaptively select essential triples based on the dynamic rules and weights. The truth scores assessed by KG-embedding are continuous, so we employ a continuous Markov logic network named probabilistic soft logic (PSL). It employs the truth scores of essential triples to assess the overall agreement among rules, weights, and observed triples. PSL enables end-to-end differentiable optimization, so we can alternately update embedding and weighted rules. On benchmark datasets, we empirically show that DiffLogic surpasses baselines in both effectiveness and efficiency..",
        "Differentiable Learning of Logical Rules for Knowledge Base Reasoning": "We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space.. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.",
        "Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning": "Reasoning is essential for the development of large knowledge graphs, especially for completion, which aims to infer new triples based on existing ones. Both rules and embeddings can be used for knowledge graph reasoning and they have their own advantages and difficulties. Rule-based reasoning is accurate and explainable but rule learning with searching over the graph always suffers from efficiency due to huge search space. Embedding-based reasoning is more scalable and efficient as the reasoning is conducted via computation between embeddings, but it has difficulty learning good representations for sparse entities because a good embedding relies heavily on data richness. Based on this observation, in this paper. we explore how embedding and rule learning can be combined together and complement each other's difficulties with their advantages. We propose a novel framework IterE iteratively learning. embeddings and rules, in which rules are learned from embeddings. with proper pruning strategy and embeddings are learned from existing triples and new triples inferred by rules. Evaluations on embedding qualities of IterE show that rules help improve the quality of sparse entity embeddings and their link prediction results. We also evaluate the efficiency of rule learning and quality of rules from IterE compared with $\\mathrm{AME+}$ , showing that IterE is capable of. generating high quality rules more efficiently. Experiments show that iteratively learning embeddings and rules benefit each other during learning and prediction..",
        "Probabilistic Logic Neural Networks for Reasoning": "Knowledge graph reasoning, which aims at predicting the missing facts through. reasoning with the observed facts, is critical to many applications. Such a problem. has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge. with first-order logic and meanwhile handle the uncertainty. However, the inference. in MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more. effective and efficient. However, they are unable to leverage domain knowledge.. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet),. which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. In the E-step, a knowledge graph embedding model is used for inferring the missing. triplets, while in the M-step, the weights of logic rules are updated based on both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines.",
        "DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs": "In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.",
        "EFFICIENT PROBABILISTIC LOGIC REASONING WITH GRAPH NEURAL NETWORKS": "Markov Logic Networks (MLNs), which elegantly combine logic rules and proba-. bilistic graphical models, can be used to address many knowledge graph problems.. However, inference in MLN is computationally intensive, making the industrial-. scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph prob-. lems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, we. explore the combination of MLNs and GNNs, and use graph neural networks for. variational inference in MLN. We propose a GNN variant, named ExpressGNN,. which strikes a nice balance between the representation power and the simplicity of. the model. Our extensive experiments on several benchmark datasets demonstrate that ExpressGNN leads to effective and efficient probabilistic logic reasoning.",
        "A Hybrid Model for Learning Embeddings and Logical Rules Simultaneously. from Knowledge Graphs.": "A Hybrid Model for Learning Embeddings and Logical Rules Simultaneously. from Knowledge Graphs.  \n\nSusheel Suresh and Jennifer Neville Computer Science Department Purdue University West Lafayette, IN, USA [suresh43, neville] $@$ purdue.edu  \n\nAbstract-The problem of knowledge graph (KG) reasoning has been widely explored by traditional rule-based systems and more recently by knowledge graph embedding methods. While logical rules can capture deterministic behavior in a KG they are brittle and mining ones that infer facts beyond the known KG is challenging. Probabilistic embedding methods are effective in capturing global soft statistical tendencies and reasoning with them is computationally efficient. While embedding representations learned from rich training data are expressive, incompleteness and sparsity in real-world KGs can impact their effectiveness. We aim to leverage the complementary properties of both methods to develop a hybrid model that learns both high-quality rules and embeddings simultaneously. Our method uses a cross feedback paradigm wherein, an embedding model is used to guide the search of a rule mining system to mine rules and infer new facts. These new facts are sampled and further used to refine the embedding model. Experiments on multiple benchmark datasets show the effectiveness of our method over other competitive standalone and hybrid baselines. We also show its efficacy in a sparse KG setting and finally explore the connection with negative sampling.",
        "RNNLOGIC: LEARNING LOGIC RULES FOR REASONING ON KNOWLEDGE GRAPHS": "This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is first updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.",
        "Biomedical Knowledge Graph Refinement with Embedding and Logic Rules": "Currently, there is a rapidly increasing need for high-quality biomedical knowledge graphs (BioKG) that provide direct and precise biomedical knowledge. In the context of COVID-19, this issue is even more necessary to be highlighted. However,. most BioKG construction inevitably includes. numerous conflicts and noises deriving from incorrect knowledge descriptions in literature and defective information extraction techniques. Many. studies have demonstrated that reasoning upon the knowledge graph is effective in eliminating such conflicts and noises. This paper proposes a method BioGRER to improve the BioKG's quality, which comprehensively combines the knowledge graph embedding and logic rules that support and negate triplets in the BioKG. In the proposed model, the BioKG refinement problem is formulated as the probability estimation for triplets in the BioKG. We employ the variational EM algorithm to optimize knowledge graph embedding and logic rule inference alternately. In this way, our model could combine efforts from both the knowledge graph embedding and logic rules, leading to better results than using them alone. We evaluate our model over a COvID-19 knowledge graph and obtain competitive results..",
        "Neural Multi-Hop Reasoning With Logical Rules on Biomedical Knowledge Graphs.": "Neural Multi-Hop Reasoning With Logical Rules on Biomedical Knowledge Graphs.  \n\nYushan Liu $^{1,3}$ , Marcel Hildebrandt $^{1,3}$ , Mitchell Joblin $^{1}$ , Martin Ringsquandl $^{1}$ Rime Raissouni $^{2,3}$ , Volker Tresp $^{1,3}$  \n\n$^{1}$ Siemens, Otto-Hahn-Ring 6, 81739 Munich, Germany {firstname.lastname}@siemens.com   \n2 Siemens Healthineers, HartmanntraBe 16, 91052 Erlangen, Germany   \n$^3$ Ludwig Maximilian University of Munich, Geschwister-Scholl-Platz 1, 80539 Munich, Germany  \n\nAbstract. Biomedical knowledge graphs permit an integrative computational approach to reasoning about biological systems. The nature of biological data leads to a graph structure that differs from those typically encountered in benchmarking datasets. To understand the implications this may have on the performance of reasoning algorithms, we. conduct an empirical study based on the real-world task of drug repurposing. We formulate this task as a link prediction problem where both compounds and diseases correspond to entities in a knowledge graph. To overcome apparent weaknesses of existing algorithms, we propose a new method, PoLo, that combines policy-guided walks based on rein-. forcement learning with logical rules. These rules are integrated into the algorithm by using a novel reward function. We apply our method to Hetionet, which integrates biomedical information from 29 prominent. bioinformatics databases. Our experiments show that our approach out-. performs several state-of-the-art methods for link prediction while providing interpretability.  \n\nKeywords: Neural multi-hop reasoning . Reinforcement learning : Logical rules : Biomedical knowledge graphs",
        "Combining Rules and Embeddings via Neuro-Symbolic AI for Knowledge Base Completion": "Recent interest in Knowledge Base Completion (KBC) has led to a plethora of approaches based on reinforcement learn-. ing, inductive logic programming and graph embeddings. In particular, rule-based KBC has led to interpretable rules while being comparable in performance with graph embeddings. Even within rule-based KBC, there exist different approaches that lead to rules of varying quality and previous work has not. always been precise in highlighting these differences. Another issue that plagues most rule-based KBC is the non-uniformity of relation paths: some relation sequences occur in very few paths while others appear very frequently. In this paper, we. show that not all rule-based KBC models are the same and propose two distinct approaches that learn in one case: 1) a. mixture of relations and the other 2) a mixture of paths. When implemented on top of neuro-symbolic AI, which learns rules. by extending Boolean logic to real-valued logic, the latter. model leads to superior KBC accuracy outperforming state-ofthe-art rule-based KBC by $2{-}10\\%$ in terms of mean reciprocal rank. Furthermore, to address the non-uniformity of relation. paths, we combine rule-based KBC with graph embeddings thus improving our results even further and achieving the best of both worlds.",
        "Rule Induction in Knowledge Graphs Using Linear Programming": "We present a simple linear programming (LP) based method. to learn compact and interpretable sets of rules encoding the facts in a knowledge graph (KG) and use these rules to solve. the KG completion problem. Our LP model chooses a set of rules of bounded complexity from a list of candidate firstorder logic rules and assigns weights to them. The complexity bound is enforced via explicit constraints. We combine simple rule generation heuristics with our rule selection LP to obtain predictions with accuracy comparable to state-of-. the-art codes, even while generating much more compact rule sets. Furthermore, when we take as input rules generated by. other codes, we often improve interpretability by reducing the number of chosen rules, while maintaining accuracy..",
        "Transductive Data Augmentation with Relational Path Rule Mining for Knowledge Graph Embedding": "Transductive Data Augmentation with Relational Path Rule Mining for Knowledge Graph Embedding  \n\nYushi Hirose', Masashi Shimbo2,3, Taro Watanabe4 1Tokyo Institute of Technology, Tokyo, Japan 2Chiba Institute of Technology, Chiba, Japan 3Riken AIP, Tokyo, Japan 4Nara Institute of Science & Technology, Nara, Japan. hirose.y.am@m.titech.ac.jp, shimbo $@$ stair.center, taro $@$ is.naist.jp  \n\nAbstract-For knowledge graph completion, two major types of prediction models exist: one based on graph embeddings, and the other based on relation path rule induction. They have different advantages and disadvantages. To take advantage of both types, hybrid models have been proposed recently. One of. the hybrid models, UniKER, alternately augments training data by relation path rules and trains an embedding model. Despite. its high prediction accuracy, it does not take full advantage of relation path rules, as it disregards low-confidence rules in. order to maintain the quality of augmented data. To mitigate. this limitation, we propose transductive data augmentation by. relation path rules and confidence-based weighting of augmented. data. The results and analysis show that our proposed method effectively improves the performance of the embedding model by. augmenting data that include true answers or entities similar to them.  \n\nIndex Terms-Knowledge graph embedding, Relation path rule, Data augmentation, Transductive learninge",
        "Rule Learning from Knowledge Graphs Guided by Embedding Models": "Rule Learning from Knowledge Graphs Guided by Embedding Models  \n\nVinh Thinh Ho $\\mathbf{\\Omega}_{\\mathbf{L}}$ , Daria Stepanova $1(\\boxtimes)$ , Mohamed H. Gad-Elrab $\\cdot$ Evgeny Kharlamov?, and Gerhard Weikum $\\cdot^{1}$  \n\n$^{1}$ Max Planck Institute for Informatics, Saarbruicken, Germany dstepano@mpi-inf .mpg.de 2 University of Oxford, Oxford, UK  \n\nAbstract. Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as confidence reflect rule quality well when the KG is reasonably complete; however,. these measures might be misleading otherwise. So it is difficult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules could be generated. Therefore, the ranking and pruning of candidate rules are major problems. To address this issue, we propose a rule learning method that utilizes probabilistic rep-. resentations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce..",
        "Neuro-Symbolic XAI: Application to Drug Repurposing for Rare Diseases": "Neuro-Symbolic XAI: Application to Drug Repurposing for Rare Diseases  \n\nMartin Drance()  \n\nInserm 1219, Bordeaux Population Health Research Center, Team ERIAS, University of Bordeaux, Bordeaux, France martin. drance@u-bordeaux.fr  \n\nAbstract. An emerging challenge in high stakes fields such as healthcare is to build a more transparent artificial intelligence. Among the fields which use AI techniques, is drug development, and more specifically drug. repurposing. DR involves finding a new indication for an existing drug.. The hypotheses generated by DR techniques must be validated. There-. fore, the mechanism of generation must be understood. In this project,. we focus on DR using link prediction algorithms. Link prediction consists of generating hypotheses about the relationships between a known. molecule and a given target. More specifically, this PhD project aims at. creating methods allowing to make transparent prediction in the context. of drug repurposing but also to understand how the organization of data. in a knowledge graph changes the quality of predictions..  \n\nKeywords: XAI . Drug repurposing . Knowledge graph",
        "Rule-enhanced iterative complementation for knowledge graph reasoning": "Article history:   \nReceived 24 July 2020   \nReceived in revised form 1 April 2021   \nAccepted 13 June 2021   \nAvailable online 16 June 2021  \n\nKeywords:   \nLogic rules   \nKnowledge graph   \nReasoning   \nGraph embedding   \nGraph convolutional network  \n\nKnowledge graph (KG) reasoning aims to infer missing valid triples from observed triples,. thereby improving the semantics of the whole KG. The general KG reasoning involves rulebased and embedding-based methods. The former can provide an interpretable reasoning. process but has low efficiency, while the latter is the converse. Therefore, some hybrid methods have been proposed, but there are still two challenges: the completeness of rule learning and the determination of hidden triples. To address these challenges, this paper. proposes a rule-enhanced iterative complementation (Rule-IC) method, which involves three components: rule learning, an embedding learner and a triple discriminator. Such an iterative process enriches the semantics of KG and further increases the completeness of rule learning to generate hidden triples. In order to precisely determine the validity of. hidden triples, a multi-relational graph convolutional network (GCN) with attentive message passing is introduced as a triple discriminator. The embedding learner for KG reasoning and the GCN discriminator complement each other by valid hidden triples. In addition,. the performance of these three components improves overall during the iterative process.. Experimental results show that most evaluation metrics of Rule-IC are better than those of. several baselines on four common KGs. Furthermore, it is scalable and can be extended to all KG embedding models theoretically..  \n\n$\\circledcirc$ 2021 Elsevier Inc. All rights reserved.",
        "Probabilistic Logic Graph Attention Networks for Reasoning": "Knowledge base completion, which involves the prediction of missing relations between entities in a knowledge graph, has been an active area of research. Markov logic networks, which combine probabilistic graphical models and first order logic, have proven to be effective on knowledge graph tasks like link prediction and question answering. However, their intractable inference limits their scalability and wider applicability across various tasks. In recent times, graph attention neural networks, which capture features of neighbouring entities, have achieved superior results on highly complex graph problems like node classification and link prediction. Combining the best of both worlds, we propose Probabilistic Logic Graph Attention Network (pGAT) for reasoning. In the proposed model, the joint distribution of all possible triplets defined by a Markov logic network is optimized with a variational EM algorithm. This helps us to efficiently combine first-order logic and graph attention networks. With the goal of establishing strong. baselines for future research on link prediction, we evaluate our model on various standard link prediction benchmarks, and obtain competitive results."
    }
}